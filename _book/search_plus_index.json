{"./":{"url":"./","title":"Kubernetes源码学习","keywords":"","body":"kubernetes-learning source code learning based on v1.32 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 12:20:59 "},"kube-scheduler/":{"url":"kube-scheduler/","title":"Scheduler","keywords":"","body":"kube-scheduler © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/01-Scheduler创建流程与调度队列.html":{"url":"kube-scheduler/01-Scheduler创建流程与调度队列.html","title":"Scheduler创建流程与调度队列","keywords":"","body":" 组件的创建 Options结构体 Setup函数 New函数 调度器实例的启动 调度器运行原理 调度队列详解 Pod在队列中的类型 调度队列 优先队列 优先队列的实现 调度队列的几种方法 Run Add Update Delete Activate 调度队列小结及流程图 Scheduler创建流程与调度队列 组件的创建 在Kubernetes的各个组件中，创建的流程都是类似的，如调度器的创建入口在cmd/kube-scheduler/scheduler.go路径下。 func main() { // 创建 command := app.NewSchedulerCommand() // 启动 code := cli.Run(command) os.Exit(code) } 创建的实现如下，我们只需要关注cobra.Command中RunE的实现，这部分代码集中在cmd/kube-scheduler/app/server.go中。 func NewSchedulerCommand(registryOptions ...Option) *cobra.Command { // explicitly register (if not already registered) the kube effective version and feature gate in DefaultComponentGlobalsRegistry, // which will be used in NewOptions. _, _ = featuregate.DefaultComponentGlobalsRegistry.ComponentGlobalsOrRegister( featuregate.DefaultKubeComponent, utilversion.DefaultBuildEffectiveVersion(), utilfeature.DefaultMutableFeatureGate) // 初始化组件基本配置 返回一个Options结构体的指针 opts := options.NewOptions() cmd := &cobra.Command{ Use: \"kube-scheduler\", // 持久化的PreRunE钩子函数 PersistentPreRunE: func(*cobra.Command, []string) error { // makes sure feature gates are set before RunE. return opts.ComponentGlobalsRegistry.Set() }, // 主要执行函数 关注这个函数即可 RunE: func(cmd *cobra.Command, args []string) error { return runCommand(cmd, opts, registryOptions...) }, // 遍历命令行的args 如果arg长度大于0报错 // scheduler不支持传入args Args: func(cmd *cobra.Command, args []string) error { for _, arg := range args { if len(arg) > 0 { return fmt.Errorf(\"%q does not take any arguments, got %q\", cmd.CommandPath(), args) } } return nil }, } nfs := opts.Flags verflag.AddFlags(nfs.FlagSet(\"global\")) globalflag.AddGlobalFlags(nfs.FlagSet(\"global\"), cmd.Name(), logs.SkipLoggingConfigurationFlags()) fs := cmd.Flags() for _, f := range nfs.FlagSets { fs.AddFlagSet(f) } cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cliflag.SetUsageAndHelpFunc(cmd, *nfs, cols) if err := cmd.MarkFlagFilename(\"config\", \"yaml\", \"yml\", \"json\"); err != nil { klog.Background().Error(err, \"Failed to mark flag filename\") } return cmd } Options结构体 看一下Options结构体的定义，在路径cmd/kube-scheduler/app/options/options.go下。 // Options has all the params needed to run a Scheduler type Options struct { // 调度器核心配置 ComponentConfig *kubeschedulerconfig.KubeSchedulerConfiguration // 调度器与客户端通信配置 SecureServing *apiserveroptions.SecureServingOptionsWithLoopback // 认证配置 Authentication *apiserveroptions.DelegatingAuthenticationOptions // 授权配置 Authorization *apiserveroptions.DelegatingAuthorizationOptions // 性能指标配置 Metrics *metrics.Options // 日志记录器配置 Logs *logs.Options // 将要弃用的选项 Deprecated *DeprecatedOptions // 选举配置 LeaderElection *componentbaseconfig.LeaderElectionConfiguration // 调度器配置文件路径 ConfigFile string // 配置写入路径 WriteConfigTo string // api server地址 Master string // 特性门控配置 ComponentGlobalsRegistry featuregate.ComponentGlobalsRegistry // 存储启动flag参数 Flags *cliflag.NamedFlagSets } 关键函数runCommand，在这个函数中通过Setup()函数生成了CompletedConfig和Scheduler类型的实例cc和sched，然后使用这两个重要对象作为参数启动调度器。 func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Option) error { verflag.PrintAndExitIfRequested() fg := opts.ComponentGlobalsRegistry.FeatureGateFor(featuregate.DefaultKubeComponent) // Activate logging as soon as possible, after that // show flags with the final logging configuration. if err := logsapi.ValidateAndApply(opts.Logs, fg); err != nil { fmt.Fprintf(os.Stderr, \"%v\\n\", err) os.Exit(1) } cliflag.PrintFlags(cmd.Flags()) // 通常创建一个新的上下文时会从context.Background()开始 ctx, cancel := context.WithCancel(context.Background()) defer cancel() go func() { stopCh := server.SetupSignalHandler() Setup函数 Setup()函数的实现如下，其中scheduler.New是调度器实例的创建点。 func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) { // 创建cfg并注入到opt中 if cfg, err := latest.Default(); err != nil { return nil, nil, err } else { opts.ComponentConfig = cfg } // 校验 if errs := opts.Validate(); len(errs) > 0 { return nil, nil, utilerrors.NewAggregate(errs) } // 生成Config // 其中包括两个客户端 一个用于对接api server 一个专门处理事件用于事件广播器中 // 创建informer工厂和Leader选举配置 c, err := opts.Conf ig(ctx) if err != nil { return nil, nil, err } // 把Config类型转换成CompletedConfig cc := c.Complete() outOfTreeRegistry := make(runtime.Registry) for _, option := range outOfTreeRegistryOptions { if err := option(outOfTreeRegistry); err != nil { return nil, nil, err } } recorderFactory := getRecorderFactory(&cc) completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0) // 根据CompletedConfig创建调度器实例 sched, err := scheduler.New(ctx, cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), scheduler.WithFrameworkOutOfTreeRegistry(outOfTreeRegistry), scheduler.WithPodMaxBackoffSeconds(cc.ComponentConfig.PodMaxBackoffSeconds), scheduler.WithPodInitialBackoffSeconds(cc.ComponentConfig.PodInitialBackoffSeconds), scheduler.WithPodMaxInUnschedulablePodsDuration(cc.PodMaxInUnschedulablePodsDuration), scheduler.WithExtenders(cc.ComponentConfig.Extenders...), scheduler.WithParallelism(cc.ComponentConfig.Parallelism), scheduler.WithBuildFrameworkCapturer(func(profile kubeschedulerconfig.KubeSchedulerProfile) { // Profiles are processed during Framework instantiation to set default plugins and configurations. Capturing them for logging completedProfiles = append(completedProfiles, profile) }), ) if err != nil { return nil, nil, err } if err := options.LogOrWriteConfig(klog.FromContext(ctx), opts.WriteConfigTo, &cc.ComponentConfig, completedProfiles); err != nil { return nil, nil, err } // 返回配置和实例 return &cc, sched, nil New函数 在New()函数中，完整地创建了一个调度器实例以及相关组件。 func New(ctx context.Context, client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, opts ...Option) (*Scheduler, error) { logger := klog.FromContext(ctx) stopEverything := ctx.Done() options := defaultSchedulerOptions for _, opt := range opts { opt(&options) } if options.applyDefaultProfile { var versionedCfg configv1.KubeSchedulerConfiguration scheme.Scheme.Default(&versionedCfg) cfg := schedulerapi.KubeSchedulerConfiguration{} if err := scheme.Scheme.Convert(&versionedCfg, &cfg, nil); err != nil { return nil, err } options.profiles = cfg.Profiles } // registry是注册的in-tree插件列表 registry := frameworkplugins.NewInTreeRegistry() // 合并in-tree和out-of-tree列表 if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil { return nil, err } metrics.Register() extenders, err := buildExtenders(logger, options.extenders, options.profiles) if err != nil { return nil, fmt.Errorf(\"couldn't build extenders: %w\", err) } // 通过Informer工厂 创建podLister和nodeLister podLister := informerFactory.Core().V1().Pods().Lister() nodeLister := informerFactory.Core().V1().Nodes().Lister() // 初始化全局快照 snapshot := internalcache.NewEmptySnapshot() metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopEverything) // waitingPods holds all the pods that are in the scheduler and waiting in the permit stage waitingPods := frameworkruntime.NewWaitingPodsMap() var resourceClaimCache *assumecache.AssumeCache var draManager framework.SharedDRAManager // 如果动态资源分配的特性门控开启 创建资源申请的Informmer Cache和DRA Manager if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) { resourceClaimInformer := informerFactory.Resource().V1beta1().ResourceClaims().Informer() resourceClaimCache = assumecache.NewAssumeCache(logger, resourceClaimInformer, \"ResourceClaim\", \"\", nil) draManager = dynamicresources.NewDRAManager(ctx, resourceClaimCache, informerFactory) } // 根据上面的参数 创建一个完整的Profile profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), frameworkruntime.WithInformerFactory(informerFactory), frameworkruntime.WithSharedDRAManager(draManager), frameworkruntime.WithSnapshotSharedLister(snapshot), frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(options.frameworkCapturer)), frameworkruntime.WithParallelism(int(options.parallelism)), frameworkruntime.WithExtenders(extenders), frameworkruntime.WithMetricsRecorder(metricsRecorder), frameworkruntime.WithWaitingPods(waitingPods), ) if err != nil { return nil, fmt.Errorf(\"initializing profiles: %v\", err) } if len(profiles) == 0 { return nil, errors.New(\"at least one profile is required\") } // 调度队列的相关配置 preEnqueuePluginMap := make(map[string][]framework.PreEnqueuePlugin) queueingHintsPerProfile := make(internalqueue.QueueingHintMapPerProfile) var returnErr error for profileName, profile := range profiles { preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins() queueingHintsPerProfile[profileName], err = buildQueueingHintMap(ctx, profile.EnqueueExtensions()) if err != nil { returnErr = errors.Join(returnErr, err) } } if returnErr != nil { return nil, returnErr } // 创建调度队列实例 PriorityQueue实例主要包括了activeQ/podBackoffQ/unschedulablePods、nsLister和nominator podQueue := internalqueue.NewSchedulingQueue( profiles[options.profiles[0].SchedulerName].QueueSortFunc(), informerFactory, internalqueue.WithPodInitialBackoffDuration(time.Duration(options.podInitialBackoffSeconds)*time.Second), internalqueue.WithPodMaxBackoffDuration(time.Duration(options.podMaxBackoffSeconds)*time.Second), internalqueue.WithPodLister(podLister), internalqueue.WithPodMaxInUnschedulablePodsDuration(options.podMaxInUnschedulablePodsDuration), internalqueue.WithPreEnqueuePluginMap(preEnqueuePluginMap), internalqueue.WithQueueingHintMapPerProfile(queueingHintsPerProfile), internalqueue.WithPluginMetricsSamplePercent(pluginMetricsSamplePercent), internalqueue.WithMetricsRecorder(*metricsRecorder), ) // 用创建出来的PQ给framework实例设置PodNominator和PodActivator for _, fwk := range profiles { fwk.SetPodNominator(podQueue) fwk.SetPodActivator(podQueue) } // 创建调度器缓存 schedulerCache := internalcache.New(ctx, durationToExpireAssumedPod) // cache debugger的作用包括比较Lister和Cache.Snapshot的数据一致性和记录缓存/调度队列信息 debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue) debugger.ListenForSignal(ctx) // 实例创建 sched := &Scheduler{ Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, percentageOfNodesToScore: options.percentageOfNodesToScore, Extenders: extenders, StopEverything: stopEverything, SchedulingQueue: podQueue, Profiles: profiles, logger: logger, } sched.NextPod = podQueue.Pop sched.applyDefaultHandlers() // 注册事件处理器 if err = addAllEventHandlers(sched, informerFactory, dynInformerFactory, resourceClaimCache, unionedGVKs(queueingHintsPerProfile)); err != nil { return nil, fmt.Errorf(\"adding event handlers: %w\", err) } return sched, nil } 调度器实例的启动 Run()函数位于cmd/kube-scheduler/app/server.go，和runCommand()函数在同一路径下，完整过程包括： 先启动日志记录器，输出Info级别的环境信息日志; 根据componentconfig注册配置configz，类型是Config指针; 启动事件广播器; 初始化健康检查设置，包括WatchDog和Shutdown检查器; 检查是否为Leader; 创建同步处理健康检查器; 协调Leader选举; 启动健康检查器Server; 启动所有Informer并等待同步; 运行调度器实例; func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error { logger := klog.FromContext(ctx) // To help debugging, immediately log version logger.Info(\"Starting Kubernetes Scheduler\", \"version\", utilversion.Get()) logger.Info(\"Golang settings\", \"GOGC\", os.Getenv(\"GOGC\"), \"GOMAXPROCS\", os.Getenv(\"GOMAXPROCS\"), \"GOTRACEBACK\", os.Getenv(\"GOTRACEBACK\")) // Configz registration. if cz, err := configz.New(\"componentconfig\"); err != nil { return fmt.Errorf(\"unable to register configz: %s\", err) } else { cz.Set(cc.ComponentConfig) } // Start events processing pipeline. cc.EventBroadcaster.StartRecordingToSink(ctx.Done()) defer cc.EventBroadcaster.Shutdown() // Setup healthz checks. var checks, readyzChecks []healthz.HealthChecker if cc.ComponentConfig.LeaderElection.LeaderElect { checks = append(checks, cc.LeaderElection.WatchDog) readyzChecks = append(readyzChecks, cc.LeaderElection.WatchDog) } readyzChecks = append(readyzChecks, healthz.NewShutdownHealthz(ctx.Done())) waitingForLeader := make(chan struct{}) isLeader := func() bool { select { case _, ok := 调度器运行原理 上面已经说到了创建完成后直接运行调度器实例，来看一下调度器的启动到底都包含哪些步骤，代码路径pkg/scheduler/scheduler.go。 Run()函数的实现非常简单，第一步先启动日志记录器，然后启动调度队列和调度循环，然后等待ctx.Done()信号使线程阻塞，如果收到了ctx.Done()信号就对调度队列和调度插件执行Close()操作释放资源，其中ScheduleOne是在一个协程中启动的，原因是为了避免在没有Pod需要调度时挂起状态的ScheduleOne阻塞了后续的信号接收，导致调度队列无法关闭造成的死锁情况。 其中涉及到一个核心结构SchedulingQueue和核心方法ScheduleOne，先抛出一个官方文档中的流程图，在后面会进行详细说明。 // Run begins watching and scheduling. It starts scheduling and blocked until the context is done. func (sched *Scheduler) Run(ctx context.Context) { logger := klog.FromContext(ctx) // 启动调度队列 sched.SchedulingQueue.Run(logger) // 启动调度循环 go wait.UntilWithContext(ctx, sched.ScheduleOne, 0) // 阻塞 等待关闭信号 调度队列详解 上一节说到启动调度器包括：启动调度队列和启动调度循环，在此详细解释调度队列的实现，如果不关注调度队列的实现可以先跳过本节。 Pod在队列中的类型 调度队列中的Pod结构是QueuedPodInfo，它是由PodInfo加上了Pod在队列中的一些属性，包括入队时间戳、尝试次数、首次入队时间和导致调度失败的插件集合所共同组成，类型定义如下，在路径pkg/scheduler/framework/types.go下。 type QueuedPodInfo struct { // Pod信息 *PodInfo // 本次入队时间 Timestamp time.Time // 失败次数 Attempts int // 首次入队时间 InitialAttemptTimestamp *time.Time // 导致Pod以Unschedulable或UnschedulableAndUnresolvable状态返回的插件列表 // 会导致这两种状态的插件类型包括PreFilter, Filter, Reserve, Permit, PreBind. UnschedulablePlugins sets.Set[string] // 导致Pod以Pending状态返回的插件列表 PendingPlugins sets.Set[string] // Whether the Pod is scheduling gated (by PreEnqueuePlugins) or not. Gated bool } PodInfo结构体是调度过程中最常见的数据类型，其中包含Pod类型和软/硬亲和/反亲和信息。 type PodInfo struct { Pod *v1.Pod RequiredAffinityTerms []AffinityTerm RequiredAntiAffinityTerms []AffinityTerm PreferredAffinityTerms []WeightedAffinityTerm PreferredAntiAffinityTerms []WeightedAffinityTerm } 调度队列 调度队列SchedulingQueue是一个接口类型 type SchedulingQueue interface { // 调度过程中可能需要同步Pod状态给提名器 和队列没有直接关系 framework.PodNominator // 向队列中添加待调度的Pod Add(logger klog.Logger, pod *v1.Pod) // 添加Pod到ActiveQ Activate(logger klog.Logger, pods map[string]*v1.Pod) // 把无法调度的Pod加回调度队列 和schedulingHint策略相关 AddUnschedulableIfNotPresent(logger klog.Logger, pod *framework.QueuedPodInfo, podSchedulingCycle int64) error // 调度周期 每Pop一次算一个周期 SchedulingCycle() int64 // 从队头弹出Pod Pop(logger klog.Logger) (*framework.QueuedPodInfo, error) // 标记一个Pod处理完成 Done(types.UID) // 更新Pod Update(logger klog.Logger, oldPod, newPod *v1.Pod) // 删除Pod Delete(pod *v1.Pod) // 把所有不可调度Pod移动到ActiveQ或BackoffQ MoveAllToActiveOrBackoffQueue(logger klog.Logger, event framework.ClusterEvent, oldObj, newObj interface{}, preCheck PreEnqueueCheck) // 关联Pod被添加 AssignedPodAdded(logger klog.Logger, pod *v1.Pod) // 关联Pod被更新 AssignedPodUpdated(logger klog.Logger, oldPod, newPod *v1.Pod, event framework.ClusterEvent) // 关闭调度队列 Close() // 启动调度队列 Run(logger klog.Logger) } 优先队列 在Kubernetes中实现了SchedulingQueue的类型是PriorityQueue优先队列，从它直接实现了SchedulingQueue接口，又通过SchedulingQueue间接实现了PodActivator，PodNominator两种接口，以上可以看出优先队列具有的能力包括：调度队列的基本能力、把一个Pod加入ActiveQ的能力和处理NominatedPod的能力。 type PriorityQueue struct { // 提名器 *nominator // 接收停止信号的通道 stop chan struct{} // 时钟 clock clock.Clock // 锁 lock sync.RWMutex // Pod初始退避时长 podInitialBackoffDuration time.Duration // Pod最大退避时长 podMaxBackoffDuration time.Duration // Pod在unschedulablePods中的最大时长 podMaxInUnschedulablePodsDuration time.Duration // ActiveQ activeQ activeQueuer // BackoffQ podBackoffQ *heap.Heap[*framework.QueuedPodInfo] // unschedulablePods unschedulablePods *UnschedulablePods // preEnqueue插件列表 preEnqueuePluginMap map[string][]framework.PreEnqueuePlugin // key是profil value是QueueingHintFunction列表 queueingHintMap QueueingHintMapPerProfile // 命名空间Lister nsLister listersv1.NamespaceLister // 指标记录器 metricsRecorder metrics.MetricAsyncRecorder // 插件指标采样百分比 pluginMetricsSamplePercent int // SchedulingQueueHint特性门控开关 isSchedulingQueueHintEnabled bool } 优先队列的实现 了解过调度队列的话，一定会听说过存在着三种队列：ActiveQ，BackoffQ，unschedulablePods。 首先通过PriorityQueue的结构，可以看到ActiveQ和BackoffQ底层都是heap.Heap，来看一下Heap到底是什么。 泛型结构体Heap的定义位于pkg/scheduler/backend/heap/heap.go中，其中只包括自定义堆中元素data和指标记录器。 type Heap[T any] struct { // data stores objects and has a queue that keeps their ordering according // to the heap invariant. data *data[T] // metricRecorder updates the counter when elements of a heap get added or // removed, and it does nothing if it's nil metricRecorder metrics.MetricRecorder } 再看堆元素data是如何实现的，堆的设计天然维护了其中元素的顺序，所以ActiveQ和BackoffQ实际上是两个优先队列。 type data[T any] struct { // 存储堆中元素的表 用于快速索引 items map[string]*heapItem[T] // 优先队列 维护了元素的顺序 queue []string // 生成item key的规则 keyFunc KeyFunc[T] // 堆元素排序的规则 lessFunc LessFunc[T] } 再来看UnschedulablePods类型，可以看到这个队列实际只是一个Map结构，其中的元素不具有顺序，严格来说不是一个队列。 type UnschedulablePods struct { // 记录Pod信息的表 key是Pod的full-name value是指针 podInfoMap map[string]*framework.QueuedPodInfo // key生成函数 keyFunc func(*v1.Pod) string // 指标记录器 unschedulableRecorder, gatedRecorder metrics.MetricRecorder } // keyFunc函数的实现 实际上就是Pod名称和namespace的拼接 func GetPodFullName(pod *v1.Pod) string { return pod.Name + \"_\" + pod.Namespace } 调度队列的几种方法 Run Run()方法的作用是启动两个goroutine，一个goroutine每秒执行一次把BackoffQ中超过退避时间的Pod移动到ActiveQ中，另一个goroutine每30秒执行一次把unschedulablePods中已到期的Pod根据一定的策略刷新其在调度队列中的位置。 // Run starts the goroutine to pump from podBackoffQ to activeQ func (p *PriorityQueue) Run(logger klog.Logger) { // 启动周期任务goroutine 把BackoffQ中完成退避的Pod移动到ActiveQ go wait.Until(func() { p.flushBackoffQCompleted(logger) }, 1.0*time.Second, p.stop) // 启动周期任务goroutine 把unschedulablePods中已到期的Pod移动到ActiveQ/BackoffQ go wait.Until(func() { p.flushUnschedulablePodsLeftover(logger) }, 30*time.Second, p.stop) } // flushBackoffQCompleted Moves all pods from backoffQ which have completed backoff in to activeQ func (p *PriorityQueue) flushBackoffQCompleted(logger klog.Logger) { p.lock.Lock() defer p.lock.Unlock() // 标志位 表示是否有Pod被移动 activated := false for { // 看队首是否有元素 如果空队列直接退出 pInfo, ok := p.podBackoffQ.Peek() if !ok || pInfo == nil { break } pod := pInfo.Pod // 比较backoffTime即(Timestamp+duration)和当前时间 //没完成退避就退出 因为堆顶元素是退避完成时间最早的 if p.isPodBackingoff(pInfo) { break } _, err := p.podBackoffQ.Pop() if err != nil { logger.Error(err, \"Unable to pop pod from backoff queue despite backoff completion\", \"pod\", klog.KObj(pod)) break } // 移动该Pod到ActiveQ if added := p.moveToActiveQ(logger, pInfo, framework.BackoffComplete); added { // 更新标志位 activated = true } } if activated { // 广播唤醒所有等待对ActiveQ执行Pop的goroutine p.activeQ.broadcast() } } func (p *PriorityQueue) moveToActiveQ(logger klog.Logger, pInfo *framework.QueuedPodInfo, event string) bool { gatedBefore := pInfo.Gated // 运行PreEnqueue插件 pInfo.Gated = !p.runPreEnqueuePlugins(context.Background(), pInfo) // 添加到ActiveQ的标志位 added := false p.activeQ.underLock(func(unlockedActiveQ unlockedActiveQueuer) { // 如果PreEnqueue插件运行没通过 加到unschedulablePods中 if pInfo.Gated { // Add the Pod to unschedulablePods if it's not passing PreEnqueuePlugins. if unlockedActiveQ.Has(pInfo) { return } if p.podBackoffQ.Has(pInfo) { return } p.unschedulablePods.addOrUpdate(pInfo) return } if pInfo.InitialAttemptTimestamp == nil { now := p.clock.Now() pInfo.InitialAttemptTimestamp = &now } // 运行通过了就加到ActiveQ unlockedActiveQ.AddOrUpdate(pInfo) // 更新标志位 added = true // 从BackoffQ和unschedulablePods删除 保证Pod信息在队列中的唯一性 p.unschedulablePods.delete(pInfo.Pod, gatedBefore) _ = p.podBackoffQ.Delete(pInfo) logger.V(5).Info(\"Pod moved to an internal scheduling queue\", \"pod\", klog.KObj(pInfo.Pod), \"event\", event, \"queue\", activeQ) metrics.SchedulerQueueIncomingPods.WithLabelValues(\"active\", event).Inc() if event == framework.EventUnscheduledPodAdd.Label() || event == framework.EventUnscheduledPodUpdate.Label() { // 置空Pod的提名节点 p.AddNominatedPod(logger, pInfo.PodInfo, nil) } }) return added } // flushUnschedulablePodsLeftover moves pods which stay in unschedulablePods // longer than podMaxInUnschedulablePodsDuration to backoffQ or activeQ. func (p *PriorityQueue) flushUnschedulablePodsLeftover(logger klog.Logger) { p.lock.Lock() defer p.lock.Unlock() var podsToMove []*framework.QueuedPodInfo currentTime := p.clock.Now() // 遍历unschedulablePods 找出所有超出最大停留时长的Pod for _, pInfo := range p.unschedulablePods.podInfoMap { lastScheduleTime := pInfo.Timestamp if currentTime.Sub(lastScheduleTime) > p.podMaxInUnschedulablePodsDuration { podsToMove = append(podsToMove, pInfo) } } if len(podsToMove) > 0 { // 把这些Pod更新到ActiveQ或BackoffQ p.movePodsToActiveOrBackoffQueue(logger, podsToMove, framework.EventUnschedulableTimeout, nil, nil) } } func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(logger klog.Logger, podInfoList []*framework.QueuedPodInfo, event framework.ClusterEvent, oldObj, newObj interface{}) { if !p.isEventOfInterest(logger, event) { // No plugin is interested in this event. return } activated := false for _, pInfo := range podInfoList { if pInfo.Gated && pInfo.UnschedulablePlugins.Has(names.SchedulingGates) { continue } // 判断schedulingHint来决定入队方式 schedulingHint := p.isPodWorthRequeuing(logger, pInfo, event, oldObj, newObj) if schedulingHint == queueSkip { logger.V(5).Info(\"Event is not making pod schedulable\", \"pod\", klog.KObj(pInfo.Pod), \"event\", event.Label()) continue } // 先从unschedulablePods删除 p.unschedulablePods.delete(pInfo.Pod, pInfo.Gated) // 再根据schedulingHint执行入队操作 并返回新的队列 queue := p.requeuePodViaQueueingHint(logger, pInfo, schedulingHint, event.Label()) logger.V(4).Info(\"Pod moved to an internal scheduling queue\", \"pod\", klog.KObj(pInfo.Pod), \"event\", event.Label(), \"queue\", queue, \"hint\", schedulingHint) // 如果进入activeQ 后续广播唤醒阻塞goroutine if queue == activeQ { activated = true } } p.moveRequestCycle = p.activeQ.schedulingCycle() if p.isSchedulingQueueHintEnabled { if added := p.activeQ.addEventIfAnyInFlight(oldObj, newObj, event); added { logger.V(5).Info(\"Event received while pods are in flight\", \"event\", event.Label()) } } if activated { p.activeQ.broadcast() } } Add 有新的Pod创建时是一个v1.Pod类型的指针，Add()方法把它转换为PodInfo在调度队列中的形态QueuedPodInfo，并通过QueuedPodInfo方法把该Pod信息加入到ActiveQ。 func (p *PriorityQueue) Add(logger klog.Logger, pod *v1.Pod) { p.lock.Lock() defer p.lock.Unlock() // Pod信息类型转换 pInfo := p.newQueuedPodInfo(pod) // 加入到ActiveQ if added := p.moveToActiveQ(logger, pInfo, framework.EventUnscheduledPodAdd.Label()); added { // 成功加入后唤醒其他协程 p.activeQ.broadcast() } } Update 如果Pod属性发生变化，考虑几种场景：是否开启SchedulingQueueHint，oldPod是否存在于在调度队列中(ActiveQ/BackoffQ/unschedulablePods)。从unschedulablePods中尝试移动时，SchedulingQueueHint特性门控的开/关分别是两个逻辑路径，如果特性门控开启则根据SchedulingQueueHint的值决定入队方式，否则一般依次尝试。 func (p *PriorityQueue) Update(logger klog.Logger, oldPod, newPod *v1.Pod) { p.lock.Lock() defer p.lock.Unlock() var events []framework.ClusterEvent // 如果开启了SchedulingQueueHint if p.isSchedulingQueueHintEnabled { events = framework.PodSchedulingPropertiesChange(newPod, oldPod) // 如果Pod处于InFlight(处理中)状态 就直接返回 if exists := p.activeQ.addEventsIfPodInFlight(oldPod, newPod, events); exists { logger.V(6).Info(\"The pod doesn't be queued for now because it's being scheduled and will be queued back if necessary\", \"pod\", klog.KObj(newPod)) return } } // 如果oldPod不为空时 if oldPod != nil { // 把v1.Pod先转换成QueuedPodInfo oldPodInfo := newQueuedPodInfoForLookup(oldPod) // 如果在ActiveQ中存在 // activeQ.update内部实现与在其他队列中更新完全相同 // 都是执行pInfo.Update + AddOrUpdate if pInfo := p.activeQ.update(newPod, oldPodInfo); pInfo != nil { // 在调度队列中处理后 也尝试在提名器中更新 p.UpdateNominatedPod(logger, oldPod, pInfo.PodInfo) return } // 如果在BackoffQ中存在 if pInfo, exists := p.podBackoffQ.Get(oldPodInfo); exists { // 更新PodInfo _ = pInfo.Update(newPod) // 尝试在提名器中更新 p.UpdateNominatedPod(logger, oldPod, pInfo.PodInfo) // 和activeQ.update实际逻辑相同 p.podBackoffQ.AddOrUpdate(pInfo) return } } // 如果在unschedulablePods中存在 if pInfo := p.unschedulablePods.get(newPod); pInfo != nil { _ = pInfo.Update(newPod) p.UpdateNominatedPod(logger, oldPod, pInfo.PodInfo) gated := pInfo.Gated // 如果SchedulingQueueHint特性门控打开 尝试重新入队 if p.isSchedulingQueueHintEnabled { for _, evt := range events { hint := p.isPodWorthRequeuing(logger, pInfo, evt, oldPod, newPod) queue := p.requeuePodViaQueueingHint(logger, pInfo, hint, evt.Label()) if queue != unschedulablePods { logger.V(5).Info(\"Pod moved to an internal scheduling queue because the Pod is updated\", \"pod\", klog.KObj(newPod), \"event\", evt.Label(), \"queue\", queue) p.unschedulablePods.delete(pInfo.Pod, gated) } if queue == activeQ { p.activeQ.broadcast() break } } return } // 如果特性门控没有打开 // 判断新旧PodInfo是否不同 if isPodUpdated(oldPod, newPod) { // 如果当前在BackingoffQ中 尝试在BackingoffQ中更新并从unschedulablePods删除 if p.isPodBackingoff(pInfo) { p.podBackoffQ.AddOrUpdate(pInfo) p.unschedulablePods.delete(pInfo.Pod, gated) logger.V(5).Info(\"Pod moved to an internal scheduling queue\", \"pod\", klog.KObj(pInfo.Pod), \"event\", framework.EventUnscheduledPodUpdate.Label(), \"queue\", backoffQ) return } // 否则尝试移动到ActiveQ if added := p.moveToActiveQ(logger, pInfo, framework.BackoffComplete); added { p.activeQ.broadcast() } return } // 没开启特性门控且没有成功移动到别的队列 // 在当前队列直接更新PodInfo并返回 p.unschedulablePods.addOrUpdate(pInfo) return } // 其他情况下 创建新的PodInfo 然后加入activeQ pInfo := p.newQueuedPodInfo(newPod) if added := p.moveToActiveQ(logger, pInfo, framework.EventUnscheduledPodUpdate.Label()); added { p.activeQ.broadcast() } } // PodSchedulingPropertiesChange interprets the update of a pod and returns corresponding UpdatePodXYZ event(s). // Once we have other pod update events, we should update here as well. func PodSchedulingPropertiesChange(newPod *v1.Pod, oldPod *v1.Pod) (events []ClusterEvent) { // 初始化并更新Pod状态 r := assignedPod if newPod.Spec.NodeName == \"\" { r = unschedulablePod } // 定义一个切片 其中是提取Pod信息的函数列表 podChangeExtracters := []podChangeExtractor{ extractPodLabelsChange, extractPodScaleDown, extractPodSchedulingGateEliminatedChange, extractPodTolerationChange, } // 如果DRA特性门控开启 增加一个相关函数 if utilfeature.DefaultFeatureGate.Enabled(features.DynamicResourceAllocation) { podChangeExtracters = append(podChangeExtracters, extractPodGeneratedResourceClaimChange) } // 遍历列表中的提取器 for _, fn := range podChangeExtracters { // 获取每个提取器的事件event 并组装成ClusterEvent加入到events切片 if event := fn(newPod, oldPod); event != none { // 注意下细节 events切片是在函数声明时就创建了的 events = append(events, ClusterEvent{Resource: r, ActionType: event}) } } if len(events) == 0 { events = append(events, ClusterEvent{Resource: r, ActionType: updatePodOther}) } return } // update updates the pod in activeQ if oldPodInfo is already in the queue. // It returns new pod info if updated, nil otherwise. func (aq *activeQueue) update(newPod *v1.Pod, oldPodInfo *framework.QueuedPodInfo) *framework.QueuedPodInfo { aq.lock.Lock() defer aq.lock.Unlock() // 判断ActiveQ中是否存在oldPodInfo 如果存在就更新信息 if pInfo, exists := aq.queue.Get(oldPodInfo); exists { // 更新Pod信息 _ = pInfo.Update(newPod) // 更新ActiveQ堆中信息 aq.queue.AddOrUpdate(pInfo) return pInfo } return nil } // 如果存在相同对象 直接用新的pod替换 否则就创建一个新的Pod并提取其亲和性信息 func (pi *PodInfo) Update(pod *v1.Pod) error { if pod != nil && pi.Pod != nil && pi.Pod.UID == pod.UID { // 有相同Pod 直接更新 pi.Pod = pod return nil } // 如果没有 解析亲和性信息 var preferredAffinityTerms []v1.WeightedPodAffinityTerm var preferredAntiAffinityTerms []v1.WeightedPodAffinityTerm if affinity := pod.Spec.Affinity; affinity != nil { if a := affinity.PodAffinity; a != nil { preferredAffinityTerms = a.PreferredDuringSchedulingIgnoredDuringExecution } if a := affinity.PodAntiAffinity; a != nil { preferredAntiAffinityTerms = a.PreferredDuringSchedulingIgnoredDuringExecution } } // Attempt to parse the affinity terms var parseErrs []error requiredAffinityTerms, err := GetAffinityTerms(pod, GetPodAffinityTerms(pod.Spec.Affinity)) if err != nil { parseErrs = append(parseErrs, fmt.Errorf(\"requiredAffinityTerms: %w\", err)) } requiredAntiAffinityTerms, err := GetAffinityTerms(pod, GetPodAntiAffinityTerms(pod.Spec.Affinity)) if err != nil { parseErrs = append(parseErrs, fmt.Errorf(\"requiredAntiAffinityTerms: %w\", err)) } weightedAffinityTerms, err := getWeightedAffinityTerms(pod, preferredAffinityTerms) if err != nil { parseErrs = append(parseErrs, fmt.Errorf(\"preferredAffinityTerms: %w\", err)) } weightedAntiAffinityTerms, err := getWeightedAffinityTerms(pod, preferredAntiAffinityTerms) if err != nil { parseErrs = append(parseErrs, fmt.Errorf(\"preferredAntiAffinityTerms: %w\", err)) } // 创建一个新对象并返回聚合的错误信息 pi.Pod = pod pi.RequiredAffinityTerms = requiredAffinityTerms pi.RequiredAntiAffinityTerms = requiredAntiAffinityTerms pi.PreferredAffinityTerms = weightedAffinityTerms pi.PreferredAntiAffinityTerms = weightedAntiAffinityTerms return utilerrors.NewAggregate(parseErrs) } // UpdateNominatedPod updates the with . func (npm *nominator) UpdateNominatedPod(logger klog.Logger, oldPod *v1.Pod, newPodInfo *framework.PodInfo) { npm.nLock.Lock() defer npm.nLock.Unlock() // In some cases, an Update event with no \"NominatedNode\" present is received right // after a node(\"NominatedNode\") is reserved for this pod in memory. // In this case, we need to keep reserving the NominatedNode when updating the pod pointer. var nominatingInfo *framework.NominatingInfo // We won't fall into below `if` block if the Update event represents: // (1) NominatedNode info is added // (2) NominatedNode info is updated // (3) NominatedNode info is removed if nominatedNodeName(oldPod) == \"\" && nominatedNodeName(newPodInfo.Pod) == \"\" { if nnn, ok := npm.nominatedPodToNode[oldPod.UID]; ok { // This is the only case we should continue reserving the NominatedNode nominatingInfo = &framework.NominatingInfo{ NominatingMode: framework.ModeOverride, NominatedNodeName: nnn, } } } // 先删除再添加 npm.deleteUnlocked(oldPod) npm.addNominatedPodUnlocked(logger, newPodInfo, nominatingInfo) } // 删除NominatedPod func (npm *nominator) deleteUnlocked(p *v1.Pod) { // 找到提名器中Pod对应的Node nnn, ok := npm.nominatedPodToNode[p.UID] if !ok { return } // 遍历这个Node上的提名节点 如果和当前一致则从Pod列表中删除 for i, np := range npm.nominatedPods[nnn] { if np.uid == p.UID { npm.nominatedPods[nnn] = append(npm.nominatedPods[nnn][:i], npm.nominatedPods[nnn][i+1:]...) if len(npm.nominatedPods[nnn]) == 0 { delete(npm.nominatedPods, nnn) } break } } // 删除NominatedPod到Node的映射 delete(npm.nominatedPodToNode, p.UID) } func (npm *nominator) addNominatedPodUnlocked(logger klog.Logger, pi *framework.PodInfo, nominatingInfo *framework.NominatingInfo) { // 先删再加 保证唯一 npm.deleteUnlocked(pi.Pod) var nodeName string if nominatingInfo.Mode() == framework.ModeOverride { nodeName = nominatingInfo.NominatedNodeName } else if nominatingInfo.Mode() == framework.ModeNoop { if pi.Pod.Status.NominatedNodeName == \"\" { return } nodeName = pi.Pod.Status.NominatedNodeName } if npm.podLister != nil { // If the pod was removed or if it was already scheduled, don't nominate it. updatedPod, err := npm.podLister.Pods(pi.Pod.Namespace).Get(pi.Pod.Name) if err != nil { logger.V(4).Info(\"Pod doesn't exist in podLister, aborted adding it to the nominator\", \"pod\", klog.KObj(pi.Pod)) return } if updatedPod.Spec.NodeName != \"\" { logger.V(4).Info(\"Pod is already scheduled to a node, aborted adding it to the nominator\", \"pod\", klog.KObj(pi.Pod), \"node\", updatedPod.Spec.NodeName) return } } npm.nominatedPodToNode[pi.Pod.UID] = nodeName for _, np := range npm.nominatedPods[nodeName] { if np.uid == pi.Pod.UID { logger.V(4).Info(\"Pod already exists in the nominator\", \"pod\", np.uid) return } } npm.nominatedPods[nodeName] = append(npm.nominatedPods[nodeName], podToRef(pi.Pod)) } Delete 尝试从所有调度队列中删除该Pod的信息。 func (p *PriorityQueue) Delete(pod *v1.Pod) { p.lock.Lock() defer p.lock.Unlock() p.DeleteNominatedPodIfExists(pod) pInfo := newQueuedPodInfoForLookup(pod) if err := p.activeQ.delete(pInfo); err != nil { // The item was probably not found in the activeQ. p.podBackoffQ.Delete(pInfo) if pInfo = p.unschedulablePods.get(pod); pInfo != nil { p.unschedulablePods.delete(pod, pInfo.Gated) } } } Activate 激活一个Pod集合，即把它们全部移动到ActiveQ中。 func (p *PriorityQueue) Activate(logger klog.Logger, pods map[string]*v1.Pod) { p.lock.Lock() defer p.lock.Unlock() activated := false for _, pod := range pods { // 移动Pod到ActiveQ 如果成功移动了返回True 不存在或移动失败返回False表示该Pod正在处理中 if p.activate(logger, pod) { activated = true continue } // 如果是一个in-flight Pod // 记录激活事件并更新moveRequestCycle字段与当前周期一致 p.activeQ.addEventsIfPodInFlight(nil, pod, []framework.ClusterEvent{framework.EventForceActivate}) p.moveRequestCycle = p.activeQ.schedulingCycle() } // 如果激活成功 唤醒其他等待协程 if activated { p.activeQ.broadcast() } } 调度队列小结及流程图 调度队列实际上都是Map，以PodName_Namespace为key，PodInfo的指针为value来进行存储，和unschedulablePods的区别在于是否通过Slice维护了优先顺序，Map的key和Slice的排序提高了查询和出队的速度。 每次有Pod加入ActiveQ，都会通过broadcast()去唤醒等待中的协程，因为如果一个线程要调用Pop()方法时会先判断队列长度，如果队列为空时通过执行cond.Wait()挂起进程。 退避队列每秒刷新一次，失败队列每三十秒刷新一次。 在调度队列中的所有Pod都处于的是Pending状态。 如果一个Pod调度成功，会通过AssignedPodAdded()方法尝试把unschedulablePods中相关的Pod移动到其他两个队列;如果一个Pod调度失败，会通过AddUnschedulableIfNotPresent方法把该Pod重新放回队列。 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/02-调度器缓存Cache的实现.html":{"url":"kube-scheduler/02-调度器缓存Cache的实现.html","title":"调度器缓存Cache的实现","keywords":"","body":" 定义与实现 相关数据结构 podState nodeInfoListItem nodeTree 相关方法 New Dump UpdateSnapshot AssumePod ForgetPod updatePod Cache中数据的同步 Pod在缓存中的流程图 调度器缓存Cache的实现 在正式进入调度器的工作流程之前，先再来了解一下Cache概念。 如果对Informer机制有一定的了解，会知道其中Indexer组件作为集群信息的Local Storage，但是在Pod调度过程中，其实使用的是调度器实现的由Cache专门提供的集群Node-Pod快照信息，下面一起看Cache的设计和实现。 需要注意的是，client-go中的Cache和此处提到的调度器Cache并不是一个。client-go中的缓存依赖List-Watch机制，虽然调度器中的缓存也依赖Informer，但缓存中的数据是不同的，如client-go中的缓存的是Kubernetes的API对象，而调度器的缓存主要保存了Node和Pod的映射和聚合信息。 定义与实现 Cache接口定义位于代码路径pkg/scheduler/backend/cache/interface.go下，此处有一个小的概念，当Pod的调度周期成功完成后，异步的绑定周期完成前，调度器会假定这个Pod会正常完成绑定，此时该Pod成为AssumePod，它所使用的资源通过reserve插件预留，在缓存中也认为该Pod会成功上线，并在cacheImpl中使用assumedPods集合存储这个状态的Pod。 type Cache interface { // 单测使用 NodeCount() int // 单测使用 PodCount() (int, error) // AssumePod是处于scheduling结束但还没binding完成状态的Pod // 调度器对调度的Pod做出乐观预期 把成功找到调度节点Pod将占用的资源也纳入计算范围 AssumePod(logger klog.Logger, pod *v1.Pod) error // AssumePod绑定成功后使用次方法通知Cache FinishBinding(logger klog.Logger, pod *v1.Pod) error // AssumePod绑定失败后使用次方法通知Cache ForgetPod(logger klog.Logger, pod *v1.Pod) error // 如果是AssumePod则确认该Pod 实际是执行UpdatePod操作 // 如果没有在已有信息中找到这个Pod 就把它添加到缓存 AddPod(logger klog.Logger, pod *v1.Pod) error // 更新Pod信息 先删除后添加 UpdatePod(logger klog.Logger, oldPod, newPod *v1.Pod) error // 把Pod从Cache的podStates、assumedPods以及cache.nodes中移除 RemovePod(logger klog.Logger, pod *v1.Pod) error // 获取Pod对象 GetPod(pod *v1.Pod) (*v1.Pod, error) // 判断Pod是否假定调度 IsAssumedPod(pod *v1.Pod) (bool, error) // 添加Node 返回Node克隆对象 AddNode(logger klog.Logger, node *v1.Node) *framework.NodeInfo // 更新Node 返回Node克隆对象 UpdateNode(logger klog.Logger, oldNode, newNode *v1.Node) *framework.NodeInfo // 删除Node RemoveNode(logger klog.Logger, node *v1.Node) error // 更新快照 UpdateSnapshot(logger klog.Logger, nodeSnapshot *Snapshot) error // 根据当前缓存生成快照 Dump() *Dump } 其中的AddPod/UpdatePod/RemovePod都与Informer相关，由其事件回调函数中使用，如下部分代码所示。 // scheduled pod cache if handlerRegistration, err = informerFactory.Core().V1().Pods().Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: return assignedPod(t) case cache.DeletedFinalStateUnknown: if _, ok := t.Obj.(*v1.Pod); ok { return true } utilruntime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, sched)) return false default: utilruntime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", sched, obj)) return false } }, // 注册回调函数 Handler: cache.ResourceEventHandlerFuncs{ AddFunc: sched.addPodToCache, UpdateFunc: sched.updatePodInCache, DeleteFunc: sched.deletePodFromCache, }, }, ); err != nil { return err } cacheImpl是Cache接口的实现，在代码路径pkg/scheduler/backend/cache/cache.go中定义。 type cacheImpl struct { // 停止channel stop 相关数据结构 podState Pod在缓存中是以namespace-podname为key，podState为value的形式存储的。 type podState struct { // Pod对象 pod *v1.Pod // 用于AssumedPod 如果在ddl之前没有完成bind则认为该AssumedPod过期 deadline *time.Time // 绑定成功标志 bindingFinished bool } nodeInfoListItem nodeInfoListItem是存储节点信息的双向链表。 type nodeInfoListItem struct { // NodeInfo info *framework.NodeInfo // 双向链表指针 next *nodeInfoListItem // 双向链表指针 prev *nodeInfoListItem } nodeTree nodeTree是根据zone划分的节点集合。 type nodeTree struct { // key是zone value是NodeName tree map[string][]string // key的列表 zones []string // 总节点数 numNodes int } 相关方法 代码逻辑都比较清晰，故仅在代码片段中简单注释。 New 启动一个Cache实例，代码实现如下。 func New(ctx context.Context, ttl time.Duration) Cache { logger := klog.FromContext(ctx) // 创建Cache cleanAssumedPeriod cache := newCache(ctx, ttl, cleanAssumedPeriod) cache.run(logger) return cache } func newCache(ctx context.Context, ttl, period time.Duration) *cacheImpl { logger := klog.FromContext(ctx) return &cacheImpl{ ttl: ttl, period: period, stop: ctx.Done(), nodes: make(map[string]*nodeInfoListItem), nodeTree: newNodeTree(logger, nil), assumedPods: sets.New[string](), podStates: make(map[string]*podState), imageStates: make(map[string]*framework.ImageStateSummary), } } func (cache *cacheImpl) run(logger klog.Logger) { // 起一个goroutine 定期清理AssumedPod go wait.Until(func() { cache.cleanupAssumedPods(logger, time.Now()) }, cache.period, cache.stop) } func (cache *cacheImpl) cleanupAssumedPods(logger klog.Logger, now time.Time) { cache.mu.Lock() defer cache.mu.Unlock() defer cache.updateMetrics() // 遍历AssumedPod for key := range cache.assumedPods { // 获取podStatus ps, ok := cache.podStates[key] if !ok { logger.Error(nil, \"Key found in assumed set but not in podStates, potentially a logical error\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } // 还在binding处理中则跳过当前Pod if !ps.bindingFinished { logger.V(5).Info(\"Could not expire cache for pod as binding is still in progress\", \"podKey\", key, \"pod\", klog.KObj(ps.pod)) continue } // 不是常驻Pod且当前时间超过了AssumedPod的ddl if cache.ttl != 0 && now.After(*ps.deadline) { logger.Info(\"Pod expired\", \"podKey\", key, \"pod\", klog.KObj(ps.pod)) // 移除过期的AssumedPod if err := cache.removePod(logger, ps.pod); err != nil { logger.Error(err, \"ExpirePod failed\", \"podKey\", key, \"pod\", klog.KObj(ps.pod)) } } } } func (cache *cacheImpl) removePod(logger klog.Logger, pod *v1.Pod) error { key, err := framework.GetPodKey(pod) if err != nil { return err } n, ok := cache.nodes[pod.Spec.NodeName] if !ok { logger.Error(nil, \"Node not found when trying to remove pod\", \"node\", klog.KRef(\"\", pod.Spec.NodeName), \"podKey\", key, \"pod\", klog.KObj(pod)) } else { // 从NodeInfo中移除过期AssumedPod if err := n.info.RemovePod(logger, pod); err != nil { return err } // 如果移除后 对应NodeInfo已经没有Pod了 // 顺便把这个Node从缓存中删除 if len(n.info.Pods) == 0 && n.info.Node() == nil { cache.removeNodeInfoFromList(logger, pod.Spec.NodeName) } else { // 删除后Node中还有Pod 移动这个节点到Head cache.moveNodeInfoToHead(logger, pod.Spec.NodeName) } } // 从缓存中删除过期AssumedPod delete(cache.podStates, key) delete(cache.assumedPods, key) return nil } Dump 主要用于调试，属于CacheDebugger的一部分，通常把Cache中的信息记录到日志，从而定位调度过程中的问题。 func (cache *cacheImpl) Dump() *Dump { cache.mu.RLock() defer cache.mu.RUnlock() nodes := make(map[string]*framework.NodeInfo, len(cache.nodes)) for k, v := range cache.nodes { // Snapshot返回一个NodeInfo的克隆对象 nodes[k] = v.info.Snapshot() } return &Dump{ Nodes: nodes, AssumedPods: cache.assumedPods.Union(nil), } } // CacheDebugger provides ways to check and write cache information for debugging. type CacheDebugger struct { Comparer CacheComparer Dumper CacheDumper } UpdateSnapshot 把Cache的信息更新到一个Snapshot快照结构，每一轮调度Pod时都会使用此方法更新快照。 func (cache *cacheImpl) UpdateSnapshot(logger klog.Logger, nodeSnapshot *Snapshot) error { cache.mu.Lock() defer cache.mu.Unlock() // 获取上次的快照编号 snapshotGeneration := nodeSnapshot.generation // 初始化更新标志位 updateAllLists := false updateNodesHavePodsWithAffinity := false updateNodesHavePodsWithRequiredAntiAffinity := false updateUsedPVCSet := false // 从headNode开始遍历节点信息并更新快照 for node := cache.headNode; node != nil; node = node.next { // 如果当次节点编号不大于快照编号就退出遍历 // 因为Cache中的节点是一个双向链表 最近被更新过的节点信息在前面 // 如果前面的节点都没有比快照更新 那后面也不会有更新 if node.info.Generation 0) != (len(clone.PodsWithAffinity) > 0) { updateNodesHavePodsWithAffinity = true } // 判断亲和性信息是否需要更新 如果缓存和快照中的列表长度不一致 updateNodesHavePodsWithRequiredAntiAffinity标志位置为True if (len(existing.PodsWithRequiredAntiAffinity) > 0) != (len(clone.PodsWithRequiredAntiAffinity) > 0) { updateNodesHavePodsWithRequiredAntiAffinity = true } // 判断PVC集合是否需要更新 如果有就把updateUsedPVCSet标志位置为True if !updateUsedPVCSet { if len(existing.PVCRefCounts) != len(clone.PVCRefCounts) { updateUsedPVCSet = true } else { for pvcKey := range clone.PVCRefCounts { if _, found := existing.PVCRefCounts[pvcKey]; !found { updateUsedPVCSet = true break } } } } // 覆盖原有信息 *existing = *clone } } // 用最新的编号给快照编号赋值 if cache.headNode != nil { nodeSnapshot.generation = cache.headNode.info.Generation } // 如果快照中的节点数量大于当前缓存中的节点数量 移除已被删除的节点 if len(nodeSnapshot.nodeInfoMap) > cache.nodeTree.numNodes { cache.removeDeletedNodesFromSnapshot(nodeSnapshot) updateAllLists = true } // 如果表示有更新项 执行具体函数更新 if updateAllLists || updateNodesHavePodsWithAffinity || updateNodesHavePodsWithRequiredAntiAffinity || updateUsedPVCSet { // 更新快照信息 cache.updateNodeInfoSnapshotList(logger, nodeSnapshot, updateAllLists) } // 检查结果的一致性 如果不一致再次执行updateNodeInfoSnapshotList函数更新 if len(nodeSnapshot.nodeInfoList) != cache.nodeTree.numNodes { errMsg := fmt.Sprintf(\"snapshot state is not consistent, length of NodeInfoList=%v not equal to length of nodes in tree=%v \"+ \", length of NodeInfoMap=%v, length of nodes in cache=%v\"+ \", trying to recover\", len(nodeSnapshot.nodeInfoList), cache.nodeTree.numNodes, len(nodeSnapshot.nodeInfoMap), len(cache.nodes)) logger.Error(nil, errMsg) // 更新快照信息 cache.updateNodeInfoSnapshotList(logger, nodeSnapshot, true) return errors.New(errMsg) } return nil } func (cache *cacheImpl) updateNodeInfoSnapshotList(logger klog.Logger, snapshot *Snapshot, updateAll bool) { // 初始化列表和集合 snapshot.havePodsWithAffinityNodeInfoList = make([]*framework.NodeInfo, 0, cache.nodeTree.numNodes) snapshot.havePodsWithRequiredAntiAffinityNodeInfoList = make([]*framework.NodeInfo, 0, cache.nodeTree.numNodes) snapshot.usedPVCSet = sets.New[string]() // updateAll标志位为True时重新构建nodeInfoList 在节点树结构发生变化(添加/删除)时被采用 更新成本更高 // 如果是False 不重新构建nodeInfoList而是基于现有的nodeInfoList进行更新 此时只涉及亲和/反亲和/PVC发生变化的情况 if updateAll { snapshot.nodeInfoList = make([]*framework.NodeInfo, 0, cache.nodeTree.numNodes) nodesList, err := cache.nodeTree.list() if err != nil { logger.Error(err, \"Error occurred while retrieving the list of names of the nodes from node tree\") } for _, nodeName := range nodesList { if nodeInfo := snapshot.nodeInfoMap[nodeName]; nodeInfo != nil { snapshot.nodeInfoList = append(snapshot.nodeInfoList, nodeInfo) if len(nodeInfo.PodsWithAffinity) > 0 { snapshot.havePodsWithAffinityNodeInfoList = append(snapshot.havePodsWithAffinityNodeInfoList, nodeInfo) } if len(nodeInfo.PodsWithRequiredAntiAffinity) > 0 { snapshot.havePodsWithRequiredAntiAffinityNodeInfoList = append(snapshot.havePodsWithRequiredAntiAffinityNodeInfoList, nodeInfo) } for key := range nodeInfo.PVCRefCounts { snapshot.usedPVCSet.Insert(key) } } else { logger.Error(nil, \"Node exists in nodeTree but not in NodeInfoMap, this should not happen\", \"node\", klog.KRef(\"\", nodeName)) } } } else { for _, nodeInfo := range snapshot.nodeInfoList { if len(nodeInfo.PodsWithAffinity) > 0 { snapshot.havePodsWithAffinityNodeInfoList = append(snapshot.havePodsWithAffinityNodeInfoList, nodeInfo) } if len(nodeInfo.PodsWithRequiredAntiAffinity) > 0 { snapshot.havePodsWithRequiredAntiAffinityNodeInfoList = append(snapshot.havePodsWithRequiredAntiAffinityNodeInfoList, nodeInfo) } for key := range nodeInfo.PVCRefCounts { snapshot.usedPVCSet.Insert(key) } } } } AssumePod 假定一个Pod调度成功，把它以AssumedPod的形式添加到节点缓存中。 func (cache *cacheImpl) AssumePod(logger klog.Logger, pod *v1.Pod) error { key, err := framework.GetPodKey(pod) if err != nil { return err } cache.mu.Lock() defer cache.mu.Unlock() if _, ok := cache.podStates[key]; ok { return fmt.Errorf(\"pod %v(%v) is in the cache, so can't be assumed\", key, klog.KObj(pod)) } return cache.addPod(logger, pod, true) } func (cache *cacheImpl) addPod(logger klog.Logger, pod *v1.Pod, assumePod bool) error { key, err := framework.GetPodKey(pod) if err != nil { eturn err } n, ok := cache.nodes[pod.Spec.NodeName] // 如果缓存中不存在对应节点就新创建一个 if !ok { n = newNodeInfoListItem(framework.NewNodeInfo()) cache.nodes[pod.Spec.NodeName] = n } // 缓存的NodeInfo中添加Pod信息 n.info.AddPod(pod) // 移动节点到头部 cache.moveNodeInfoToHead(logger, pod.Spec.NodeName) ps := &podState{ pod: pod, } // podStates中添加Pod信息 cache.podStates[key] = ps if assumePod { // 如果是AssumePod在assumedPods列表中也添加 cache.assumedPods.Insert(key) } return nil } ForgetPod AssumePod过期，将其从调度缓存中移除。 func (cache *cacheImpl) ForgetPod(logger klog.Logger, pod *v1.Pod) error { key, err := framework.GetPodKey(pod) if err != nil { return err } cache.mu.Lock() defer cache.mu.Unlock() currState, ok := cache.podStates[key] if ok && currState.pod.Spec.NodeName != pod.Spec.NodeName { return fmt.Errorf(\"pod %v(%v) was assumed on %v but assigned to %v\", key, klog.KObj(pod), pod.Spec.NodeName, currState.pod.Spec.NodeName) } // Only assumed pod can be forgotten. if ok && cache.assumedPods.Has(key) { return cache.removePod(logger, pod) } return fmt.Errorf(\"pod %v(%v) wasn't assumed so cannot be forgotten\", key, klog.KObj(pod)) } updatePod 更新缓存中Pod信息，先删除后添加能够保证缓存信息的一致性。 func (cache *cacheImpl) updatePod(logger klog.Logger, oldPod, newPod *v1.Pod) error { // 先删除 if err := cache.removePod(logger, oldPod); err != nil { return err } // 再添加 return cache.addPod(logger, newPod, false) } Cache中数据的同步 在调度器启动过程中，代码位于cmd/kube-scheduler/app/server.go，Cache同步的重要代码就在Run()函数中，此处会涉及到Informer机制。但可以提前了解到，在调度器启动之前，通过和Informer的协同，保证了Cache中数据和集群信息的一致性。 startInformersAndWaitForSync := func(ctx context.Context) { // 启动所有Informer cc.InformerFactory.Start(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.Start(ctx.Done()) } // 等待Cache中的数据同步完成 cc.InformerFactory.WaitForCacheSync(ctx.Done()) // DynInformerFactory can be nil in tests. if cc.DynInformerFactory != nil { cc.DynInformerFactory.WaitForCacheSync(ctx.Done()) } // Wait for all handlers to sync (all items in the initial list delivered) before scheduling. if err := sched.WaitForHandlersSync(ctx); err != nil { logger.Error(err, \"waiting for handlers to sync\") } close(handlerSyncReadyCh) logger.V(3).Info(\"Handlers synced\") } 等待缓存同步用到下面的这个函数，但在未来版本会被其他函数替代。 func WaitForCacheSync(stopCh Pod在缓存中的流程图 // State Machine of a pod's events in scheduler's cache: // // // +-------------------------------------------+ +----+ // | Add | | | // | | | | Update // + Assume Add v v | // Initial +--------> Assumed +------------+---> Added Expired +------> Deleted // Forget Expire // © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-17 10:05:06 "},"kube-scheduler/03-Framework框架和调度流程.html":{"url":"kube-scheduler/03-Framework框架和调度流程.html","title":"Framework框架和调度流程","keywords":"","body":" 配置阶段 配置相关的类型梳理 调度阶段 Framework框架和调度流程 由于生产环境中Kubernetes系统的复杂性越来越高，原生default- scheduler无法满足复杂的调度需求，为了避免代码过于庞大和复杂，Kubernetes通过扩展插件Scheduler Extender和多调度器Multiple Schedulers的方式来增强调度器的可扩展性。在1.21版本GA了Scheduler Framework，调度框架解耦了核心流程与插件、通过profile配置支持插件配置和多调度器。 参考官方文档调度器配置，我们可以创建一个配置文件，包含调度过程中所需要的插件列表以及调度器实例。 // 扩展插件 apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - plugins: score: disabled: - name: PodTopologySpread enabled: - name: MyCustomPluginA weight: 2 - name: MyCustomPluginB weight: 1 // 多调度器 apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: default-scheduler - schedulerName: no-scoring-scheduler plugins: preScore: disabled: - name: '*' score: disabled: - name: '*' 配置阶段 回顾调度器配置在创建过程中的变化，在整个过程中的第一件事就是生成并补全配置信息，也就是完善一个Options结构体，整体流程如下，只保留最重要的代码部分以及和Framework相关的结构。 首先在程序入口NewSchedulerCommand处会创建一个options.Options对象，并在后续过程中一直被使用和更改。 // cmd路径下的调度器创建入口 func NewSchedulerCommand(registryOptions ...Option) *cobra.Command { ...... // 新建对象 opts := options.NewOptions() cmd := &cobra.Command{ ...... // 把opt传入runCommand RunE: func(cmd *cobra.Command, args []string) error { return runCommand(cmd, opts, registryOptions...) } ...... } 实际创建逻辑runCommand中，Options对象被用于生成CompletedConfig配置信息和Scheduler实例。 func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Option) error { ...... cc, sched, err := Setup(ctx, opts, registryOptions...) return Run(ctx, cc, sched) } 在Setup以前，Options对象中的配置主要是开关类型，在这个流程中被赋值，然后根据Options对象创建Config类型并传入到调度器实例的创建参数中。 func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) { // 从这里开始给Options对象注入ComponentConfig字段 if cfg, err := latest.Default(); err != nil { return nil, nil, err } else { opts.ComponentConfig = cfg } ...... // 根据Options对象创建Config类型 内部的ApplyTo方法merge了文件配置和默认配置 c, err := opts.Config(ctx) // 封装成CompletedConfig类型 cc := c.Complete() ...... sched, err := scheduler.New(ctx, cc.Client, cc.InformerFactory, cc.DynInformerFactory, recorderFactory, // 在创建调度器实例时传入CompletedConfig scheduler.WithComponentConfigVersion(cc.ComponentConfig.TypeMeta.APIVersion), scheduler.WithKubeConfig(cc.KubeConfig), scheduler.WithProfiles(cc.ComponentConfig.Profiles...), scheduler.WithPercentageOfNodesToScore(cc.ComponentConfig.PercentageOfNodesToScore), ...... ) } New()函数中真正创建了Framework对象，也就是profile.Map类型的集合，并将其赋值给调度器实例。 var defaultSchedulerOptions = schedulerOptions{ percentageOfNodesToScore: schedulerapi.DefaultPercentageOfNodesToScore, podInitialBackoffSeconds: int64(internalqueue.DefaultPodInitialBackoffDuration.Seconds()), podMaxBackoffSeconds: int64(internalqueue.DefaultPodMaxBackoffDuration.Seconds()), podMaxInUnschedulablePodsDuration: internalqueue.DefaultPodMaxInUnschedulablePodsDuration, parallelism: int32(parallelize.DefaultParallelism), applyDefaultProfile: true, } func New(ctx context.Context, client clientset.Interface, informerFactory informers.SharedInformerFactory, dynInformerFactory dynamicinformer.DynamicSharedInformerFactory, recorderFactory profile.RecorderFactory, opts ...Option) (*Scheduler, error) { ...... // 初始化schedulerOptions对象 options := defaultSchedulerOptions for _, opt := range opts { opt(&options) } // New使用了WithProfiles此处为false if options.applyDefaultProfile { var versionedCfg configv1.KubeSchedulerConfiguration scheme.Scheme.Default(&versionedCfg) cfg := schedulerapi.KubeSchedulerConfiguration{} if err := scheme.Scheme.Convert(&versionedCfg, &cfg, nil); err != nil { return nil, err } options.profiles = cfg.Profiles } // 创建Profile的集合 类型为profile.Map profiles, err := profile.NewMap(ctx, options.profiles, registry, recorderFactory, frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion), frameworkruntime.WithClientSet(client), frameworkruntime.WithKubeConfig(options.kubeConfig), ...... ) ...... sched := &Scheduler{ Cache: schedulerCache, client: client, nodeInfoSnapshot: snapshot, percentageOfNodesToScore: options.percentageOfNodesToScore, Extenders: extenders, StopEverything: stopEverything, SchedulingQueue: podQueue, Profiles: profiles, logger: logger, } ...... return sched, nil } NewMap()函数的实现如下，遍历KubeSchedulerProfile对象，生成Framework对象作为value，SchedulerName作为key，组成一个Map对象返回给上层。 type Map map[string]framework.Framework func NewMap(ctx context.Context, cfgs []config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, opts ...frameworkruntime.Option) (Map, error) { m := make(Map) v := cfgValidator{m: m} // 遍历profile for _, cfg := range cfgs { // KubeSchedulerProfile -> Framework p, err := newProfile(ctx, cfg, r, recorderFact, opts...) if err != nil { return nil, fmt.Errorf(\"creating profile for scheduler name %s: %v\", cfg.SchedulerName, err) } if err := v.validate(cfg, p); err != nil { return nil, err } m[cfg.SchedulerName] = p } return m, nil } func newProfile(ctx context.Context, cfg config.KubeSchedulerProfile, r frameworkruntime.Registry, recorderFact RecorderFactory, opts ...frameworkruntime.Option) (framework.Framework, error) { recorder := recorderFact(cfg.SchedulerName) opts = append(opts, frameworkruntime.WithEventRecorder(recorder)) return frameworkruntime.NewFramework(ctx, r, &cfg, opts...) } NewFramework是调度框架的初始化入口，根据KubeSchedulerProfile创建Framework实例，每个Framework对象对应的就是一个调度器的业务逻辑配置。 func NewFramework(ctx context.Context, r Registry, profile *config.KubeSchedulerProfile, opts ...Option) (framework.Framework, error) { options := defaultFrameworkOptions(ctx.Done()) for _, opt := range opts { opt(&options) } logger := klog.FromContext(ctx) if options.logger != nil { logger = *options.logger } // 初始化frameworkImpl实例 // 各种组件通过With作为Option传递并注入 f := &frameworkImpl{ registry: r, snapshotSharedLister: options.snapshotSharedLister, scorePluginWeight: make(map[string]int), waitingPods: options.waitingPods, clientSet: options.clientSet, kubeConfig: options.kubeConfig, eventRecorder: options.eventRecorder, informerFactory: options.informerFactory, sharedDRAManager: options.sharedDRAManager, metricsRecorder: options.metricsRecorder, extenders: options.extenders, PodNominator: options.podNominator, PodActivator: options.podActivator, parallelizer: options.parallelizer, logger: logger, } if len(f.extenders) > 0 { f.enqueueExtensions = []framework.EnqueueExtensions{&defaultEnqueueExtension{pluginName: framework.ExtenderName}} } if profile == nil { return f, nil } f.profileName = profile.SchedulerName f.percentageOfNodesToScore = profile.PercentageOfNodesToScore if profile.Plugins == nil { return f, nil } // 根据配置文件记录启用的插件列表(这个集合与扩展点无关 仅为了确定初始化哪些插件) Set结构天生去重 pg := f.pluginsNeeded(profile.Plugins) pluginConfig := make(map[string]runtime.Object, len(profile.PluginConfig)) // 记录每种插件的自定义配置Args参数列表 用于下面的插件初始化 for i := range profile.PluginConfig { name := profile.PluginConfig[i].Name if _, ok := pluginConfig[name]; ok { return nil, fmt.Errorf(\"repeated config for plugin %s\", name) } pluginConfig[name] = profile.PluginConfig[i].Args } outputProfile := config.KubeSchedulerProfile{ SchedulerName: f.profileName, PercentageOfNodesToScore: f.percentageOfNodesToScore, Plugins: profile.Plugins, PluginConfig: make([]config.PluginConfig, 0, len(pg)), } f.pluginsMap = make(map[string]framework.Plugin) // 遍历Registry中注册的工厂函数 for name, factory := range r { // 只加载使用到的插件 if !pg.Has(name) { continue } // 获取插件参数 args := pluginConfig[name] if args != nil { outputProfile.PluginConfig = append(outputProfile.PluginConfig, config.PluginConfig{ Name: name, Args: args, }) } // 调用构造函数创建插件实例 p, err := factory(ctx, args, f) if err != nil { return nil, fmt.Errorf(\"initializing plugin %q: %w\", name, err) } // 保存到插件集合中 f.pluginsMap[name] = p // 设置EnqueueExtensions 用于监听入队事件 可用于任意阶段 f.fillEnqueueExtensions(p) } // 给扩展点配置插件链，根据指针的指向实际是填充了如[]framework.FilterPlugin的插件列表 for _, e := range f.getExtensionPoints(profile.Plugins) { // 通过反射修改 slicePtr指向[]framework.xxxPlugin对象 所以实际是修改了frameworkImpl.xxxPlugins字段的切片内容 if err := updatePluginList(e.slicePtr, *e.plugins, f.pluginsMap); err != nil { return nil, err } } // 如果有多扩展点插件(可作用于12标准流程中的多个扩展点)，把添加到对应的插件链中 if len(profile.Plugins.MultiPoint.Enabled) > 0 { if err := f.expandMultiPointPlugins(logger, profile); err != nil { return nil, err } } // 必须有一个QueueSortPlugin if len(f.queueSortPlugins) != 1 { return nil, fmt.Errorf(\"only one queue sort plugin required for profile with scheduler name %q, but got %d\", profile.SchedulerName, len(f.queueSortPlugins)) } // 必须有BindPlugin if len(f.bindPlugins) == 0 { return nil, fmt.Errorf(\"at least one bind plugin is needed for profile with scheduler name %q\", profile.SchedulerName) } // 设置ScorePlugin插件权重 if err := getScoreWeights(f, append(profile.Plugins.Score.Enabled, profile.Plugins.MultiPoint.Enabled...)); err != nil { return nil, err } // 校验ScorePlugin插件权重 for _, scorePlugin := range f.scorePlugins { if f.scorePluginWeight[scorePlugin.Name()] == 0 { return nil, fmt.Errorf(\"score plugin %q is not configured with weight\", scorePlugin.Name()) } } // 如果调度器的启动参数中设置了捕获配置 把outputProfile写入captureProfile字段 if options.captureProfile != nil { if len(outputProfile.PluginConfig) != 0 { sort.Slice(outputProfile.PluginConfig, func(i, j int) bool { return outputProfile.PluginConfig[i].Name 配置相关的类型梳理 总结配置中的一些核心结构，最顶层的配置结构是Options，是进程级别的全局配置参数，在NewOptions()函数中解析命令行参数并初始化，如--kubeconfig和--port等，主要承载了调度器组件的启动参数。 type Options struct { // 只关注ComponentConfig类型 在Setup阶段被赋值 ComponentConfig *kubeschedulerconfig.KubeSchedulerConfiguration SecureServing *apiserveroptions.SecureServingOptionsWithLoopback Authentication *apiserveroptions.DelegatingAuthenticationOptions Authorization *apiserveroptions.DelegatingAuthorizationOptions Metrics *metrics.Options Logs *logs.Options Deprecated *DeprecatedOptions LeaderElection *componentbaseconfig.LeaderElectionConfiguration ConfigFile string WriteConfigTo string Master string ComponentGlobalsRegistry featuregate.ComponentGlobalsRegistry Flags *cliflag.NamedFlagSets } 在Setup阶段(更准确地说是在ApplyTo()方法)基于Options生成了Config/CompletedConfig对象，合并了配置文件以及默认配置的内容，并在后续直接使用的是CompletedConfig去创建调度器实例，其中Config类型我们不特别关注，仍关注其中的字段ComponentConfig也就是类型KubeSchedulerConfiguration。 type Config struct { // 和Options中的ComponentConfig意义完全相同 ComponentConfig kubeschedulerconfig.KubeSchedulerConfiguration LoopbackClientConfig *restclient.Config Authentication apiserver.AuthenticationInfo Authorization apiserver.AuthorizationInfo SecureServing *apiserver.SecureServingInfo Client clientset.Interface KubeConfig *restclient.Config InformerFactory informers.SharedInformerFactory DynInformerFactory dynamicinformer.DynamicSharedInformerFactory EventBroadcaster events.EventBroadcasterAdapter LeaderElection *leaderelection.LeaderElectionConfig PodMaxInUnschedulablePodsDuration time.Duration } KubeSchedulerConfiguration类型的初始化也是在Setup中，字段Profiles是KubeSchedulerProfile的切片类型，所以支持多调度器配置，该结构的来源是调度器配置的静态文件，默认配置可以查找SetDefaults_KubeSchedulerConfiguration()函数，此函数逻辑在pkg/scheduler/apis/config/v1/register.go中的init()被调用。 type KubeSchedulerConfiguration struct { metav1.TypeMeta Parallelism int32 LeaderElection componentbaseconfig.LeaderElectionConfiguration ClientConnection componentbaseconfig.ClientConnectionConfiguration componentbaseconfig.DebuggingConfiguration PercentageOfNodesToScore *int32 PodInitialBackoffSeconds int64 PodMaxBackoffSeconds int64 // key是调度器名称 value是Framework实例 Profiles []KubeSchedulerProfile Extenders []Extender DelayCacheUntilActive bool } KubeSchedulerProfile类型包含了各阶段PluginSet的指针和调度器名称，是在调度框架流程中具体的调度配置。 type KubeSchedulerProfile struct { // profile名称 SchedulerName string // 打分节点抽样比例 PercentageOfNodesToScore *int32 // 插件集合 Plugins *Plugins // 插件配置Args PluginConfig []PluginConfig } 在New()函数初始化了schedulerOptions对象，是根据KubeSchedulerProfile而来的核心子集，仅包括调度器核心逻辑参数。 type schedulerOptions struct { componentConfigVersion string kubeConfig *restclient.Config percentageOfNodesToScore int32 podInitialBackoffSeconds int64 podMaxBackoffSeconds int64 podMaxInUnschedulablePodsDuration time.Duration frameworkOutOfTreeRegistry frameworkruntime.Registry // 由schedulerOptions.profiles赋值而来 profiles []schedulerapi.KubeSchedulerProfile extenders []schedulerapi.Extender frameworkCapturer FrameworkCapturer parallelism int32 applyDefaultProfile bool } NewMap使用schedulerOptions.profiles创建Map/frameworkImpl，frameworkImpl实现了Framework接口，每个frameworkImpl就代表一份动态的运行时配置，这也是调度周期中使用到的核心配置。 type Map map[string]framework.Framework type frameworkImpl struct { registry Registry snapshotSharedLister framework.SharedLister waitingPods *waitingPodsMap scorePluginWeight map[string]int // 12个标准阶段的插件列表 preEnqueuePlugins []framework.PreEnqueuePlugin // 入队事件扩展点 enqueueExtensions []framework.EnqueueExtensions queueSortPlugins []framework.QueueSortPlugin preFilterPlugins []framework.PreFilterPlugin filterPlugins []framework.FilterPlugin postFilterPlugins []framework.PostFilterPlugin preScorePlugins []framework.PreScorePlugin scorePlugins []framework.ScorePlugin reservePlugins []framework.ReservePlugin preBindPlugins []framework.PreBindPlugin bindPlugins []framework.BindPlugin postBindPlugins []framework.PostBindPlugin permitPlugins []framework.PermitPlugin pluginsMap map[string]framework.Plugin clientSet clientset.Interface kubeConfig *restclient.Config eventRecorder events.EventRecorder informerFactory informers.SharedInformerFactory sharedDRAManager framework.SharedDRAManager logger klog.Logger metricsRecorder *metrics.MetricAsyncRecorder profileName string percentageOfNodesToScore *int32 extenders []framework.Extender framework.PodNominator framework.PodActivator parallelizer parallelize.Parallelizer } Framework接口定义如下，可以看出主要还是和插件相关。 type Framework interface { Handle // PreEnqueuePlugins returns the registered preEnqueue plugins. PreEnqueuePlugins() []PreEnqueuePlugin // EnqueueExtensions returns the registered Enqueue extensions. EnqueueExtensions() []EnqueueExtensions // QueueSortFunc returns the function to sort pods in scheduling queue QueueSortFunc() LessFunc // RunPreFilterPlugins runs the set of configured PreFilter plugins. It returns // *Status and its code is set to non-success if any of the plugins returns // anything but Success. If a non-success status is returned, then the scheduling // cycle is aborted. // It also returns a PreFilterResult, which may influence what or how many nodes to // evaluate downstream. // The third returns value contains PreFilter plugin that rejected some or all Nodes with PreFilterResult. // But, note that it doesn't contain any plugin when a plugin rejects this Pod with non-success status, // not with PreFilterResult. RunPreFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod) (*PreFilterResult, *Status, sets.Set[string]) // RunPostFilterPlugins runs the set of configured PostFilter plugins. // PostFilter plugins can either be informational, in which case should be configured // to execute first and return Unschedulable status, or ones that try to change the // cluster state to make the pod potentially schedulable in a future scheduling cycle. RunPostFilterPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, filteredNodeStatusMap NodeToStatusReader) (*PostFilterResult, *Status) // RunPreBindPlugins runs the set of configured PreBind plugins. It returns // *Status and its code is set to non-success if any of the plugins returns // anything but Success. If the Status code is \"Unschedulable\", it is // considered as a scheduling check failure, otherwise, it is considered as an // internal error. In either case the pod is not going to be bound. RunPreBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // RunPostBindPlugins runs the set of configured PostBind plugins. RunPostBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) // RunReservePluginsReserve runs the Reserve method of the set of // configured Reserve plugins. If any of these calls returns an error, it // does not continue running the remaining ones and returns the error. In // such case, pod will not be scheduled. RunReservePluginsReserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // RunReservePluginsUnreserve runs the Unreserve method of the set of // configured Reserve plugins. RunReservePluginsUnreserve(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) // RunPermitPlugins runs the set of configured Permit plugins. If any of these // plugins returns a status other than \"Success\" or \"Wait\", it does not continue // running the remaining plugins and returns an error. Otherwise, if any of the // plugins returns \"Wait\", then this function will create and add waiting pod // to a map of currently waiting pods and return status with \"Wait\" code. // Pod will remain waiting pod for the minimum duration returned by the Permit plugins. RunPermitPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // WaitOnPermit will block, if the pod is a waiting pod, until the waiting pod is rejected or allowed. WaitOnPermit(ctx context.Context, pod *v1.Pod) *Status // RunBindPlugins runs the set of configured Bind plugins. A Bind plugin may choose // whether or not to handle the given Pod. If a Bind plugin chooses to skip the // binding, it should return code=5(\"skip\") status. Otherwise, it should return \"Error\" // or \"Success\". If none of the plugins handled binding, RunBindPlugins returns // code=5(\"skip\") status. RunBindPlugins(ctx context.Context, state *CycleState, pod *v1.Pod, nodeName string) *Status // HasFilterPlugins returns true if at least one Filter plugin is defined. HasFilterPlugins() bool // HasPostFilterPlugins returns true if at least one PostFilter plugin is defined. HasPostFilterPlugins() bool // HasScorePlugins returns true if at least one Score plugin is defined. HasScorePlugins() bool // ListPlugins returns a map of extension point name to list of configured Plugins. ListPlugins() *config.Plugins // ProfileName returns the profile name associated to a profile. ProfileName() string // PercentageOfNodesToScore returns percentageOfNodesToScore associated to a profile. PercentageOfNodesToScore() *int32 // SetPodNominator sets the PodNominator SetPodNominator(nominator PodNominator) // SetPodActivator sets the PodActivator SetPodActivator(activator PodActivator) // Close calls Close method of each plugin. Close() error } 调度阶段 调度流程被标准化为了12个阶段，每个阶段都可以进行扩展，在Plugins结构体中可以看出这些阶段的顺序，对应下图，通过对调度器不断的了解，会发现它的重要性。 在Plugins结构体的定义中，包含了12个标准阶段的插件集合，以及一个多扩展点插件集合的字段。 type Plugins struct { PreEnqueue PluginSet QueueSort PluginSet PreFilter PluginSet Filter PluginSet PostFilter PluginSet PreScore PluginSet Score PluginSet Reserve PluginSet Permit PluginSet PreBind PluginSet Bind PluginSet PostBind PluginSet MultiPoint PluginSet } 已经了解了Framework的由来以及插件扩展点，所谓Framework就是一份kube-scheduler在运行时用到的配置信息，改变了最初调度算法都硬编码在流程里对于程序扩展的限制，正如目前的大多数扩展性较好的项目如PostgreSQL，都是插件化的，调度器也通过Scheduler Framework实现了这一点。 从上图中可以看出，在Pod加入调度队列以后，包含了两个Cycle，也就是说在整个调度的过程中，包含两个大的生命周期。在Scheduler创建流程与调度队列提到过实例启动的最外层逻辑是Run()方法。 // Run begins watching and scheduling. It starts scheduling and blocked until the context is done. func (sched *Scheduler) Run(ctx context.Context) { logger := klog.FromContext(ctx) // 启动调度队列 sched.SchedulingQueue.Run(logger) // 启动调度循环 go wait.UntilWithContext(ctx, sched.ScheduleOne, 0) // 阻塞 等待关闭信号 在本文中，我们重点关注调度的整体流程，也就是通过协程启动的sched.ScheduleOne()循环。其中的wait.UntilWithContext()函数接收三个参数，分别是上下文对象、循环执行的函数、以及循环的间隔，所以调度时就是不间断地执行sched.ScheduleOne()方法。先来简单地了解ScheduleOne()的整体实现，有关两个周期的具体实现会在后续深入说明。 func (sched *Scheduler) ScheduleOne(ctx context.Context) { logger := klog.FromContext(ctx) // 从调度队列Pop出一个QueuedPodInfo对象 用于调度生命周期中 podInfo, err := sched.NextPod(logger) if err != nil { logger.Error(err, \"Error while retrieving next pod from scheduling queue\") return } // pod could be nil when schedulerQueue is closed if podInfo == nil || podInfo.Pod == nil { return } // 获取Pod对象 用于获取Pod直接相关的信息 pod := podInfo.Pod logger = klog.LoggerWithValues(logger, \"pod\", klog.KObj(pod)) ctx = klog.NewContext(ctx, logger) logger.V(4).Info(\"About to try and schedule pod\", \"pod\", klog.KObj(pod)) // 每一轮都获取配置 允许在不重启调度器的情况下动态更新策略 // 根据PodSpec的SchedulerName获取对应的Framework(可能存在多调度器配置) 如果Pod没有配置调度器则会在SetDefaults_PodSpec过程中注入默认的\"default-scheduler\" fwk, err := sched.frameworkForPod(pod) if err != nil { logger.Error(err, \"Error occurred\") // 如果一个Pod不被调度处理 也会在Pop之后通过Done()通知ActiveQ该Pod处理完毕 sched.SchedulingQueue.Done(pod.UID) return } // 如果Pod正在被删除或已经假定调度了就不处理 if sched.skipPodSchedule(ctx, fwk, pod) { // We don't put this Pod back to the queue, but we have to cleanup the in-flight pods/events. sched.SchedulingQueue.Done(pod.UID) return } logger.V(3).Info(\"Attempting to schedule pod\", \"pod\", klog.KObj(pod)) // 记录开始调度的时间 start := time.Now() // 初始化CycleState对象 state := framework.NewCycleState() // 生成随机数 如果值小于10就记录插件指标 state.SetRecordPluginMetrics(rand.Intn(100) 通过上面的代码可以看出，在一个Pod调度的的完整生命周期中，CycleState是Pod专属上下文，负责在调度过程中传递和共享状态信息。共存在三个动作，即Pod出队、调度、异步绑定，其中Pod出队动作比较简单，经过Scheduler创建流程与调度队列中调度队列部分的学习，可以知道Pop()动作就是弹出ActiveQ的队首元素，如果队列为空会阻塞等待唤醒。调度和绑定是两个清晰的生命周期，其中调度周期的扩展点以ReservePlugins，此时默认Pod会被调度成功，提前预留资源刷新调度缓存，Pod的内部状态为Assumed。绑定周期由于包括存储、网络等资源的设置而耗时较长所以异步执行，可以理解为在完成一个Pod的节点计算选择，就立刻进入了下一个Pod的调度。 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/04-详解调度周期SchedulingCycle-上.html":{"url":"kube-scheduler/04-详解调度周期SchedulingCycle-上.html","title":"详解调度周期SchedulingCycle(上)","keywords":"","body":" SchedulePod阶段 Predicates阶段 核心函数findNodesThatFitPod PreFilter扩展点 Filter扩展点 节点列表长度确定规则 运行Filter插件 详解调度周期SchedulingCycle(上) 调度周期的实现可以说是整个调度器的核心内容，所以展开说明。在ScheduleOne()中调度周期的入口方法是schedulingCycl()。 scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) 根据函数签名部分，它接收六个参数，包括调度周期的上下文ctx，用于协程的生命周期管理;调度周期状态state，调度插件通过该对象读取或写入数据以便协同工作;调度框架接口对象fwk，调度过程中根据Pod.Status.SchedulerName字段调度框架获取对应Framework实例，对应调度流程中的配置以及扩展点插件列表;Pod的信息podInfo，根据其中的信息选择节点;开始调度的时间戳start;待激活Pod集合podsToActivate。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) 然后来分析schedulingCycle()方法的实现逻辑，完整代码如下。 // schedulingCycle tries to schedule a single Pod. func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) if err != nil { defer func() { metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) }() if err == ErrNoNodesAvailable { status := framework.NewStatus(framework.UnschedulableAndUnresolvable).WithError(err) return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, status } fitError, ok := err.(*framework.FitError) if !ok { logger.Error(err, \"Error selecting node for pod\", \"pod\", klog.KObj(pod)) return ScheduleResult{nominatingInfo: clearNominatedNode}, podInfo, framework.AsStatus(err) } if !fwk.HasPostFilterPlugins() { logger.V(3).Info(\"No PostFilter plugins are registered, so no preemption will be performed\") return ScheduleResult{}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) } // Run PostFilter plugins to attempt to make the pod schedulable in a future scheduling cycle. result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatus) msg := status.Message() fitError.Diagnosis.PostFilterMsg = msg if status.Code() == framework.Error { logger.Error(nil, \"Status after running PostFilter plugins for pod\", \"pod\", klog.KObj(pod), \"status\", msg) } else { logger.V(5).Info(\"Status after running PostFilter plugins for pod\", \"pod\", klog.KObj(pod), \"status\", msg) } var nominatingInfo *framework.NominatingInfo if result != nil { nominatingInfo = result.NominatingInfo } return ScheduleResult{nominatingInfo: nominatingInfo}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) } metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost) if err != nil { return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.AsStatus(err) } // Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \"Scheduler cache ForgetPod failed\") } if sts.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, sts) fitErr.Diagnosis.AddPluginStatus(sts) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(sts.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, sts } // Run \"permit\" plugins. runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() && !runPermitStatus.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \"Scheduler cache ForgetPod failed\") } if runPermitStatus.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, runPermitStatus) fitErr.Diagnosis.AddPluginStatus(runPermitStatus) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(runPermitStatus.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, runPermitStatus } // At the end of a successful scheduling cycle, pop and move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // Clear the entries after activation. podsToActivate.Map = make(map[string]*v1.Pod) } return scheduleResult, assumedPodInfo, nil } SchedulePod阶段 因为选节点失败会触发抢占流程，先对可以成功选到节点的标准情况进行了解，也就是下面简化后的代码片段，做概述说明。首先调用sched.SchedulePod()方法，这个步骤可以说是整个调度周期的核心，其中包括了我们常说的预选Predicates和优选Priorities流程。回顾一下调度器实例的创建sched.applyDefaultHandlers()步骤中设置了调度函数和调度失败handler，没有采用硬编码的方式设置逻辑，提高了代码的灵活性。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod // 核心逻辑 调度Pod 包含Predicates和Priorities全流程 scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) // 调度失败处理 暂且忽略 ...... metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start)) // 避免影响到原始数据 深拷贝一个新对象 SchedulePod完成后Pod处于Assumed阶段 assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // 修改NodeName字段 err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost) // 失败处理 暂且忽略 ...... // 运行资源预留插件 标准扩展点之一 if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // 资源预留插件失败处理(运行Unreserve) 暂且忽略 ...... } // 运行准入插件 runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() && !runPermitStatus.IsSuccess() { // 准入插件返回失败处理 完全同上 ...... } // 调度周期结束前 激活待激活Pod if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // 清理集合 podsToActivate.Map = make(map[string]*v1.Pod) } // 返回结果 return scheduleResult, assumedPodInfo, nil } 分析一下schedulePod()方法的逻辑， func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { // 创建一个调度过程中的跟踪器 trace := utiltrace.New(\"Scheduling\", utiltrace.Field{Key: \"namespace\", Value: pod.Namespace}, utiltrace.Field{Key: \"name\", Value: pod.Name}) // 超过100ms打印日志 defer trace.LogIfLong(100 * time.Millisecond) // 每次执行调度的开始会更新调度缓存 // 对象的更新依据是Generation 缓存中节点是双向链表 当遍历到节点Generation小于当前快照Generation时退出能够提高效率 if err := sched.Cache.UpdateSnapshot(klog.FromContext(ctx), sched.nodeInfoSnapshot); err != nil { return result, err } trace.Step(\"Snapshotting scheduler cache and node infos done\") // 快照中没有节点时返回错误 if sched.nodeInfoSnapshot.NumNodes() == 0 { return result, ErrNoNodesAvailable } // 核心函数 Predicates流程 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) if err != nil { return result, err } trace.Step(\"Computing predicates done\") // 没找到可用节点返回错误 if len(feasibleNodes) == 0 { return result, &framework.FitError{ Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, } } // 找到的可用节点只有一个直接选用 不需要后续Priorities流程 if len(feasibleNodes) == 1 { return ScheduleResult{ SuggestedHost: feasibleNodes[0].Node().Name, EvaluatedNodes: 1 + diagnosis.NodeToStatus.Len(), FeasibleNodes: 1, }, nil } // 核心函数 Priorities流程 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil { return result, err } // 节点打分后的最终选择 host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) trace.Step(\"Prioritizing done\") // 返回节点选择结果 return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + diagnosis.NodeToStatus.Len(), FeasibleNodes: len(feasibleNodes), }, err } Predicates阶段 在schedulePod()方法中，Predicates阶段的入口代码如下，其中返回结果为PodInfo类型的列表feasibleNodes和节点不符合条件的原因diagnosis。 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) 其中Diagnosis类型组成如下，记录全局的Predicates节点诊断信息。 type Diagnosis struct { // PreFilter/Filter阶段不可用节点的信息集合 NodeToStatus *NodeToStatus // 使其返回UnschedulablePlugins或UnschedulableAndUnresolvable状态的插件集合信息 UnschedulablePlugins sets.Set[string] // 使其返回Pending状态的插件集合信息 PendingPlugins sets.Set[string] // PreFilter插件返回消息 PreFilterMsg string // PostFilter插件返回消息 PostFilterMsg string } type NodeToStatus struct { nodeToStatus map[string]*Status // 插件返回失败后标记节点状态为Unschedulable/UnschedulableAndUnresolvable // 与后续抢占逻辑有关 抢占不会去尝试UnschedulableAndUnresolvable的节点 absentNodesStatus *Status } type Status struct { code Code reasons []string err error plugin string } 补充说明一下插件返回的内部状态，为枚举类型： Success：插件执行成功; Error：内部错误，立即入队重试; Unschedulable：表示临时的不可调度，是动态(资源)的条件不满足，比如节点CPU资源不足，这种失败后Pod会重新入队等待调度，有退避时间; UnschedulableAndUnresolvable是静态(配置)条件不满足，如要求节点上存在某个标签但实际不存在，调度器不会重试，对应事件如节点更新可能会触发重新调度; Wait：仅和Permit插件有关，要求Pod进入等待状态; Skip：跳过当前插件检查; Pending是外部依赖条件不满足导致的等待，如存储卷未准备好或有依赖Pod的处理项未完成; 核心函数findNodesThatFitPod func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*framework.NodeInfo, framework.Diagnosis, error) { logger := klog.FromContext(ctx) // 初始化节点级的诊断字典 diagnosis := framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), } // 获取全量节点列表 allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List() if err != nil { return nil, diagnosis, err } // 运行PreFilter扩展点的插件 // 返回值为 过滤后的节点(全量为nil)、插件运行结果(Success/Error/Unschedulable/UnschedulableAndUnresolvable/Wait/Skip/Pending)、导致不可调度的插件集合 preRes, s, unscheduledPlugins := fwk.RunPreFilterPlugins(ctx, state, pod) diagnosis.UnschedulablePlugins = unscheduledPlugins if !s.IsSuccess() { if !s.IsRejected() { // 如果是Error/Skip直接返回 return nil, diagnosis, s.AsError() } // 更新节点不可用原因Unschedulable/UnschedulableAndUnresolvable 抢占相关 diagnosis.NodeToStatus.SetAbsentNodesStatus(s) // 返回失败 组装错误信息 msg := s.Message() diagnosis.PreFilterMsg = msg logger.V(5).Info(\"Status after running PreFilter plugins for pod\", \"pod\", klog.KObj(pod), \"status\", msg) diagnosis.AddPluginStatus(s) return nil, diagnosis, nil } // 逻辑1 如果存在被提名节点字段 尝试处理被提名节点 if len(pod.Status.NominatedNodeName) > 0 { // evaluateNominatedNode()方法内部调用了findNodesThatPassFilters()和findNodesThatPassExtenders() // 和逻辑2中实际逻辑一致 feasibleNodes, err := sched.evaluateNominatedNode(ctx, pod, fwk, state, diagnosis) if err != nil { logger.Error(err, \"Evaluation failed on nominated node\", \"pod\", klog.KObj(pod), \"node\", pod.Status.NominatedNodeName) } // 有被提名节点切通过插件的情况返回成功结果 if len(feasibleNodes) != 0 { return feasibleNodes, diagnosis, nil } } // 逻辑2 Pod信息不存在被提名节点字段 正常处理 nodes := allNodes if !preRes.AllNodes() { nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for nodeName := range preRes.NodeNames { if nodeInfo, err := sched.nodeInfoSnapshot.Get(nodeName); err == nil { nodes = append(nodes, nodeInfo) } } diagnosis.NodeToStatus.SetAbsentNodesStatus(framework.NewStatus(framework.UnschedulableAndUnresolvable, fmt.Sprintf(\"node(s) didn't satisfy plugin(s) %v\", sets.List(unscheduledPlugins)))) } // 运行Filter扩展点的插件 feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &diagnosis, nodes) processedNodes := len(feasibleNodes) + diagnosis.NodeToStatus.Len() sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % len(allNodes) if err != nil { return nil, diagnosis, err } feasibleNodesAfterExtender, err := findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatus) if err != nil { return nil, diagnosis, err } if len(feasibleNodesAfterExtender) != len(feasibleNodes) { if diagnosis.UnschedulablePlugins == nil { diagnosis.UnschedulablePlugins = sets.New[string]() } diagnosis.UnschedulablePlugins.Insert(framework.ExtenderName) } return feasibleNodesAfterExtender, diagnosis, nil } PreFilter扩展点 PreFilter的作用主要是缩小集群范围，可能会收集集群/节点信息，一般会通过cycleState.Write()方法，以扩展点+插件名为key写入CycleState对象中。 func (f *frameworkImpl) RunPreFilterPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (_ *framework.PreFilterResult, status *framework.Status, _ sets.Set[string]) { // 开始时间戳 startTime := time.Now() // 已经在PreFilter扩展点处理过的插件跳过Filter扩展点的处理 skipPlugins := sets.New[string]() defer func() { state.SkipFilterPlugins = skipPlugins metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PreFilter, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime)) }() // 初始化变量 var result *framework.PreFilterResult pluginsWithNodes := sets.New[string]() logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"PreFilter\") } var returnStatus *framework.Status // 遍历当前扩展点的插件列表 for _, pl := range f.preFilterPlugins { ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } // 运行单个插件 返回结果和状态 // PreFilter一般都是准备工作 所以基本上返回值PreFilterResult都是nil r, s := f.runPreFilterPlugin(ctx, pl, state, pod) if s.IsSkip() { skipPlugins.Insert(pl.Name()) continue } // status不是Success时 返回或记录状态 if !s.IsSuccess() { s.SetPlugin(pl.Name()) if s.Code() == framework.UnschedulableAndUnresolvable { // 静态条件不会被满足 UnschedulableAndUnresolvable状态直接退出 return nil, s, nil } if s.Code() == framework.Unschedulable { // 动态条件可能后面会被满足 Unschedulable状态可能触发抢占流程 returnStatus = s continue } return nil, framework.AsStatus(fmt.Errorf(\"running PreFilter plugin %q: %w\", pl.Name(), s.AsError())).WithPlugin(pl.Name()), nil } // 只有在PreFilter阶段缩小了节点范围 即返回不是全量的节点集合时 if !r.AllNodes() { // 记录使节点范围缩小的插件 pluginsWithNodes.Insert(pl.Name()) } // 合并PreFilter结果 result = result.Merge(r) // 取交集后节点集合为空 也就是PreFilter阶段筛掉了所有的节点 if !result.AllNodes() && len(result.NodeNames) == 0 { msg := fmt.Sprintf(\"node(s) didn't satisfy plugin(s) %v simultaneously\", sets.List(pluginsWithNodes)) if len(pluginsWithNodes) == 1 { msg = fmt.Sprintf(\"node(s) didn't satisfy plugin %v\", sets.List(pluginsWithNodes)[0]) } return result, framework.NewStatus(framework.UnschedulableAndUnresolvable, msg), pluginsWithNodes } } return result, returnStatus, pluginsWithNodes } 一个插件可以实现多个扩展点的接口，如Fit插件就同时实现了PreFilter、Filter和Score。 Filter扩展点 以有被提名节点的处理流程为例，与标准流程相同，Filter阶段的两个重要方法为findNodesThatPassFilters()和findNodesThatPassExtenders。 func (sched *Scheduler) evaluateNominatedNode(ctx context.Context, pod *v1.Pod, fwk framework.Framework, state *framework.CycleState, diagnosis framework.Diagnosis) ([]*framework.NodeInfo, error) { // 获取被提名Node的信息 nnn := pod.Status.NominatedNodeName nodeInfo, err := sched.nodeInfoSnapshot.Get(nnn) if err != nil { return nil, err } // 准备节点列表 node := []*framework.NodeInfo{nodeInfo} // 阶段1 运行Filter插件返回 feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &diagnosis, node) if err != nil { return nil, err } // 阶段2 仍属于Filter扩展点 运行自定义扩展过滤插件 feasibleNodes, err = findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatus) if err != nil { return nil, err } // 返回feasibleNodes列表 return feasibleNodes, nil } findNodesThatPassFilters()方法过滤出了通过Filter插件的节点列表。 func (sched *Scheduler) findNodesThatPassFilters( ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, diagnosis *framework.Diagnosis, nodes []*framework.NodeInfo) ([]*framework.NodeInfo, error) { numAllNodes := len(nodes) // 提前确定要初步过滤出的节点数量 numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes)) // 如果调度周期没有其他扩展点 那节点数量必须是1 if !sched.hasExtenderFilters() && !sched.hasScoring(fwk) { numNodesToFind = 1 } // 初始化变量 feasibleNodes := make([]*framework.NodeInfo, numNodesToFind) // 如果Filter扩展点没有插件 避免头部节点成为热点 从索引点拿够数量的Node就返回 if !fwk.HasFilterPlugins() { for i := range feasibleNodes { feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes] } return feasibleNodes, nil } // 有Filter插件的标准流程 errCh := parallelize.NewErrorChannel() var feasibleNodesLen int32 ctx, cancel := context.WithCancel(ctx) defer cancel() type nodeStatus struct { node string status *framework.Status } result := make([]*nodeStatus, numAllNodes) // 定义checkNode函数 即每个节点的Filter逻辑 checkNode := func(i int) { nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes] // 运行Filter插件 status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo) if status.Code() == framework.Error { errCh.SendErrorWithCancel(status.AsError(), cancel) return } if status.IsSuccess() { // 计数先加一再判断 length := atomic.AddInt32(&feasibleNodesLen, 1) if length > numNodesToFind { // 如果已经超过目标值 停止所有剩余节点的检查并会退计数 cancel() atomic.AddInt32(&feasibleNodesLen, -1) } else { // 在范围内就正常添加节点 feasibleNodes[length-1] = nodeInfo } } else { // 插件执行失败记录错误信息 result[i] = &nodeStatus{node: nodeInfo.Node().Name, status: status} } } // 记录开始时间 beginCheckNode := time.Now() statusCode := framework.Success defer func() { metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Filter, statusCode.String(), fwk.ProfileName()).Observe(metrics.SinceInSeconds(beginCheckNode)) }() // 使用并行器检查节点 fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, metrics.Filter) // 避免并发条件下的超额计数问题 feasibleNodes = feasibleNodes[:feasibleNodesLen] // 聚合节点结果到diagnosis对象 for _, item := range result { if item == nil { continue } diagnosis.NodeToStatus.Set(item.node, item.status) diagnosis.AddPluginStatus(item.status) } // 如过errCh收到错误返回筛选结果和错误 if err := errCh.ReceiveError(); err != nil { statusCode = framework.Error return feasibleNodes, err } return feasibleNodes, nil } 节点列表长度确定规则 为了平衡调度的效率，不会把所有符合条件的节点都列出并打分，所以feasiblenodes切片会有一个预估长度，最小长度是100。numFeasibleNodesToFind()抽样方法接收两个参数，分别是打分抽样百分比和集群节点总数。 func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) { // 节点总数 运行Filter插件 RunFilterPluginsWithNominatedPods()是真正运行插件RunFilterPlugins()前的重要方法，也是Filter插件的上层入口函数。在之前先说明一个关键的点，NominatedPod是在抢占计算后产生的，会在后续调度循环中尝试调度。 此处的巧妙设计充分表现了调度器的保守决策，两次插件执行必须全部通过才算做节点可用：第一次循环先设置模拟添加提名Pod标识位podsAdded，然后调用RunFilterPlugins()执行Filter插件，第二次时判断标识位和上一次结果，如果没有提名Pod可模拟或模拟后节点不可用，直接返回不用再执行第二次。如果考虑了提名Pod且第一次结果为成功，那么还需要执行第二次，保证如Pod间亲和性等条件在没有提名Pod在节点上运行时仍然能够满足。如果第二次评估失败，那么会覆盖第一次的评估结果，认为当前节点不可用。 func (f *frameworkImpl) RunFilterPluginsWithNominatedPods(ctx context.Context, state *framework.CycleState, pod *v1.Pod, info *framework.NodeInfo) *framework.Status { var status *framework.Status podsAdded := false logger := klog.FromContext(ctx) logger = klog.LoggerWithName(logger, \"FilterWithNominatedPods\") ctx = klog.NewContext(ctx, logger) // 两次循环 for i := 0; i addNominatedPods()函数把被提名到当前节点且优先级高于当前对象的Pod临时添加到节点信息中，以模拟成功抢占后的状态。 func addNominatedPods(ctx context.Context, fh framework.Handle, pod *v1.Pod, state *framework.CycleState, nodeInfo *framework.NodeInfo) (bool, *framework.CycleState, *framework.NodeInfo, error) { if fh == nil { return false, state, nodeInfo, nil } // 要在当前Node上抢占的Pod列表 nominatedPodInfos := fh.NominatedPodsForNode(nodeInfo.Node().Name) if len(nominatedPodInfos) == 0 { // 如果没有就跳过这一步 return false, state, nodeInfo, nil } // 如果有NominatedPod 拷贝一份快照和状态信息 nodeInfoOut := nodeInfo.Snapshot() stateOut := state.Clone() podsAdded := false // 遍历NominatedPod列表 for _, pi := range nominatedPodInfos { // 优先级比当前Pod高才会被尝试加入NodeInfo if corev1.PodPriority(pi.Pod) >= corev1.PodPriority(pod) && pi.Pod.UID != pod.UID { nodeInfoOut.AddPodInfo(pi) // 不是直接加入 status := fh.RunPreFilterExtensionAddPod(ctx, stateOut, pod, pi, nodeInfoOut) if !status.IsSuccess() { // 任意提名Pod调度失败 表示节点原因抢占失败 一票否决 return false, state, nodeInfo, status.AsError() } // 成功模拟后修改标识位 podsAdded = true } } return podsAdded, stateOut, nodeInfoOut, nil } 运行Filter插件的实际入口在RunFilterPlugins()方法中，和PreFilter的调用类似，都是循环执行集合中的插件然后返回状态，没有需要特别说明的地方。 func (f *frameworkImpl) RunFilterPlugins( ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, ) *framework.Status { logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"Filter\") } for _, pl := range f.filterPlugins { if state.SkipFilterPlugins.Has(pl.Name()) { continue } ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } if status := f.runFilterPlugin(ctx, pl, state, pod, nodeInfo); !status.IsSuccess() { if !status.IsRejected() { status = framework.AsStatus(fmt.Errorf(\"running %q filter plugin: %w\", pl.Name(), status.AsError())) } status.SetPlugin(pl.Name()) return status } } return nil } 回到之前的流程中，也就是evaluateNominatedNode()方法中，这里涉及到调度扩展器Scheduler Extenders，在此处先不详细说明 func findNodesThatPassExtenders(ctx context.Context, extenders []framework.Extender, pod *v1.Pod, feasibleNodes []*framework.NodeInfo, statuses *framework.NodeToStatus) ([]*framework.NodeInfo, error) { logger := klog.FromContext(ctx) // 遍历所有扩展器 for _, extender := range extenders { if len(feasibleNodes) == 0 { break } if !extender.IsInterested(pod) { continue } // 调用扩展器的Filter方法 // 返回可用Node列表、失败可重试节点列表、失败不可重试节点列表 feasibleList, failedMap, failedAndUnresolvableMap, err := extender.Filter(pod, feasibleNodes) if err != nil { if extender.IsIgnorable() { logger.Info(\"Skipping extender as it returned error and has ignorable flag set\", \"extender\", extender, \"err\", err) continue } return nil, err } // 状态写入 for failedNodeName, failedMsg := range failedAndUnresolvableMap { statuses.Set(failedNodeName, framework.NewStatus(framework.UnschedulableAndUnresolvable, failedMsg)) } for failedNodeName, failedMsg := range failedMap { if _, found := failedAndUnresolvableMap[failedNodeName]; found { // 两种失败都存在时 只记录UnschedulableAndUnresolvable状态就可以 continue } statuses.Set(failedNodeName, framework.NewStatus(framework.Unschedulable, failedMsg)) } // 更新最终节点列表 feasibleNodes = feasibleList } return feasibleNodes, nil } 至此Predicates阶段以返回一个feasibleNodes为结束，简单来看一下Pod中没有NominatedNodeName的标准情况，部分代码如下，并没有实际上的区别，基本相当于是未被封装的evaluateNominatedNode() // 初始化nodes为全部节点 nodes := allNodes // 如果Prefilter阶段返回的不是全部节点 // 就重新设置nodes为preRes中的节点 if !preRes.AllNodes() { nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames)) for nodeName := range preRes.NodeNames { if nodeInfo, err := sched.nodeInfoSnapshot.Get(nodeName); err == nil { nodes = append(nodes, nodeInfo) } } // 记录诊断信息 diagnosis.NodeToStatus.SetAbsentNodesStatus(framework.NewStatus(framework.UnschedulableAndUnresolvable, fmt.Sprintf(\"node(s) didn't satisfy plugin(s) %v\", sets.List(unscheduledPlugins)))) } // 同evaluateNominatedNode()中的调用流程 feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, &diagnosis, nodes) // 处理过的节点数量=通过数量+未通过数量 processedNodes := len(feasibleNodes) + diagnosis.NodeToStatus.Len() // 更新下次选节点的起始索引 sched.nextStartNodeIndex = (sched.nextStartNodeIndex + processedNodes) % len(allNodes) if err != nil { return nil, diagnosis, err } // 同evaluateNominatedNode()中的调用流程 feasibleNodesAfterExtender, err := findNodesThatPassExtenders(ctx, sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatus) if err != nil { return nil, diagnosis, err } // 如果运行扩展过滤器的节点数量和之前不一样 更新不可调度插件集合 if len(feasibleNodesAfterExtender) != len(feasibleNodes) { if diagnosis.UnschedulablePlugins == nil { diagnosis.UnschedulablePlugins = sets.New[string]() } diagnosis.UnschedulablePlugins.Insert(framework.ExtenderName) } return feasibleNodesAfterExtender, diagnosis, nil © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/05-详解调度周期SchedulingCycle-下.html":{"url":"kube-scheduler/05-详解调度周期SchedulingCycle-下.html","title":"详解调度周期SchedulingCycle(下)","keywords":"","body":" Priorities阶段 Score扩展点与NormalizeScore评分归一化 NormalizeScore插件 SelectHost Assume阶段 Reserve扩展点 失败后的状态回滚 Permit扩展点 Permit扩展点的延伸 详解调度周期SchedulingCycle(下) 在上篇中详细介绍了调度周期的上半段，也就是Predicates过滤阶段，在其过程中返回了一个重要的对象feasibleNodes，它是一个NodeInfo类型的切片，保存了在条件过滤后符合Pod调度要求的节点信息。 回到Predicates结束的位置，也就是schedulePod()方法中 func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) { ...... // Predicates阶段 返回预选Node和诊断结果 feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod) ...... // Case1:节点列表为空 返回失败 if len(feasibleNodes) == 0 { // 上层函数只会关注err是否为空 result未被赋值 return result, &framework.FitError{ Pod: pod, NumAllNodes: sched.nodeInfoSnapshot.NumNodes(), Diagnosis: diagnosis, } } // Case2:列表长度为1 不需要Priorities阶段 直接走后续流程 if len(feasibleNodes) == 1 { // 在Predicates阶段成功时会根据feasibleNodes填充result对象 return ScheduleResult{ SuggestedHost: feasibleNodes[0].Node().Name, EvaluatedNodes: 1 + diagnosis.NodeToStatus.Len(), FeasibleNodes: 1, }, nil } // Case3:列表长度>1 需要Priorities阶段选出最合适的节点再返回 priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes) if err != nil { return result, err } host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) trace.Step(\"Prioritizing done\") return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + diagnosis.NodeToStatus.Len(), FeasibleNodes: len(feasibleNodes), }, err } Priorities阶段 Priorities阶段的入口函数是prioritizeNodes()，对调度流程有过基本了解的一定都知道Pod的调度有预选和优选两个阶段，很明显在这个阶段要做的事情就是对上一步中过滤出来的节点进行排序，然后选择最合适的一个。 func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, ) ([]framework.NodePluginScores, error) { logger := klog.FromContext(ctx) if len(extenders) == 0 && !fwk.HasScorePlugins() { result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodePluginScores{ Name: nodes[i].Node().Name, TotalScore: 1, }) } return result, nil } // Run PreScore plugins. preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() { return nil, preScoreStatus.AsError() } // Run the Score plugins. nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() { return nil, scoreStatus.AsError() } // Additional details logged at level 10 if enabled. loggerVTen := logger.V(10) if loggerVTen.Enabled() { for _, nodeScore := range nodesScores { for _, pluginScore := range nodeScore.Scores { loggerVTen.Info(\"Plugin scored node for pod\", \"pod\", klog.KObj(pod), \"plugin\", pluginScore.Name, \"node\", nodeScore.Name, \"score\", pluginScore.Score) } } } if len(extenders) != 0 && nodes != nil { // allNodeExtendersScores has all extenders scores for all nodes. // It is keyed with node name. allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes)) var mu sync.Mutex var wg sync.WaitGroup for i := range extenders { if !extenders[i].IsInterested(pod) { continue } wg.Add(1) go func(extIndex int) { metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Inc() defer func() { metrics.Goroutines.WithLabelValues(metrics.PrioritizingExtender).Dec() wg.Done() }() prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes) if err != nil { // Prioritization errors from extender can be ignored, let k8s/other extenders determine the priorities logger.V(5).Info(\"Failed to run extender's priority function. No score given by this extender.\", \"error\", err, \"pod\", klog.KObj(pod), \"extender\", extenders[extIndex].Name()) return } mu.Lock() defer mu.Unlock() for i := range *prioritizedList { nodename := (*prioritizedList)[i].Host score := (*prioritizedList)[i].Score if loggerVTen.Enabled() { loggerVTen.Info(\"Extender scored node for pod\", \"pod\", klog.KObj(pod), \"extender\", extenders[extIndex].Name(), \"node\", nodename, \"score\", score) } // MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore, // therefore we need to scale the score returned by extenders to the score range used by the scheduler. finalscore := score * weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority) if allNodeExtendersScores[nodename] == nil { allNodeExtendersScores[nodename] = &framework.NodePluginScores{ Name: nodename, Scores: make([]framework.PluginScore, 0, len(extenders)), } } allNodeExtendersScores[nodename].Scores = append(allNodeExtendersScores[nodename].Scores, framework.PluginScore{ Name: extenders[extIndex].Name(), Score: finalscore, }) allNodeExtendersScores[nodename].TotalScore += finalscore } }(i) } // wait for all go routines to finish wg.Wait() for i := range nodesScores { if score, ok := allNodeExtendersScores[nodes[i].Node().Name]; ok { nodesScores[i].Scores = append(nodesScores[i].Scores, score.Scores...) nodesScores[i].TotalScore += score.TotalScore } } } if loggerVTen.Enabled() { for i := range nodesScores { loggerVTen.Info(\"Calculated node's final score for pod\", \"pod\", klog.KObj(pod), \"node\", nodesScores[i].Name, \"score\", nodesScores[i].TotalScore) } } return nodesScores, nil } 通过对Kubernetes源码的学习，可以感觉到它的代码是很结构化的，prioritizeNodes()函数整体较长，我们分块来理清它的逻辑。首先根据函数签名，它的入参包括：用于控制生命周期的上下文参数ctx，调度扩展器extenders，调度框架实例fwk，存储调度状态的state，Pod信息对象pod，以及上一步返回的节点列表nodes。返回值是一个NodePluginScores切片类型的节点评分结果，后续会经过selectHost()函数，最终敲定的目标节点并返回节点名称。 func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, ) ([]framework.NodePluginScores, error) 先来看不包括调度扩展器的逻辑部分，如果没有调度扩展器和Score插件，就把所有的节点都打1分然后返回。 func prioritizeNodes( ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, ) ([]framework.NodePluginScores, error) { logger := klog.FromContext(ctx) // 没有扩展器也没有评分插件 统一给一分并返回 if len(extenders) == 0 && !fwk.HasScorePlugins() { result := make([]framework.NodePluginScores, 0, len(nodes)) for i := range nodes { result = append(result, framework.NodePluginScores{ Name: nodes[i].Node().Name, TotalScore: 1, }) } return result, nil } // PreScore扩展点 preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes) if !preScoreStatus.IsSuccess() { return nil, preScoreStatus.AsError() } // Score扩展点 nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes) if !scoreStatus.IsSuccess() { return nil, scoreStatus.AsError() } // 详细日志输出 loggerVTen := logger.V(10) if loggerVTen.Enabled() { for _, nodeScore := range nodesScores { for _, pluginScore := range nodeScore.Scores { loggerVTen.Info(\"Plugin scored node for pod\", \"pod\", klog.KObj(pod), \"plugin\", pluginScore.Name, \"node\", nodeScore.Name, \"score\", pluginScore.Score) } } } ...... } RunPreScorePlugins()的实现和RunPreFilterPlugins()非常类似，同样是做了两大类事情：在遍历执行PreScore插件的过程中，调用cycleState.Write()记录信息到cycleState中，如污点容忍和亲和性等，并在遍历结束后把没有相关条件后续不需要执行的Score插件也记录到cycleState。 func (f *frameworkImpl) RunPreScorePlugins( ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, ) (status *framework.Status) { startTime := time.Now() skipPlugins := sets.New[string]() // 最后把Score阶段不需要执行的插件记录到cycleState defer func() { state.SkipScorePlugins = skipPlugins metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.PreScore, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime)) }() logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"PreScore\") } // 遍历执行PreScore插件逻辑 for _, pl := range f.preScorePlugins { ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } status = f.runPreScorePlugin(ctx, pl, state, pod, nodes) if status.IsSkip() { skipPlugins.Insert(pl.Name()) continue } if !status.IsSuccess() { return framework.AsStatus(fmt.Errorf(\"running PreScore plugin %q: %w\", pl.Name(), status.AsError())) } } return nil } 以Kubernetes的默认插件配置为例，分析PreScore和Score插件的行为，由于Weight权重字段一定是作用于Score相关的扩展点，所以选择TaintToleration插件作为分析对象。 func getDefaultPlugins() *v1.Plugins { plugins := &v1.Plugins{ MultiPoint: v1.PluginSet{ Enabled: []v1.Plugin{ {Name: names.SchedulingGates}, {Name: names.PrioritySort}, {Name: names.NodeUnschedulable}, {Name: names.NodeName}, {Name: names.TaintToleration, Weight: ptr.To[int32](3)}, {Name: names.NodeAffinity, Weight: ptr.To[int32](2)}, {Name: names.NodePorts}, {Name: names.NodeResourcesFit, Weight: ptr.To[int32](1)}, {Name: names.VolumeRestrictions}, {Name: names.NodeVolumeLimits}, {Name: names.VolumeBinding}, {Name: names.VolumeZone}, {Name: names.PodTopologySpread, Weight: ptr.To[int32](2)}, {Name: names.InterPodAffinity, Weight: ptr.To[int32](2)}, {Name: names.DefaultPreemption}, {Name: names.NodeResourcesBalancedAllocation, Weight: ptr.To[int32](1)}, {Name: names.ImageLocality, Weight: ptr.To[int32](1)}, {Name: names.DefaultBinder}, }, }, } applyFeatureGates(plugins) return plugins } 在经过从算法硬编码到Scheduler Framework的重构后，插件都在路径pkg/scheduler/framework/plugins下定义。TaintToleration插件可以在该路径下找到，一般每个插件目录下都是由算法实现和单元测试两个文件组成。 在taint_toleration.go文件中，可以看到该插件实现了Filter、PreScore、Score、NormalizeScore接口，其中的逻辑比较简单，PreScore扩展点时使用cycleState.Write()记录污点容忍信息到cycleState，到Score扩展点时TaintToleration根据写入时的key使用cycleState.Read()读出PreScore阶段记录的数据，如果不能容忍软性污点就计数加一，最后返回结果。NormalizeScore扩展点调用了默认的归一化逻辑。 func (pl *TaintToleration) PreScore(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status { if len(nodes) == 0 { return nil } tolerationsPreferNoSchedule := getAllTolerationPreferNoSchedule(pod.Spec.Tolerations) state := &preScoreState{ tolerationsPreferNoSchedule: tolerationsPreferNoSchedule, } cycleState.Write(preScoreStateKey, state) return nil } func (pl *TaintToleration) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) { nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName) if err != nil { return 0, framework.AsStatus(fmt.Errorf(\"getting node %q from Snapshot: %w\", nodeName, err)) } node := nodeInfo.Node() s, err := getPreScoreState(state) if err != nil { return 0, framework.AsStatus(err) } score := int64(countIntolerableTaintsPreferNoSchedule(node.Spec.Taints, s.tolerationsPreferNoSchedule)) return score, nil } func (pl *TaintToleration) NormalizeScore(ctx context.Context, _ *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status { return helper.DefaultNormalizeScore(framework.MaxNodeScore, true, scores) } func countIntolerableTaintsPreferNoSchedule(taints []v1.Taint, tolerations []v1.Toleration) (intolerableTaints int) { for _, taint := range taints { // 仅处理软性污点 if taint.Effect != v1.TaintEffectPreferNoSchedule { continue } // 不能容忍该污点时计数+1 if !v1helper.TolerationsTolerateTaint(tolerations, &taint) { intolerableTaints++ } } // 返回不能容忍污点计数结果 return } Score扩展点与NormalizeScore评分归一化 RunScorePlugins()的实现和RunFilterPlugins()类似。首先，Filter插件执行的过程中，状态被存储在CycleState对象并读写，过滤的行为属于责任链模式，如果一处不通过就直接失败退出，所以过滤阶段节点层面并行但插件层面是串行的。评分阶段也是节点层面串行和插件层面并行，如有疑问可以对比Predicates阶段的findNodesThatPassFilters()与Priorities阶段的RunScorePlugins()方法并加以详细对比。 func (f *frameworkImpl) RunScorePlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) (ns []framework.NodePluginScores, status *framework.Status) { startTime := time.Now() defer func() { metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Score, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime)) }() allNodePluginScores := make([]framework.NodePluginScores, len(nodes)) numPlugins := len(f.scorePlugins) plugins := make([]framework.ScorePlugin, 0, numPlugins) pluginToNodeScores := make(map[string]framework.NodeScoreList, numPlugins) // 初始化Score扩展点使用的插件列表 for _, pl := range f.scorePlugins { if state.SkipScorePlugins.Has(pl.Name()) { continue } plugins = append(plugins, pl) pluginToNodeScores[pl.Name()] = make(framework.NodeScoreList, len(nodes)) } // 创建上下文对象 ctx, cancel := context.WithCancel(ctx) defer cancel() errCh := parallelize.NewErrorChannel() // Score插件列表不为空时 if len(plugins) > 0 { logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"Score\") } // 为每个节点并发执行评分操作 f.Parallelizer().Until(ctx, len(nodes), func(index int) { nodeName := nodes[index].Node().Name logger := logger if verboseLogs { logger = klog.LoggerWithValues(logger, \"node\", klog.ObjectRef{Name: nodeName}) } // Score插件级别串行 for _, pl := range plugins { ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } s, status := f.runScorePlugin(ctx, pl, state, pod, nodeName) if !status.IsSuccess() { err := fmt.Errorf(\"plugin %q failed with: %w\", pl.Name(), status.AsError()) errCh.SendErrorWithCancel(err, cancel) return } // 记录评分信息到pluginToNodeScores pluginToNodeScores[pl.Name()][index] = framework.NodeScore{ Name: nodeName, Score: s, } } }, metrics.Score) if err := errCh.ReceiveError(); err != nil { return nil, framework.AsStatus(fmt.Errorf(\"running Score plugins: %w\", err)) } } // NormalizeScore扩展点 // 为每个插件并发执行评分归一化 f.Parallelizer().Until(ctx, len(plugins), func(index int) { pl := plugins[index] // Score插件必须实现ScoreExtensions()方法 如果不需要归一化就在该方法中返回nil if pl.ScoreExtensions() == nil { return } nodeScoreList := pluginToNodeScores[pl.Name()] // 有归一化需要的执行插件的NormalizeScore()方法并返回结果 status := f.runScoreExtension(ctx, pl, state, pod, nodeScoreList) if !status.IsSuccess() { err := fmt.Errorf(\"plugin %q failed with: %w\", pl.Name(), status.AsError()) errCh.SendErrorWithCancel(err, cancel) return } }, metrics.Score) if err := errCh.ReceiveError(); err != nil { return nil, framework.AsStatus(fmt.Errorf(\"running Normalize on Score plugins: %w\", err)) } // 按节点粒度并发执行 根据权重调整最终评分 f.Parallelizer().Until(ctx, len(nodes), func(index int) { nodePluginScores := framework.NodePluginScores{ Name: nodes[index].Node().Name, Scores: make([]framework.PluginScore, len(plugins)), } for i, pl := range plugins { weight := f.scorePluginWeight[pl.Name()] nodeScoreList := pluginToNodeScores[pl.Name()] score := nodeScoreList[index].Score // 评分的范围如果不在1-100之间返回错误 if score > framework.MaxNodeScore || score NormalizeScore插件 一些插件的NormalizeScore()实现是直接调用了默认的归一化评分方法DefaultNormalizeScore()，在文件pkg/scheduler/framework/plugins/helper/normalize_score.go中定义，仅注释不做过多说明。 func DefaultNormalizeScore(maxPriority int64, reverse bool, scores framework.NodeScoreList) *framework.Status { var maxCount int64 // 获取所有节点中的最高分 for i := range scores { if scores[i].Score > maxCount { maxCount = scores[i].Score } } // 如果所有节点评分都是0的情况 if maxCount == 0 { if reverse { for i := range scores { scores[i].Score = maxPriority } } return nil } // 正常情况 for i := range scores { score := scores[i].Score // 100*评分/最高分 score = maxPriority * score / maxCount // reverse用于Score评分低表示更高优先级的情况 如TaintToleration插件 if reverse { // 低分反转变成高分 score = maxPriority - score } // 记录归一化后的评分 scores[i].Score = score } return nil } 对上面例如TaintToleration插件的了解，不难发现其实调度器的算法实现并不复杂，重点在于整体流程的设计，实际上在了解了Scheduler Framework后，Pod的整个调度流程就已经非常清晰了。到目前为止总共接触到了PreFilter、Filter、PreScore、Score这四个扩展点的插件(其中NormalizeScore在Score扩展点内部，不属于12个标准扩展点之一)。在结合流程图中，前面有三个扩展点没有看到，分别是PreEnqueue、QueueSort和PostFilter，其中PreEnqueue和QueueSort是调度队列相关的两个插件，所以没有出现在调度周期内，如果感兴趣可以回到调度队列的runPreEnqueuePlugins()方法中，在Pod添加到ActiveQ时调用。QueueSort调用点较为隐蔽，可以以func (aq *activeQueue) update()为入口，调用关系如下方所示，调度队列实例创建之初，就向其中注册了Less()方法，它的调用点不像其他的插件是runXXXPlugin()而是Less()，Pod的入队和出队都会调用Less()方法。PostFilter插件只与抢占流程有关，在后面会单独介绍。 // 入队的调用链 activeQueue.update->queue.AddOrUpdate->heap.Push->up func (aq *activeQueue) update(newPod *v1.Pod, oldPodInfo *framework.QueuedPodInfo) *framework.QueuedPodInfo { aq.lock.Lock() defer aq.lock.Unlock() if pInfo, exists := aq.queue.Get(oldPodInfo); exists { _ = pInfo.Update(newPod) aq.queue.AddOrUpdate(pInfo) return pInfo } return nil } func (h *Heap[T]) AddOrUpdate(obj T) { key := h.data.keyFunc(obj) if _, exists := h.data.items[key]; exists { h.data.items[key].obj = obj heap.Fix(h.data, h.data.items[key].index) } else { heap.Push(h.data, &itemKeyValue[T]{key, obj}) if h.metricRecorder != nil { h.metricRecorder.Inc() } } } // golang的container/heap包 func Push(h Interface, x any) { h.Push(x) up(h, h.Len()-1) } func up(h Interface, j int) { for { i := (j - 1) / 2 // parent // 直接调用点 if i == j || !h.Less(j, i) { break } h.Swap(i, j) j = i } } // 出队的调用链 activeQueue.pop->activeQueue.unlockedPop->queue.Pop->heap.Pop->down func (aq *activeQueue) pop(logger klog.Logger) (*framework.QueuedPodInfo, error) { aq.lock.Lock() defer aq.lock.Unlock() return aq.unlockedPop(logger) } func (aq *activeQueue) unlockedPop(logger klog.Logger) (*framework.QueuedPodInfo, error) { for aq.queue.Len() == 0 { if aq.closed { logger.V(2).Info(\"Scheduling queue is closed\") return nil, nil } aq.cond.Wait() } pInfo, err := aq.queue.Pop() ...... } func (h *Heap[T]) Pop() (T, error) { obj := heap.Pop(h.data) if obj != nil { if h.metricRecorder != nil { h.metricRecorder.Dec() } return obj.(T), nil } var zero T return zero, fmt.Errorf(\"heap is empty\") } // golang的container/heap包 func Pop(h Interface) any { n := h.Len() - 1 h.Swap(0, n) down(h, 0, n) return h.Pop() } func down(h Interface, i0, n int) bool { i := i0 for { j1 := 2*i + 1 if j1 >= n || j1 i0 } SelectHost 此时已经得到了每个节点和其评分的对应关系，还需要进行Priorities阶段的最后一步，那就是从上一步的结果中选出最优先的那个，然后以ScheduleResult结构的形式返回给上层。 host, _, err := selectHost(priorityList, numberOfHighestScoredNodesToReport) return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + diagnosis.NodeToStatus.Len(), FeasibleNodes: len(feasibleNodes), }, err 其中的一个输入参数是numberOfHighestScoredNodesToReport，它的值为3，可以覆盖一主两备的场景并避免记录过多信息的内存占用，同时用于抢占流程和错误记录，该数值是信息完整性和性能之间的平衡点。 // numberOfHighestScoredNodesToReport is the number of node scores // to be included in ScheduleResult. numberOfHighestScoredNodesToReport = 3 下面分析selectHost()函数，首先是对长度做例行的判断，然后开始提取前三评分的节点，这里的设计采用了蓄水池抽样法。首先初始化了一个列表，并把nodeScoreHeap堆中的第一个元素Pop出来加入到列表中，因为nodeScoreHeap是按TotalScore从高到低来排序的。然后开始循环添加元素，在切片实际长度小于容量时，先拿当前节点的TotalScore和第一个元素的TotalScore做比较，如果值相等就根据蓄水池抽样法，以1/相同分数节点数量的概率替换第一个元素，以selectedIndex动态记录其位置，然后在元素加满之后交换列表中两个索引位置的值，最终返回第一个元素的节点名称和分数前三的节点列表，至此Priorities阶段完全结束。 func (h nodeScoreHeap) Less(i, j int) bool { return h[i].TotalScore > h[j].TotalScore } func selectHost(nodeScoreList []framework.NodePluginScores, count int) (string, []framework.NodePluginScores, error) { if len(nodeScoreList) == 0 { return \"\", nil, errEmptyPriorityList } // 初始化堆结构的节点评分列表 var h nodeScoreHeap = nodeScoreList heap.Init(&h) cntOfMaxScore := 1 selectedIndex := 0 // 先提取第一个节点 必然是最高分 sortedNodeScoreList := make([]framework.NodePluginScores, 0, count) sortedNodeScoreList = append(sortedNodeScoreList, heap.Pop(&h).(framework.NodePluginScores)) // 循环添加节点 for ns := heap.Pop(&h).(framework.NodePluginScores); ; ns = heap.Pop(&h).(framework.NodePluginScores) { // 当前节点分数和最高分不同 且节点列表已满时退出循环 if ns.TotalScore != sortedNodeScoreList[0].TotalScore && len(sortedNodeScoreList) == count { break } // 最高同分节点的选择 蓄水池抽样法 if ns.TotalScore == sortedNodeScoreList[0].TotalScore { // 最高分同分节点数量计数+1 cntOfMaxScore++ if rand.Intn(cntOfMaxScore) == 0 { // 同分节点有1/cntOfMaxScore的概率替代最优节点 selectedIndex = cntOfMaxScore - 1 } } sortedNodeScoreList = append(sortedNodeScoreList, ns) // 堆为空时退出循环 if h.Len() == 0 { break } } // 如果最高分同分节点替换了0号元素 if selectedIndex != 0 { // 在sortedNodeScoreList中交换元素位置 previous := sortedNodeScoreList[0] sortedNodeScoreList[0] = sortedNodeScoreList[selectedIndex] sortedNodeScoreList[selectedIndex] = previous } // 保证只有count个元素 if len(sortedNodeScoreList) > count { sortedNodeScoreList = sortedNodeScoreList[:count] } // 返回第一个节点的名称和整个列表 return sortedNodeScoreList[0].Name, sortedNodeScoreList, nil } schedulePod()方法返回的ScheduleResult中包括最后要尝试绑定的节点SuggestedHost，本次总共评估过的节点总数EvaluatedNodes，其值是过滤阶段返回的feasibleNodes长度与已知不可调度节点列表NodeToStatus的长度总和，还有可用节点数量FeasibleNodes。 return ScheduleResult{ SuggestedHost: host, EvaluatedNodes: len(feasibleNodes) + diagnosis.NodeToStatus.Len(), FeasibleNodes: len(feasibleNodes), }, err Assume阶段 ScheduleResult对象返回以后，schedulingCycle()方法中的第一个逻辑终于结束了，先不纠结于失败处理，继续分析标准的成功流程。 在SchedulePod()方法返回了成功以后，Kubernetes调度器对此有乐观的预期，认为经过精密而又保守的计算逻辑以后，这个Pod最终会被成功绑定，而绑定周期又是异步进行的，所以此时Pod会进入一个Assumed的中间状态，它会存在于调度缓存Cache中。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) // err不为空的处理 暂且忽略 // ...... // 深拷贝PodInfo assumedPodInfo := podInfo.DeepCopy() assumedPod := assumedPodInfo.Pod // 在缓存中对Pod的预期状态做更新 err = sched.assume(logger, assumedPod, scheduleResult.SuggestedHost) ...... } 调用assume()方法更新调度缓存中的内容，其中的Cache.AssumePod()在第一篇调度队列部分有过简单说明，这一步实际就是更新缓存一遍下一个Pod的计算能够考虑到处于Assume状态的Pod，不至于因为资源冲突导致绑定失败。 func (sched *Scheduler) assume(logger klog.Logger, assumed *v1.Pod, host string) error { // 修改NodeName字段 assumed.Spec.NodeName = host // 添加到调度缓存中 if err := sched.Cache.AssumePod(logger, assumed); err != nil { logger.Error(err, \"Scheduler cache AssumePod failed\") return err } // if \"assumed\" is a nominated pod, we should remove it from internal cache if sched.SchedulingQueue != nil { // 更新清除提名器中的信息 sched.SchedulingQueue.DeleteNominatedPodIfExists(assumed) } return nil } Reserve扩展点 在Assume阶段后，调度周期还剩下两个扩展点，此时关于调度的选择已经结束了，所做的内容是要为绑定和下次调度做准备，接下来的扩展点是资源预留Reserve扩展点和准入Permit扩展点。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { ...... // Run the Reserve method of reserve plugins. if sts := fwk.RunReservePluginsReserve(ctx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \"Scheduler cache ForgetPod failed\") } if sts.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, sts) fitErr.Diagnosis.AddPluginStatus(sts) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(sts.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, sts } // Run \"permit\" plugins. runPermitStatus := fwk.RunPermitPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) if !runPermitStatus.IsWait() && !runPermitStatus.IsSuccess() { // trigger un-reserve to clean up state associated with the reserved Pod fwk.RunReservePluginsUnreserve(ctx, state, assumedPod, scheduleResult.SuggestedHost) if forgetErr := sched.Cache.ForgetPod(logger, assumedPod); forgetErr != nil { logger.Error(forgetErr, \"Scheduler cache ForgetPod failed\") } if runPermitStatus.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, runPermitStatus) fitErr.Diagnosis.AddPluginStatus(runPermitStatus) return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, framework.NewStatus(runPermitStatus.Code()).WithError(fitErr) } return ScheduleResult{nominatingInfo: clearNominatedNode}, assumedPodInfo, runPermitStatus } // At the end of a successful scheduling cycle, pop and move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // Clear the entries after activation. podsToActivate.Map = make(map[string]*v1.Pod) } return scheduleResult, assumedPodInfo, nil } 下面看资源预留的逻辑RunReservePluginsReserve()和runReservePluginReserve()方法，外层和之前的几个RunXXXPlugin()没有什么区别。 func (f *frameworkImpl) RunReservePluginsReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (status *framework.Status) { startTime := time.Now() defer func() { metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Reserve, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime)) }() logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"Reserve\") logger = klog.LoggerWithValues(logger, \"node\", klog.ObjectRef{Name: nodeName}) } for _, pl := range f.reservePlugins { ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } status = f.runReservePluginReserve(ctx, pl, state, pod, nodeName) if !status.IsSuccess() { if status.IsRejected() { logger.V(4).Info(\"Pod rejected by plugin\", \"pod\", klog.KObj(pod), \"plugin\", pl.Name(), \"status\", status.Message()) status.SetPlugin(pl.Name()) return status } err := status.AsError() logger.Error(err, \"Plugin failed\", \"plugin\", pl.Name(), \"pod\", klog.KObj(pod)) return framework.AsStatus(fmt.Errorf(\"running Reserve plugin %q: %w\", pl.Name(), err)) } } return nil } func (f *frameworkImpl) runReservePluginReserve(ctx context.Context, pl framework.ReservePlugin, state *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status { if !state.ShouldRecordPluginMetrics() { return pl.Reserve(ctx, state, pod, nodeName) } startTime := time.Now() status := pl.Reserve(ctx, state, pod, nodeName) f.metricsRecorder.ObservePluginDurationAsync(metrics.Reserve, pl.Name(), status.Code().String(), metrics.SinceInSeconds(startTime)) return status } 那么看一个具体的资源预留插件VolumeBinding实现， func (pl *VolumeBinding) Reserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status { // 从CycleState中获取信息 state, err := getStateData(cs) if err != nil { return framework.AsStatus(err) } // we don't need to hold the lock as only one node will be reserved for the given pod podVolumes, ok := state.podVolumesByNode[nodeName] if ok { allBound, err := pl.Binder.AssumePodVolumes(klog.FromContext(ctx), pod, nodeName, podVolumes) if err != nil { return framework.AsStatus(err) } // 有卷需要绑定时 值设置为allBound返回的bool值 state.allBound = allBound } else { // 没有卷需要绑定时 值直接设置为true state.allBound = true } return nil } 其中调用了AssumePodVolumes()方法，更新了AssumeCache缓存和原PodVolumes对象中的信息，其中的pvcCache、pvcCache和Pod的Assume原理类似，都是对预期将会存在的对象做一份在缓存中的记录。 // AssumePodVolumes will take the matching PVs and PVCs to provision in pod's // volume information for the chosen node, and: // 1. Update the pvCache with the new prebound PV. // 2. Update the pvcCache with the new PVCs with annotations set // 3. Update PodVolumes again with cached API updates for PVs and PVCs. func (b *volumeBinder) AssumePodVolumes(logger klog.Logger, assumedPod *v1.Pod, nodeName string, podVolumes *PodVolumes) (allFullyBound bool, err error) { logger.V(4).Info(\"AssumePodVolumes\", \"pod\", klog.KObj(assumedPod), \"node\", klog.KRef(\"\", nodeName)) defer func() { if err != nil { metrics.VolumeSchedulingStageFailed.WithLabelValues(\"assume\").Inc() } }() // 检查Pod需要的所有卷是否都已经被绑定 // 如果已经绑定了就直接返回 if allBound := b.arePodVolumesBound(logger, assumedPod); allBound { logger.V(4).Info(\"AssumePodVolumes: all PVCs bound and nothing to do\", \"pod\", klog.KObj(assumedPod), \"node\", klog.KRef(\"\", nodeName)) return true, nil } // 静态卷预占 PV newBindings := []*BindingInfo{} for _, binding := range podVolumes.StaticBindings { newPV, dirty, err := volume.GetBindVolumeToClaim(binding.pv, binding.pvc) logger.V(5).Info(\"AssumePodVolumes: GetBindVolumeToClaim\", \"pod\", klog.KObj(assumedPod), \"PV\", klog.KObj(binding.pv), \"PVC\", klog.KObj(binding.pvc), \"newPV\", klog.KObj(newPV), \"dirty\", dirty, ) if err != nil { logger.Error(err, \"AssumePodVolumes: fail to GetBindVolumeToClaim\") b.revertAssumedPVs(newBindings) return false, err } // TODO: can we assume every time? if dirty { // 缓存更新 err = b.pvCache.Assume(newPV) if err != nil { b.revertAssumedPVs(newBindings) return false, err } } newBindings = append(newBindings, &BindingInfo{pv: newPV, pvc: binding.pvc}) } // 动态卷预占 PVC newProvisionedPVCs := []*v1.PersistentVolumeClaim{} for _, claim := range podVolumes.DynamicProvisions { // The claims from method args can be pointing to watcher cache. We must not // modify these, therefore create a copy. claimClone := claim.DeepCopy() metav1.SetMetaDataAnnotation(&claimClone.ObjectMeta, volume.AnnSelectedNode, nodeName) // 缓存更新 err = b.pvcCache.Assume(claimClone) if err != nil { b.revertAssumedPVs(newBindings) b.revertAssumedPVCs(newProvisionedPVCs) return } newProvisionedPVCs = append(newProvisionedPVCs, claimClone) } // PodVolumes对象更新 podVolumes.StaticBindings = newBindings podVolumes.DynamicProvisions = newProvisionedPVCs return } 失败后的状态回滚 仍以VolumeBinding为例，失败后的Unreserve插件获取调度开始时从ApiServer中获取到的对象信息，并借助Restore()方法向FIFO中发送更新事件，使本地缓存中的数据回滚到最初状态。 func (f *frameworkImpl) runReservePluginUnreserve(ctx context.Context, pl framework.ReservePlugin, state *framework.CycleState, pod *v1.Pod, nodeName string) { if !state.ShouldRecordPluginMetrics() { pl.Unreserve(ctx, state, pod, nodeName) return } startTime := time.Now() pl.Unreserve(ctx, state, pod, nodeName) f.metricsRecorder.ObservePluginDurationAsync(metrics.Unreserve, pl.Name(), framework.Success.String(), metrics.SinceInSeconds(startTime)) } func (pl *VolumeBinding) Unreserve(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) { s, err := getStateData(cs) if err != nil { return } // we don't need to hold the lock as only one node may be unreserved podVolumes, ok := s.podVolumesByNode[nodeName] if !ok { return } pl.Binder.RevertAssumedPodVolumes(podVolumes) } func (b *volumeBinder) RevertAssumedPodVolumes(podVolumes *PodVolumes) { b.revertAssumedPVs(podVolumes.StaticBindings) b.revertAssumedPVCs(podVolumes.DynamicProvisions) } func (b *volumeBinder) revertAssumedPVs(bindings []*BindingInfo) { for _, BindingInfo := range bindings { b.pvCache.Restore(BindingInfo.pv.Name) } } func (c *AssumeCache) Restore(objName string) { defer c.emitEvents() c.rwMutex.Lock() defer c.rwMutex.Unlock() objInfo, err := c.getObjInfo(objName) if err != nil { // This could be expected if object got deleted c.logger.V(5).Info(\"Restore object\", \"description\", c.description, \"cacheKey\", objName, \"err\", err) } else { if objInfo.latestObj != objInfo.apiObj { c.pushEvent(objInfo.latestObj, objInfo.apiObj) objInfo.latestObj = objInfo.apiObj } c.logger.V(4).Info(\"Restored object\", \"description\", c.description, \"cacheKey\", objName) } } 然后调用Cache.ForgetPod()方法，把Pod的信息从本地缓存的cache.podStates以及cache.assumedPods集合中删除。Permit扩展点的失败处理与之完全相同。 Permit扩展点 Reserve扩展点之后紧接着就是Permit扩展点，通过代码逻辑可以看出，在执行完具体插件之后会对返回的状态status做出判断，这里和其他插件返回状态相比，多了一个Wait状态，如果没有返回Success而是Wait时，根据返回值设置插件等待的超时时长(最大不超过15分钟)，并修改statusCode标识位，在遍历执行完所有的插件之后，如果是需要等待的，就将其加入到等待队列中。 func (f *frameworkImpl) RunPermitPlugins(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (status *framework.Status) { startTime := time.Now() defer func() { metrics.FrameworkExtensionPointDuration.WithLabelValues(metrics.Permit, status.Code().String(), f.profileName).Observe(metrics.SinceInSeconds(startTime)) }() pluginsWaitTime := make(map[string]time.Duration) statusCode := framework.Success logger := klog.FromContext(ctx) verboseLogs := logger.V(4).Enabled() if verboseLogs { logger = klog.LoggerWithName(logger, \"Permit\") logger = klog.LoggerWithValues(logger, \"node\", klog.ObjectRef{Name: nodeName}) } for _, pl := range f.permitPlugins { ctx := ctx if verboseLogs { logger := klog.LoggerWithName(logger, pl.Name()) ctx = klog.NewContext(ctx, logger) } status, timeout := f.runPermitPlugin(ctx, pl, state, pod, nodeName) // 返回状态判断 if !status.IsSuccess() { if status.IsRejected() { logger.V(4).Info(\"Pod rejected by plugin\", \"pod\", klog.KObj(pod), \"plugin\", pl.Name(), \"status\", status.Message()) return status.WithPlugin(pl.Name()) } if status.IsWait() { // 返回状态是Wait 设置超时时间 // 最大是15分钟 if timeout > maxTimeout { timeout = maxTimeout } // 设置插件等待超时时长并修改标statusCode识位 pluginsWaitTime[pl.Name()] = timeout statusCode = framework.Wait } else { err := status.AsError() logger.Error(err, \"Plugin failed\", \"plugin\", pl.Name(), \"pod\", klog.KObj(pod)) return framework.AsStatus(fmt.Errorf(\"running Permit plugin %q: %w\", pl.Name(), err)).WithPlugin(pl.Name()) } } } // 如果遍历结束后最终状态是Wait if statusCode == framework.Wait { waitingPod := newWaitingPod(pod, pluginsWaitTime) f.waitingPods.add(waitingPod) msg := fmt.Sprintf(\"one or more plugins asked to wait and no plugin rejected pod %q\", pod.Name) logger.V(4).Info(\"One or more plugins asked to wait and no plugin rejected pod\", \"pod\", klog.KObj(pod)) return framework.NewStatus(framework.Wait, msg) } return nil } 在frameworkImpl中包含一个等待队列waitingPods的结构，其类型是waitingPodsMap，由waitingPod的集合和一个读写锁组成。 type waitingPodsMap struct { pods map[types.UID]*waitingPod mu sync.RWMutex } type waitingPod struct { pod *v1.Pod // 定时器 pendingPlugins map[string]*time.Timer // 接收结果 s chan *framework.Status mu sync.RWMutex } 这里很好理解了，就是组装一个waitingPod结构，为每个其中要等待的插件设置一下定时器，然后把这个waitingPod对象加入集合中，如果时间到了就会自动执行Reject()方法。 func newWaitingPod(pod *v1.Pod, pluginsMaxWaitTime map[string]time.Duration) *waitingPod { // 组装waitingPod结构 wp := &waitingPod{ pod: pod, s: make(chan *framework.Status, 1), } wp.pendingPlugins = make(map[string]*time.Timer, len(pluginsMaxWaitTime)) wp.mu.Lock() defer wp.mu.Unlock() // 遍历等待结果的插件 分别设置定时器 for k, v := range pluginsMaxWaitTime { plugin, waitTime := k, v wp.pendingPlugins[plugin] = time.AfterFunc(waitTime, func() { msg := fmt.Sprintf(\"rejected due to timeout after waiting %v at plugin %v\", waitTime, plugin) // 如果超时会执行Reject方法 wp.Reject(plugin, msg) }) } return wp } 如果插件返回成功会调用Allow()方法，从pendingPlugins的集合中获取插件的信息，然后停止定时器并删除当前元素。通过对集合长度的判断，在长度为0时表示所有的Permit插件都已经允许这个Pod的调度了，就向waitingPod的s通道中发送一个framework.Success的信号，该信号会在绑定周期被接收并判断。 func (w *waitingPod) Allow(pluginName string) { w.mu.Lock() defer w.mu.Unlock() if timer, exist := w.pendingPlugins[pluginName]; exist { // 停止当前定时器 timer.Stop() // 从插件列表中删除当前插件 delete(w.pendingPlugins, pluginName) } // 还需要等待其他插件的结果 if len(w.pendingPlugins) != 0 { return } // pendingPlugins中已经没有元素了 向channel中发送成功信号 select { case w.s Permit扩展点运行之后，也就是在整个SchedulingCycle结束之前，会激活podsToActivate集合中的Pod，把它们重新加入到ActiveQ里，为下一次调度的SchedulingCycle做好准备。 // At the end of a successful scheduling cycle, pop and move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) // Clear the entries after activation. podsToActivate.Map = make(map[string]*v1.Pod) } return scheduleResult, assumedPodInfo, nil } Permit扩展点的延伸 在调度周期中，Permit阶段如果返回的是Wait状态，调度器不会因为等待它返回的结果而去影响其他Pod调度的效率，而是在异步执行绑定周期BindingCycle的开始处确认waitingPod的最终结果是Success还是Rejected。 // Run \"permit\" plugins. if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() { if status.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: assumedPodInfo.Pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), UnschedulablePlugins: sets.New(status.Plugin()), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, status) return framework.NewStatus(status.Code()).WithError(fitErr) } return status } 先判断要绑定的Pod是否是waitingPod，如果不存在就直接进入后面的流程。 func (f *frameworkImpl) WaitOnPermit(ctx context.Context, pod *v1.Pod) *framework.Status { // 从waitingPods集合中获取当前Pod waitingPod := f.waitingPods.get(pod.UID) // 集合中不存在这个对象直接返回 if waitingPod == nil { return nil } // WaitOnPermit流程的最后从waitingPods集合中移除当前对象 defer f.waitingPods.remove(pod.UID) logger := klog.FromContext(ctx) logger.V(4).Info(\"Pod waiting on permit\", \"pod\", klog.KObj(pod)) startTime := time.Now() // waitingPod的s是一个channel 用于接收最终的Permit结果 s := 至此，调度周期的标准逻辑完全结束，关于失败处理逻辑FailureHandler会放到关于Preemption的章节里一起说明。 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/06-详解绑定周期BindingCycle.html":{"url":"kube-scheduler/06-详解绑定周期BindingCycle.html","title":"详解绑定周期BindingCycle","keywords":"","body":" 绑定周期逻辑 PreBind扩展点 Bind扩展点 PostBind扩展点 新的开始 详解绑定周期BindingCycle 上一节调度周期的流程已经完全结束了，整个Pod调度的生命周期已经结束了大半，回到ScheduleOne()方法来看，在调度周期schedulingCycle()返回了结果以后，如果是失败就return了，如果是成功就通过go关键字启动一个Goroutine协程来做绑定周期的逻辑，也相当于是这一次的ScheduleOne()调用结束了，但别忘了它是通过UntilWithContext()启动的间隔为0的循环逻辑，表明调度周期结束后就立刻开始了下个Pod的调度计算，而绑定逻辑以及它的结果交给协程去处理，调度器的ScheduleOne()每次只能处理一个Pod，因此如果等待不确定会耗时多久的绑定动作结束才开始调度新Pod是不明智的，包括Permit插件返回Wait后的结果也是在绑定周期的一开始去等待接收的，这十分符合效率优先的原则。 func (sched *Scheduler) ScheduleOne(ctx context.Context) { ...... // 调度周期 scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) // 失败退出 if !status.IsSuccess() { sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return } // 或者通过协程处理绑定逻辑 不影响下一次调度的开始 go func() { bindingCycleCtx, cancel := context.WithCancel(ctx) defer cancel() metrics.Goroutines.WithLabelValues(metrics.Binding).Inc() defer metrics.Goroutines.WithLabelValues(metrics.Binding).Dec() // 进入绑定生命周期 status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() { sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) return } }() } 绑定周期逻辑 下面对绑定周期的逻辑进行分析，首先获取等待绑定Pod(AssumedPod)的对象，然后从等待队列的channel中读取数据并返回对应状态。源代码在调用WaitOnPermit方法时注释为// Run \"permit\" plugins.，此处注释是不准确的，并且可能会造成误导，实际的Permit插件调用点在SchedulingCycle周期的最后，个人认为此处应该视为Permit扩展点的状态接收点，不涉及插件逻辑的实际调用。整体上绑定周期涉及了半个扩展点的状态接收，和三个扩展点的执行，包括PreBind、Bind和PostBind。 在源码注释中的PreBind插件执行后，有一段注释值得注意，意为：任何此扩展点之后的失败都不会导致Pod被视为Unschedulable，只有当Pod在某些扩展点被拒绝时，才会将状态修改为Unschedulable，PreBind是这些中的最后一个扩展点。我们调用调度队列的Done()方法，可以尽早释放调度队列中存储的集群事件以优化内存消耗减轻集群压力。 这里相关的设计思想会在PreBind扩展点详细展开分析。 // Any failures after this point cannot lead to the Pod being considered unschedulable. // We define the Pod as \"unschedulable\" only when Pods are rejected at specific extension points, and PreBind is the last one in the scheduling/binding cycle. // // We can call Done() here because // we can free the cluster events stored in the scheduling queue sonner, which is worth for busy clusters memory consumption wise. func (sched *Scheduler) bindingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, scheduleResult ScheduleResult, assumedPodInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate) *framework.Status { logger := klog.FromContext(ctx) // 获取AssumedPod对象 assumedPod := assumedPodInfo.Pod // 接收Permit插件返回状态 if status := fwk.WaitOnPermit(ctx, assumedPod); !status.IsSuccess() { // 处理失败状态 if status.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: assumedPodInfo.Pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), UnschedulablePlugins: sets.New(status.Plugin()), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, status) return framework.NewStatus(status.Code()).WithError(fitErr) } return status } // 执行PreBind插件 if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() { // 处理失败状态 if status.IsRejected() { fitErr := &framework.FitError{ NumAllNodes: 1, Pod: assumedPodInfo.Pod, Diagnosis: framework.Diagnosis{ NodeToStatus: framework.NewDefaultNodeToStatus(), UnschedulablePlugins: sets.New(status.Plugin()), }, } fitErr.Diagnosis.NodeToStatus.Set(scheduleResult.SuggestedHost, status) return framework.NewStatus(status.Code()).WithError(fitErr) } return status } // Any failures after this point cannot lead to the Pod being considered unschedulable. // We define the Pod as \"unschedulable\" only when Pods are rejected at specific extension points, and PreBind is the last one in the scheduling/binding cycle. // // We can call Done() here because // we can free the cluster events stored in the scheduling queue sonner, which is worth for busy clusters memory consumption wise. sched.SchedulingQueue.Done(assumedPod.UID) // 执行Bind插件 if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() { return status } // Calculating nodeResourceString can be heavy. Avoid it if klog verbosity is below 2. logger.V(2).Info(\"Successfully bound pod to node\", \"pod\", klog.KObj(assumedPod), \"node\", scheduleResult.SuggestedHost, \"evaluatedNodes\", scheduleResult.EvaluatedNodes, \"feasibleNodes\", scheduleResult.FeasibleNodes) metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start)) metrics.PodSchedulingAttempts.Observe(float64(assumedPodInfo.Attempts)) if assumedPodInfo.InitialAttemptTimestamp != nil { metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp)) metrics.PodSchedulingSLIDuration.WithLabelValues(getAttemptsLabel(assumedPodInfo)).Observe(metrics.SinceInSeconds(*assumedPodInfo.InitialAttemptTimestamp)) } // 执行PostBind插件 fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) // 调度结束前把待激活Pod加入ActiveQ if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) } return nil } PreBind扩展点 PreBind插件调用入口和其他扩展点没有任何区别，查看PreBind()方法主要有DynamicResources和VolumeBinding两个插件实现，简单分析这两个插件在该阶段都实现了什么逻辑。首先VolumeBinding插件通过CycleState对象获取状态信息，然后判断是否所有需要的卷都已经和Pod进行了绑定，如果都已绑定则跳过。否则获取目标节点上的卷，然后调用BindPodVolumes()方法绑定Pod和卷。 func (pl *VolumeBinding) PreBind(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status { // 获取状态信息 s, err := getStateData(cs) if err != nil { return framework.AsStatus(err) } if s.allBound { // 所有卷都绑定直接返回 return nil } // 获取节点上要与Pod绑定的卷 podVolumes, ok := s.podVolumesByNode[nodeName] if !ok { return framework.AsStatus(fmt.Errorf(\"no pod volumes found for node %q\", nodeName)) } logger := klog.FromContext(ctx) logger.V(5).Info(\"Trying to bind volumes for pod\", \"pod\", klog.KObj(pod)) // 进行绑定 err = pl.Binder.BindPodVolumes(ctx, pod, podVolumes) if err != nil { logger.V(5).Info(\"Failed to bind volumes for pod\", \"pod\", klog.KObj(pod), \"err\", err) return framework.AsStatus(err) } logger.V(5).Info(\"Success binding volumes for pod\", \"pod\", klog.KObj(pod)) return nil } 第二个插件DynamicResources处理动态资源如GPU在Pod绑定到节点前的分配，首先还是从CycleState获取状态信息，然后判断声明的资源是否已经在节点上被预留，如果没有被预留则执行bindClaim()方法，bindClaim()返回更新后的资源对象，实际上是更新了它的ReservedFor、Allocation和Finalizers字段，把返回的对象更新到调度上下文CycleState中。 func (pl *DynamicResources) PreBind(ctx context.Context, cs *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status { if !pl.enabled { return nil } // 获取状态信息 state, err := getStateData(cs) if err != nil { return statusError(klog.FromContext(ctx), err) } if len(state.claims) == 0 { return nil } logger := klog.FromContext(ctx) // 遍历动态资源是否都已经被预留 for index, claim := range state.claims { // 如果没有被预留执行处理逻辑 if !resourceclaim.IsReservedForPod(pod, claim) { claim, err := pl.bindClaim(ctx, state, index, pod, nodeName) if err != nil { return statusError(logger, err) } // 更新资源状态 state.claims[index] = claim } } return nil } func (pl *DynamicResources) bindClaim(ctx context.Context, state *stateData, index int, pod *v1.Pod, nodeName string) (patchedClaim *resourceapi.ResourceClaim, finalErr error) { logger := klog.FromContext(ctx) // 深拷贝动态资源 claim := state.claims[index].DeepCopy() // 获取动态资源分配信息 allocation := state.informationsForClaim[index].allocation defer func() { // 结束前清理分配状态 if allocation != nil { if finalErr == nil { if err := pl.draManager.ResourceClaims().AssumeClaimAfterAPICall(claim); err != nil { logger.V(5).Info(\"Claim not stored in assume cache\", \"err\", finalErr) } } pl.draManager.ResourceClaims().RemoveClaimPendingAllocation(claim.UID) } }() logger.V(5).Info(\"preparing claim status update\", \"claim\", klog.KObj(state.claims[index]), \"allocation\", klog.Format(allocation)) // 资源版本比较 refreshClaim := false // RetryOnConflict方法接收一个重试次数和匿名函数 retryErr := retry.RetryOnConflict(retry.DefaultRetry, func() error { // 后续重试时如果标识位为true if refreshClaim { // 通过API获取最新资源对象 updatedClaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Get(ctx, claim.Name, metav1.GetOptions{}) if err != nil { return fmt.Errorf(\"get updated claim %s after conflict: %w\", klog.KObj(claim), err) } logger.V(5).Info(\"retrying update after conflict\", \"claim\", klog.KObj(claim)) // 覆盖旧版本资源对象 claim = updatedClaim } else { // 第一次执行先设置标识位为true refreshClaim = true } // 检查资源是否已经标记为删除 if claim.DeletionTimestamp != nil { return fmt.Errorf(\"claim %s got deleted in the meantime\", klog.KObj(claim)) } // 处理资源分配结果 if allocation != nil { if claim.Status.Allocation != nil { return fmt.Errorf(\"claim %s got allocated elsewhere in the meantime\", klog.KObj(claim)) } // 如果没有Finalizer if !slices.Contains(claim.Finalizers, resourceapi.Finalizer) { // 给资源对象添加Finalizer避免被意外删除 claim.Finalizers = append(claim.Finalizers, resourceapi.Finalizer) // 通过API更新资源对象 updatedClaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).Update(ctx, claim, metav1.UpdateOptions{}) if err != nil { return fmt.Errorf(\"add finalizer to claim %s: %w\", klog.KObj(claim), err) } // 更新资源对象 claim = updatedClaim } // 把从CycleState中临时存储的分配决策赋给资源对象 claim.Status.Allocation = allocation } // 添加到资源预留列表 claim.Status.ReservedFor = append(claim.Status.ReservedFor, resourceapi.ResourceClaimConsumerReference{Resource: \"pods\", Name: pod.Name, UID: pod.UID}) // 通过API更新资源对象 updatedClaim, err := pl.clientset.ResourceV1beta1().ResourceClaims(claim.Namespace).UpdateStatus(ctx, claim, metav1.UpdateOptions{}) if err != nil { if allocation != nil { return fmt.Errorf(\"add allocation and reservation to claim %s: %w\", klog.KObj(claim), err) } return fmt.Errorf(\"add reservation to claim %s: %w\", klog.KObj(claim), err) } // 更新资源对象 claim = updatedClaim return nil }) if retryErr != nil { return nil, retryErr } logger.V(5).Info(\"reserved\", \"pod\", klog.KObj(pod), \"node\", klog.ObjectRef{Name: nodeName}, \"resourceclaim\", klog.Format(claim)) return claim, nil } 根据以上的了解，可以感受到几个扩展点之间的协作关系： Reserve阶段在内存中锁定资源并写入CycleState; Permit阶段验证资源的依赖是否就绪、资源是否未被占用; PreBind阶段把分配结果持久化到资源对象API; PreBind与Reserve和Permit共同完成了从资源临时锁定到许可绑定再到最终确认的过程，与更早的Filter/Score完成了静态匹配到动态确认的过程。PreBind是绑定执行前的最终确认者，是最后一个失败后会导致Pod进入Unschedulable的扩展点，如果在其后的Bind阶段失败，通常会进行重试绑定的操作，而不会标记为Unschedulable而重新进行调度计算。 Bind扩展点 PreBind结束后调用Done()方法释放调度队列中的Pod对象，随后调用bind()方法进行绑定操作。其中包括执行扩展绑定插件、执行绑定插件和绑定结果确认三部分。 func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) { logger := klog.FromContext(ctx) defer func() { // 绑定结果检查 sched.finishBinding(logger, fwk, assumed, targetNode, status) }() bound, err := sched.extendersBinding(logger, assumed, targetNode) if bound { return framework.AsStatus(err) } // 执行Bind插件 return fwk.RunBindPlugins(ctx, state, assumed, targetNode) } 分析DefaultBinder默认绑定插件的实现，实际上只是组装一个Binding对象，其中包括绑定主体的ObjectMeta和绑定目标的资源类型和名称，然后通过调用API Server的Pods.Bind()接口提交请求信息。可以理解Bind阶段只涉及API接口调用，不涉及其他的逻辑。 func (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status { logger := klog.FromContext(ctx) logger.V(3).Info(\"Attempting to bind pod to node\", \"pod\", klog.KObj(p), \"node\", klog.KRef(\"\", nodeName)) // 创建Binding对象 binding := &v1.Binding{ // 对象元数据 ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID}, // 绑定目标对象 Target: v1.ObjectReference{Kind: \"Node\", Name: nodeName}, } // 调用API Server接口 err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{}) if err != nil { return framework.AsStatus(err) } return nil } PostBind扩展点 根据绑定周期的代码实现来看，PostBind并不是像PostFilter一样的失败后处理流程，而是标准成功流程中的一部分，但是默认调度器中没有设置PostBind插件，一般来说该扩展点的作用包括：执行非阻塞的后置任务、传递结果触发外部联动、临时资源清理等。作为所有扩展点的最后一站，其核心价值在于绑定成功后的自定义后置操作，扩展调度器和外部系统的联动能力，而且不影响调度逻辑的稳定性。 func (sched *Scheduler) bindingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, scheduleResult ScheduleResult, assumedPodInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate) *framework.Status { ...... // Run \"prebind\" plugins. if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) ...... // Run \"bind\" plugins. if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state) ...... // Run \"postbind\" plugins. fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost) // At the end of a successful binding cycle, move up Pods if needed. if len(podsToActivate.Map) != 0 { sched.SchedulingQueue.Activate(logger, podsToActivate.Map) } return nil } 新的开始 如果podsToActivate集合不为空，把其中的Pod激活放入ActiveQ，这里的待激活Pod是UnschedulableQ中的，通过检查外部条件的变化，使满足条件的Pod转换为活跃状态，需要通过Activate()方法激活，而BackoffQ依赖退避机制，重新入队的动作由内部计时器自动触发。 在绑定成功后把满足调度条件的Pod激活后，一次完整的调度流程就此结束了。总的来看调度器的设计是巧妙的，其中包含如Filter阶段的二次判断(结合NominatedPod的条件判断)，Score阶段的蓄水池算法等，以Scheduler Framework为骨架，十二个标准扩展点构成调度的流水线，扩展点的插件为其填充血肉，插件的精细策略构成调度器的灵魂。ActiveQ、BackoffQ、UnschedulableQ的组成的调度队列协同工作，实现了高并发场景下的调度效率和稳定性的平衡。 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/07-失败处理与抢占调度Preemption.html":{"url":"kube-scheduler/07-失败处理与抢占调度Preemption.html","title":"失败处理与抢占调度Preemption","keywords":"","body":" 抢占调度流程 抢占事件发生的时机 关键组件 评估器 创建评估器 提名器 抢占流程详解 抢占资格判断 获取候选节点 Pod干扰预算 节点批量选择 模拟抢占 在节点上选择被驱逐的Pod 执行扩展器过滤 获取最优候选节点 候选节点提名前准备 同步抢占 异步抢占 抢占的结束 调度的失败处理 失败处理与抢占调度Preemption 抢占调度流程 抢占事件发生的时机 在代码中搜索关键字Preempt会找到路径pkg/scheduler/framework/preemption下的Preempt()方法，向上寻找调用关系可以到路径pkg/scheduler/framework/plugins/defaultpreemption，被其中default_preemption.go中的PostFilter()方法直接调用，可以说明抢占流程作为调度插件之一存在于PostFilter扩展点。RunPostFilterPlugins()方法的调用发生在SchedulePod()方法返回的错误信息不为空时，也就是Predicates与Priorities两个阶段存在失败，没有能找到一个符合条件的节点运行Pod，方法调用如下方代码内容所示。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) if err != nil{ ...... fitError, ok := err.(*framework.FitError) ...... result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatus) ...... } ...... } RunPostFilterPlugins()方法与PostFilter()方法之间的逻辑与其他种类插件一致，下面从分析DefaultPreemption实现的PostFilter()方法开始分析抢占的流程。 func (pl *DefaultPreemption) PostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, m framework.NodeToStatusReader) (*framework.PostFilterResult, *framework.Status) { defer func() { metrics.PreemptionAttempts.Inc() }() result, status := pl.Evaluator.Preempt(ctx, state, pod, m) msg := status.Message() if len(msg) > 0 { return result, framework.NewStatus(status.Code(), \"preemption: \"+msg) } return result, status } PostFilter()中调用了Preempt()方法，它是实际的抢占逻辑入口，在简单了解抢占的相关组件后，再正式开始深入分析抢占逻辑的实现。 关键组件 评估器 在kube-scheduler中，Evaluator评估器是负责抢占流程的核心组件。在抢占中Evaluator负责流程控制和协调，具体的策略逻辑由实现了Interface接口的插件控制，PreemptPod()方法是抢占过程中的核心动作之一。 type Evaluator struct { // 插件名称 PluginName string // 调度框架句柄 Handler framework.Handle // Pod Lister接口 PodLister corelisters.PodLister // PDB LIster接口 PdbLister policylisters.PodDisruptionBudgetLister // 异步抢占开关 enableAsyncPreemption bool // 读写锁 mu sync.RWMutex // 抢占流程中的Pod集合 preempting sets.Set[types.UID] // 抢占逻辑函数 PreemptPod func(ctx context.Context, c Candidate, preemptor, victim *v1.Pod, pluginName string) error // 插件接口 Interface } Interface接口实现的方法如下，其中的方法实现在抢占流程中分析，此处不做说明。 type Interface interface { // 获取候选节点偏移量 GetOffsetAndNumCandidates(nodes int32) (int32, int32) // 候选节点到受害Pod的映射 CandidatesToVictimsMap(candidates []Candidate) map[string]*extenderv1.Victims // 判断Pod是否有抢占资格 PodEligibleToPreemptOthers(ctx context.Context, pod *v1.Pod, nominatedNodeStatus *framework.Status) (bool, string) // 在候选节点上选择受害Pod SelectVictimsOnNode(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, pdbs []*policy.PodDisruptionBudget) ([]*v1.Pod, int, *framework.Status) // 节点评分排序函数 OrderedScoreFuncs(ctx context.Context, nodesToVictims map[string]*extenderv1.Victims) []func(node string) int64 } 创建评估器 评估器实例的创建最早可以追溯到NewInTreeRegistry()函数，它实现了内置插件的创建和注册。插件结构包括调度框架句柄fh，特性开关集合fts，插件参数args，Pod和PDB的Lister以及评估器实例Evaluator。 func NewInTreeRegistry() runtime.Registry { ...... registry := runtime.Registry{ // 注册默认抢占插件 defaultpreemption.Name: runtime.FactoryAdapter(fts, defaultpreemption.New), } return registry } func New(_ context.Context, dpArgs runtime.Object, fh framework.Handle, fts feature.Features) (framework.Plugin, error) { args, ok := dpArgs.(*config.DefaultPreemptionArgs) if !ok { return nil, fmt.Errorf(\"got args of type %T, want *DefaultPreemptionArgs\", dpArgs) } if err := validation.ValidateDefaultPreemptionArgs(nil, args); err != nil { return nil, err } podLister := fh.SharedInformerFactory().Core().V1().Pods().Lister() pdbLister := getPDBLister(fh.SharedInformerFactory()) // 创建插件实例 pl := DefaultPreemption{ fh: fh, fts: fts, args: *args, podLister: podLister, pdbLister: pdbLister, } pl.Evaluator = preemption.NewEvaluator(Name, fh, &pl, fts.EnableAsyncPreemption) return &pl, nil } // 创建评估器 func NewEvaluator(pluginName string, fh framework.Handle, i Interface, enableAsyncPreemption bool) *Evaluator { podLister := fh.SharedInformerFactory().Core().V1().Pods().Lister() pdbLister := fh.SharedInformerFactory().Policy().V1().PodDisruptionBudgets().Lister() ev := &Evaluator{ PluginName: names.DefaultPreemption, Handler: fh, PodLister: podLister, PdbLister: pdbLister, Interface: i, enableAsyncPreemption: enableAsyncPreemption, preempting: sets.New[types.UID](), } // 注册PreemptPod()方法 ev.PreemptPod = func(ctx context.Context, c Candidate, preemptor, victim *v1.Pod, pluginName string) error { ...... } return ev } 提名器 在前面的学习中了解过调度队列的组件Nominator提名器，虽然它是SchedulingQueue的组件，但是和抢占流程也有一些关系。因为抢占插件返回的结构framework.PostFilterResult实际是一个NominatingInfo的指针，该结构的使用一般出现在提名器中。并且在抢占流程的最后，会修改Pod对象的Status.NominatedNodeName字段，在后面的调度周期中使用，如在Predicates阶段findNodesThatFitPod()方法的逻辑中，如果Pod.Status.NominatedNodeName不为空，会优先单独评估NominatedNodeName是否满足条件，如果不满足才会走后续标准流程。 type PostFilterResult struct { *NominatingInfo } type NominatingInfo struct { NominatedNodeName string NominatingMode NominatingMode } 抢占流程详解 前面已经做好了铺垫，在PostFilter()方法中执行Preempt()方法直接返回抢占的结果，所以从pkg/scheduler/framework/preemption/preemption.go路径的Preempt()方法开始详细的抢占流程分析。 根据源码注释可以看出，抢占大致分为六步： 第一步：通过PodLister获取Pod的最新信息状态; 第二步：确认这个Pod是否有资格进行抢占; 第三步：获取所有可发生抢占的候选节点; 第四步：调用注册的扩展器进一步缩小候选节点范围; 第五步：根据各种条件选择出最优的候选节点; 第六步：执行抢占，驱逐低优先级Pod; 这六个步骤执行后会返回抢占的最终结果给调度器，整体来看和标准的调度流程很类似，都会包括预选和优选的过程，实际上这些逻辑都紧紧围绕着调度器的本职工作：为Pod选择合适的Node，然后把Node的名称告诉Pod。 func (ev *Evaluator) Preempt(ctx context.Context, state *framework.CycleState, pod *v1.Pod, m framework.NodeToStatusReader) (*framework.PostFilterResult, *framework.Status) { logger := klog.FromContext(ctx) // 0) Fetch the latest version of . // It's safe to directly fetch pod here. Because the informer cache has already been // initialized when creating the Scheduler obj. // However, tests may need to manually initialize the shared pod informer. podNamespace, podName := pod.Namespace, pod.Name pod, err := ev.PodLister.Pods(pod.Namespace).Get(pod.Name) if err != nil { logger.Error(err, \"Could not get the updated preemptor pod object\", \"pod\", klog.KRef(podNamespace, podName)) return nil, framework.AsStatus(err) } // 1) Ensure the preemptor is eligible to preempt other pods. nominatedNodeStatus := m.Get(pod.Status.NominatedNodeName) if ok, msg := ev.PodEligibleToPreemptOthers(ctx, pod, nominatedNodeStatus); !ok { logger.V(5).Info(\"Pod is not eligible for preemption\", \"pod\", klog.KObj(pod), \"reason\", msg) return nil, framework.NewStatus(framework.Unschedulable, msg) } // 2) Find all preemption candidates. allNodes, err := ev.Handler.SnapshotSharedLister().NodeInfos().List() if err != nil { return nil, framework.AsStatus(err) } candidates, nodeToStatusMap, err := ev.findCandidates(ctx, state, allNodes, pod, m) if err != nil && len(candidates) == 0 { return nil, framework.AsStatus(err) } // Return a FitError only when there are no candidates that fit the pod. if len(candidates) == 0 { fitError := &framework.FitError{ Pod: pod, NumAllNodes: len(allNodes), Diagnosis: framework.Diagnosis{ NodeToStatus: nodeToStatusMap, // Leave UnschedulablePlugins or PendingPlugins as nil as it won't be used on moving Pods. }, } fitError.Diagnosis.NodeToStatus.SetAbsentNodesStatus(framework.NewStatus(framework.UnschedulableAndUnresolvable, \"Preemption is not helpful for scheduling\")) // Specify nominatedNodeName to clear the pod's nominatedNodeName status, if applicable. return framework.NewPostFilterResultWithNominatedNode(\"\"), framework.NewStatus(framework.Unschedulable, fitError.Error()) } // 3) Interact with registered Extenders to filter out some candidates if needed. candidates, status := ev.callExtenders(logger, pod, candidates) if !status.IsSuccess() { return nil, status } // 4) Find the best candidate. bestCandidate := ev.SelectCandidate(ctx, candidates) if bestCandidate == nil || len(bestCandidate.Name()) == 0 { return nil, framework.NewStatus(framework.Unschedulable, \"no candidate node for preemption\") } logger.V(2).Info(\"the target node for the preemption is determined\", \"node\", bestCandidate.Name(), \"pod\", klog.KObj(pod)) // 5) Perform preparation work before nominating the selected candidate. if ev.enableAsyncPreemption { ev.prepareCandidateAsync(bestCandidate, pod, ev.PluginName) } else { if status := ev.prepareCandidate(ctx, bestCandidate, pod, ev.PluginName); !status.IsSuccess() { return nil, status } } return framework.NewPostFilterResultWithNominatedNode(bestCandidate.Name()), framework.NewStatus(framework.Success) } 下面对每一个阶段分别进行分析，Pod信息更新作为常规操作不多说明。 抢占资格判断 通过NodeToStatusReader的Get()方法获取提名节点的状态信息记录，传递给PodEligibleToPreemptOthers()方法判断Pod是否有资格进行抢占操作。首先判断Pod.Spec.PreemptionPolicy中的抢占策略是否存在且不为Never，然后获取节点快照和Pod的NominatedNodeName，如果字段存在表示已经在之前进行过抢占流程了，现在又出现在PostFilter阶段表示抢占失败。此时需要判断这个Pod是否重新进行抢占操作，如果是UnschedulableAndUnresolvable表示提名节点因为某些问题出现了永久不可用的情况，允许开始重新抢占;如果是其他失败原因如Unschedulable则需要没有低于当前优先级的Pod正因抢占而退出才可以重新抢占，在这种因为临时资源导致抢占失败的情况下，为了避免资源浪费Pod还可以重试抢占尝试调度到其他节点。一般来说，抢占解决的是Unschedulable的问题，而UnschedulableAndUnresolvable的重试是上一次调度到当前调度周期之间发生了预期以外的变化，所以允许重新抢占。 回顾一下Filter扩展点的错误状态，Unschedulable表示临时的调度失败，如CPU资源不足、Pod间亲和性不满足等情况，可能下一轮调度情况变化就会可以调度了，这种情况不用人工干预只需要调度器重试;UnschedulableAndUnresolvable表示硬性条件的不满足，比如NodeSelector节点标签不满足、PV绑定失败等情况，其中临时条件和永久条件的关键区别在于资源是否随着Pod的生命周期而发生变化。 // 1) Ensure the preemptor is eligible to preempt other pods. nominatedNodeStatus := m.Get(pod.Status.NominatedNodeName) if ok, msg := ev.PodEligibleToPreemptOthers(ctx, pod, nominatedNodeStatus); !ok { logger.V(5).Info(\"Pod is not eligible for preemption\", \"pod\", klog.KObj(pod), \"reason\", msg) return nil, framework.NewStatus(framework.Unschedulable, msg) } func (pl *DefaultPreemption) PodEligibleToPreemptOthers(_ context.Context, pod *v1.Pod, nominatedNodeStatus *framework.Status) (bool, string) { // 确认抢占策略开启 if pod.Spec.PreemptionPolicy != nil && *pod.Spec.PreemptionPolicy == v1.PreemptNever { return false, \"not eligible due to preemptionPolicy=Never.\" } // 从快照获取节点信息 nodeInfos := pl.fh.SnapshotSharedLister().NodeInfos() nomNodeName := pod.Status.NominatedNodeName // 如果Pod已经经过一轮抢占计算并且有NominatedNodeName // 但是还在本轮调度的Filter阶段失败而走到了抢占流程 if len(nomNodeName) > 0 { // 抢占失败 允许重试 if nominatedNodeStatus.Code() == framework.UnschedulableAndUnresolvable { return true, \"\" } // 遍历提名节点的Pod if nodeInfo, _ := nodeInfos.Get(nomNodeName); nodeInfo != nil { podPriority := corev1helpers.PodPriority(pod) for _, p := range nodeInfo.Pods { if corev1helpers.PodPriority(p.Pod) 获取候选节点 确认Pod可以进行抢占后，通过NodeLister获取全量节点信息，findCandidates返回所有候选节点和状态。然后通过NodeToStatusReader接口获取信息记录为Unschedulable的节点，因为UnschedulableAndUnresolvable是硬性条件的不满足，这种条件不会以Pod的生命周期变化转换为满足，所以不在抢占的考虑范围以内。如果此时的潜在节点potentialNodes长度为0，已经没有合适的节点可以发生抢占，会清除当前Pod的NominatedNodeName信息并结束。potentialNodes长度大于0时，先获取集群中的所有PDB信息，驱逐某个Pod可能还会影响到其他命名空间的PDB，所以此处获取全量对象。 func (ev *Evaluator) findCandidates(ctx context.Context, state *framework.CycleState, allNodes []*framework.NodeInfo, pod *v1.Pod, m framework.NodeToStatusReader) ([]Candidate, *framework.NodeToStatus, error) { // 全量节点数量判断 if len(allNodes) == 0 { return nil, nil, errors.New(\"no nodes available\") } logger := klog.FromContext(ctx) // 通过状态Unschedulable初步缩小潜在候选节点的范围 potentialNodes, err := m.NodesForStatusCode(ev.Handler.SnapshotSharedLister().NodeInfos(), framework.Unschedulable) if err != nil { return nil, nil, err } // 潜在节点数量判断 if len(potentialNodes) == 0 { logger.V(3).Info(\"Preemption will not help schedule pod on any node\", \"pod\", klog.KObj(pod)) // In this case, we should clean-up any existing nominated node name of the pod. if err := util.ClearNominatedNodeName(ctx, ev.Handler.ClientSet(), pod); err != nil { logger.Error(err, \"Could not clear the nominatedNodeName field of pod\", \"pod\", klog.KObj(pod)) // We do not return as this error is not critical. } return nil, framework.NewDefaultNodeToStatus(), nil } // 获取PDB信息 pdbs, err := getPodDisruptionBudgets(ev.PdbLister) if err != nil { return nil, nil, err } // 确定当前批次候选节点范围 offset, candidatesNum := ev.GetOffsetAndNumCandidates(int32(len(potentialNodes))) // 执行预抢占模拟 return ev.DryRunPreemption(ctx, state, pod, potentialNodes, pdbs, offset, candidatesNum) } Pod干扰预算 PDB(PodDisruptionBudget，Pod干扰预算)用于控制Pod副本的最小可用/最大不可用数量，保护应用避免发生服务中断。涉及到如抢占驱逐、节点排空、滚动更新、水平扩缩容等场景，该特性在1.21版本进入稳定状态，详情可见官方文档。 节点批量选择 GetOffsetAndNumCandidates()方法接收一个INT32的整数，返回两个值，分别代表选择节点的起始偏移量和候选节点数量。 func (pl *DefaultPreemption) GetOffsetAndNumCandidates(numNodes int32) (int32, int32) { return rand.Int31n(numNodes), pl.calculateNumCandidates(numNodes) } 候选节点数量的选择规则如下，此处涉及到两个参数值MinCandidateNodesPercentage和MinCandidateNodesAbsolute，分别表示最小百分比和最小绝对值，这两个变量的默认值可以在pkg/scheduler/apis/config/testing/defaults/defaults.go中找到。 var PluginConfigsV1 = []config.PluginConfig{ { Name: \"DefaultPreemption\", Args: &config.DefaultPreemptionArgs{ MinCandidateNodesPercentage: 10, MinCandidateNodesAbsolute: 100, }, }, } 采样数量=节点总数*最小百分比，且不小于最小绝对值，不大于节点总数。 func (pl *DefaultPreemption) calculateNumCandidates(numNodes int32) int32 { // 采样数量=节点总数*最小百分比 n := (numNodes * pl.args.MinCandidateNodesPercentage) / 100 // 如果采样数量节点总数 if n > numNodes { // 采样数量=节点总数 n = numNodes } return n } 模拟抢占 DryRunPreemption()方法进行模拟抢占，根据函数签名，接收上下文ctx、调度状态state、抢占主体pod、潜在节点列表potentialNodes、Pod干扰预算pdbs、索引偏移量offset、采样数量candidatesNum，返回[]Candidate类型的候选节点列表和NodeToStatus类型的节点状态映射。具体的逻辑实现是先初始化两个列表分别记录违反PDB和不违反PDB的候选节点，然后使用并行器在所有的采样节点中执行闭包函数checkNode()来获取每个节点上能让出足够资源的最小Pod集合，然后经过类型转换为Candidate对象，并根据是否违反PDB做区分加入对应的列表中，如果没有成功返回Pod集合victims，就更新该节点的状态信息，最终返回合并后的节点列表和节点状态信息。 func (ev *Evaluator) DryRunPreemption(ctx context.Context, state *framework.CycleState, pod *v1.Pod, potentialNodes []*framework.NodeInfo, pdbs []*policy.PodDisruptionBudget, offset int32, candidatesNum int32) ([]Candidate, *framework.NodeToStatus, error) { fh := ev.Handler // 初始化违反PDB和不违反PDB的候选节点列表 nonViolatingCandidates := newCandidateList(candidatesNum) violatingCandidates := newCandidateList(candidatesNum) ctx, cancel := context.WithCancel(ctx) defer cancel() nodeStatuses := framework.NewDefaultNodeToStatus() logger := klog.FromContext(ctx) logger.V(5).Info(\"Dry run the preemption\", \"potentialNodesNumber\", len(potentialNodes), \"pdbsNumber\", len(pdbs), \"offset\", offset, \"candidatesNumber\", candidatesNum) var statusesLock sync.Mutex var errs []error // 节点检查函数 checkNode := func(i int) { // 根据偏移量和索引拷贝节点信息 nodeInfoCopy := potentialNodes[(int(offset)+i)%len(potentialNodes)].Snapshot() logger.V(5).Info(\"Check the potential node for preemption\", \"node\", nodeInfoCopy.Node().Name) // 拷贝状态信息 stateCopy := state.Clone() // 核心方法 挑选被驱逐的Pod pods, numPDBViolations, status := ev.SelectVictimsOnNode(ctx, stateCopy, pod, nodeInfoCopy, pdbs) // 如果成功选到victims 先做类型转换 然后加入对应的列表 if status.IsSuccess() && len(pods) != 0 { victims := extenderv1.Victims{ Pods: pods, NumPDBViolations: int64(numPDBViolations), } c := &candidate{ victims: &victims, name: nodeInfoCopy.Node().Name, } if numPDBViolations == 0 { nonViolatingCandidates.add(c) } else { violatingCandidates.add(c) } // 采样节点达到数量后停止计算 nvcSize, vcSize := nonViolatingCandidates.size(), violatingCandidates.size() if nvcSize > 0 && nvcSize+vcSize >= candidatesNum { cancel() } return } // 如果没有在节点上选到victims 更新节点状态记录 if status.IsSuccess() && len(pods) == 0 { status = framework.AsStatus(fmt.Errorf(\"expected at least one victim pod on node %q\", nodeInfoCopy.Node().Name)) } statusesLock.Lock() if status.Code() == framework.Error { errs = append(errs, status.AsError()) } nodeStatuses.Set(nodeInfoCopy.Node().Name, status) statusesLock.Unlock() } // 并行检查潜在节点 fh.Parallelizer().Until(ctx, len(potentialNodes), checkNode, ev.PluginName) // 合并返回不违反PDB与违反PDB的候选节点列表 return append(nonViolatingCandidates.get(), violatingCandidates.get()...), nodeStatuses, utilerrors.NewAggregate(errs) } 在节点上选择被驱逐的Pod 并行器会在每个潜在的候选节点上执行checkNode()函数，其中的SelectVictimsOnNode()方法会在节点上找出能为抢占Pod让出足够资源的最小Pod集合。首先会初始化一个潜在受害者列表potentialVictims，然后遍历节点上的所有Pod，比较优先级把所有低于抢占者的Pod加入这个列表，并且临时移除这些Pod。此时已经没有更多的Pod可以被抢占驱逐了，执行在标准Filter流程中就使用的RunFilterPluginsWithNominatedPods()方法确认抢占者是否可以调度，如果仍然不可调度表示即使抢占也无法调度到该节点。如果可以调度，那么下一步就需要寻找最小的驱逐成本了，先根据是否违反PDB对这些低优先级的Pod进行分类，然后优先尝试恢复违反PDB的Pod，因为这类Pod更不希望收到影响。恢复Pod就是先在NodeInfo中添加Pod信息，然后执行RunFilterPluginsWithNominatedPods()方法验证Pod是否仍可以调度，如果添加后导致资源不足以让抢占者调度，那么添加这个Pod到victims列表，如果是违反PDB受害者的Pod，还需要对numViolatingVictim计数加一，该变量会返回给上层并作为评估标准之一。如果违反PDB和不违反PDB的节点都存在，需要对victims按优先级进行一次排序，最终返回受害者列表、违反PDB的受害者数量和成功状态。 SelectVictimsOnNode()方法作为抢占的核心逻辑之一，使用了最大删除-验证-逐步恢复的筛选策略，使用闭包函数减少了代码冗余;逐步恢复阶段分别处理两类受害者列表，优先保障了高优先级Pod和服务可用(PDB)。体现了调度器在复杂场景下的实用主义思想，在算法复杂度和执行效率之间寻找动态平衡点。 func (pl *DefaultPreemption) SelectVictimsOnNode( ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, pdbs []*policy.PodDisruptionBudget) ([]*v1.Pod, int, *framework.Status) { logger := klog.FromContext(ctx) // 初始化潜在受害者列表 var potentialVictims []*framework.PodInfo // 定义闭包函数 移除Pod removePod := func(rpi *framework.PodInfo) error { if err := nodeInfo.RemovePod(logger, rpi.Pod); err != nil { return err } // 预过滤扩展的执行保证Pod被安全移除/添加 status := pl.fh.RunPreFilterExtensionRemovePod(ctx, state, pod, rpi, nodeInfo) if !status.IsSuccess() { return status.AsError() } return nil } // 定义闭包函数 添加Pod addPod := func(api *framework.PodInfo) error { nodeInfo.AddPodInfo(api) status := pl.fh.RunPreFilterExtensionAddPod(ctx, state, pod, api, nodeInfo) if !status.IsSuccess() { return status.AsError() } return nil } // 在节点中找出所有优先级低于抢占者的Pod加入potentialVictims列表 并暂时从节点上移除它们 podPriority := corev1helpers.PodPriority(pod) for _, pi := range nodeInfo.Pods { if corev1helpers.PodPriority(pi.Pod) 执行扩展器过滤 获取到候选节点和受害者后，会遍历执行注册的扩展器并执行其逻辑，把每个扩展器的输出作为下一个扩展器的输入，最终经过所有扩展器的过滤，返回了更小范围的候选节点列表。 HTTPExtender是通过HTTP接口调用外部扩展程序的机制，extender.ProcessPreemption()方法把抢占信息封装成HTTP请求发送给外部程序，接收到外部程序的响应后将其转换为内部数据形式并返回。 func (ev *Evaluator) callExtenders(logger klog.Logger, pod *v1.Pod, candidates []Candidate) ([]Candidate, *framework.Status) { // 获取注册的扩展器 extenders := ev.Handler.Extenders() nodeLister := ev.Handler.SnapshotSharedLister().NodeInfos() if len(extenders) == 0 { return candidates, nil } // []Candidate类型转换为map[string]*extenderv1.Victims victimsMap := ev.CandidatesToVictimsMap(candidates) if len(victimsMap) == 0 { return candidates, nil } // 遍历候选节点执行 for _, extender := range extenders { if !extender.SupportsPreemption() || !extender.IsInterested(pod) { continue } nodeNameToVictims, err := extender.ProcessPreemption(pod, victimsMap, nodeLister) if err != nil { if extender.IsIgnorable() { logger.Info(\"Skipped extender as it returned error and has ignorable flag set\", \"extender\", extender.Name(), \"err\", err) continue } return nil, framework.AsStatus(err) } // 校验返回结果 for nodeName, victims := range nodeNameToVictims { // 如果是无效节点 返回错误 if victims == nil || len(victims.Pods) == 0 { if extender.IsIgnorable() { delete(nodeNameToVictims, nodeName) logger.Info(\"Ignored node for which the extender didn't report victims\", \"node\", klog.KRef(\"\", nodeName), \"extender\", extender.Name()) continue } return nil, framework.AsStatus(fmt.Errorf(\"expected at least one victim pod on node %q\", nodeName)) } } // 更新变量结果 传入下一个扩展器 victimsMap = nodeNameToVictims if len(victimsMap) == 0 { break } } // 转换回[]Candidate类型并返回 var newCandidates []Candidate for nodeName := range victimsMap { newCandidates = append(newCandidates, &candidate{ victims: victimsMap[nodeName], name: nodeName, }) } return newCandidates, nil } 获取最优候选节点 这个阶段可以理解为标准流程中的Priorities，根据执行评分插件，选出最适合抢占的候选节点，其中的核心函数是pickOneNodeForPreemption()，作用可以类比标准流程的RunScorePlugins()方法。 func (ev *Evaluator) SelectCandidate(ctx context.Context, candidates []Candidate) Candidate { logger := klog.FromContext(ctx) if len(candidates) == 0 { return nil } // 只有一个候选节点直接返回 等同于len(feasibleNode)==1的场景 if len(candidates) == 1 { return candidates[0] } // 数据类型转换 victimsMap := ev.CandidatesToVictimsMap(candidates) // 获取评分函数列表 scoreFuncs := ev.OrderedScoreFuncs(ctx, victimsMap) // 执行评分函数 candidateNode := pickOneNodeForPreemption(logger, victimsMap, scoreFuncs) // 返回最有节点对象 if victims := victimsMap[candidateNode]; victims != nil { return &candidate{ victims: victims, name: candidateNode, } } // 异常处理 不会执行到此处 logger.Error(errors.New(\"no candidate selected\"), \"Should not reach here\", \"candidates\", candidates) // 避免流程结束 return candidates[0] } pickOneNodeForPreemption()函数实现了节点的选择，根据对代码的分析，它的实现比Priorities要简单很多，不涉及节点的并行计算和归一化。使用贪心算法不断更新最高评分和最高分节点列表，并把当前评分函数的输出作为下一个评分函数的输入。值得一提的是，评分函数列表中的策略是存在优先级的，前一个评分函数的重要性永远高于后一个评分函数，只有当一个评分函数无法完全区分节点是否最优时，才会进入策略链的下一环继续评估。如果某一个评分函数只得到了一个最高分节点，那么就可以直接返回该节点了。 func pickOneNodeForPreemption(logger klog.Logger, nodesToVictims map[string]*extenderv1.Victims, scoreFuncs []func(node string) int64) string { if len(nodesToVictims) == 0 { return \"\" } // 初始化候选节点列表 allCandidates := make([]string, 0, len(nodesToVictims)) for node := range nodesToVictims { allCandidates = append(allCandidates, node) } // 如果没有获取到评分函数 设置默认函数 if len(scoreFuncs) == 0 { // 最少违反PDB minNumPDBViolatingScoreFunc := func(node string) int64 { // 违反PDB受害者数量越少评分越高 return -nodesToVictims[node].NumPDBViolations } // 最高优先级受害者等级低 minHighestPriorityScoreFunc := func(node string) int64 { // 受害者列表中第一个(最高优先级)元素的优先级越低评分越高 highestPodPriority := corev1helpers.PodPriority(nodesToVictims[node].Pods[0]) return -int64(highestPodPriority) } // 受害者优先级总和最小 minSumPrioritiesScoreFunc := func(node string) int64 { var sumPriorities int64 for _, pod := range nodesToVictims[node].Pods { // 受害者的优先级总和越小评分越高 sumPriorities += int64(corev1helpers.PodPriority(pod)) + int64(math.MaxInt32+1) } return -sumPriorities } // 受害者数量最少 minNumPodsScoreFunc := func(node string) int64 { // 受害者数量越少评分越高 return -int64(len(nodesToVictims[node].Pods)) } // 最高优先级受害者的启动时间最晚 latestStartTimeScoreFunc := func(node string) int64 { // 最高优先级受害者的启动时间越晚评分越高 earliestStartTimeOnNode := util.GetEarliestPodStartTime(nodesToVictims[node]) if earliestStartTimeOnNode == nil { logger.Error(errors.New(\"earliestStartTime is nil for node\"), \"Should not reach here\", \"node\", node) return int64(math.MinInt64) } return earliestStartTimeOnNode.UnixNano() } // 注册默认函数到scoreFuncs scoreFuncs = []func(string) int64{ // A node with a minimum number of PDB is preferable. minNumPDBViolatingScoreFunc, // A node with a minimum highest priority victim is preferable. minHighestPriorityScoreFunc, // A node with the smallest sum of priorities is preferable. minSumPrioritiesScoreFunc, // A node with the minimum number of pods is preferable. minNumPodsScoreFunc, // A node with the latest start time of all highest priority victims is preferable. latestStartTimeScoreFunc, // If there are still ties, then the first Node in the list is selected. } } // 执行所有评分函数 for _, f := range scoreFuncs { selectedNodes := []string{} maxScore := int64(math.MinInt64) // 遍历每个候选节点 for _, node := range allCandidates { score := f(node) if score > maxScore { maxScore = score selectedNodes = []string{} } if score == maxScore { selectedNodes = append(selectedNodes, node) } } // 如果经过筛选只有最后一个候选节点直接返回 if len(selectedNodes) == 1 { return selectedNodes[0] } // 更新节点列表 allCandidates = selectedNodes } // 返回首个元素 return allCandidates[0] } 候选节点提名前准备 确定了候选节点和受害者以后，就需要把这些倒霉的Pod从节点上驱逐出去，当前Kubernetes支持同步抢占和异步抢占两种方式。调度器根据特性门控EnableAsyncPreemption的开关决定使用同步或异步。 if ev.enableAsyncPreemption { // 异步抢占 ev.prepareCandidateAsync(bestCandidate, pod, ev.PluginName) } else { // 同步抢占 if status := ev.prepareCandidate(ctx, bestCandidate, pod, ev.PluginName); !status.IsSuccess() { return nil, status } } 同步抢占 同步抢占通过prepareCandidate()方法实现，通过并行器驱逐所有受害者，然后检查是否存在比抢占者优先级低但是提名了同一个节点的Pod，如果存在就清除它们的NominatedNodeName字段。 func (ev *Evaluator) prepareCandidate(ctx context.Context, c Candidate, pod *v1.Pod, pluginName string) *framework.Status { // 初始化对象 fh := ev.Handler cs := ev.Handler.ClientSet() ctx, cancel := context.WithCancel(ctx) defer cancel() logger := klog.FromContext(ctx) errCh := parallelize.NewErrorChannel() // 并行驱逐所有受害者 fh.Parallelizer().Until(ctx, len(c.Victims().Pods), func(index int) { if err := ev.PreemptPod(ctx, c, pod, c.Victims().Pods[index], pluginName); err != nil { errCh.SendErrorWithCancel(err, cancel) } }, ev.PluginName) if err := errCh.ReceiveError(); err != nil { return framework.AsStatus(err) } // 记录监控指标 metrics.PreemptionVictims.Observe(float64(len(c.Victims().Pods))) // 是否有低于抢占者优先级的Pod提名到同一节点 清除NominatedNodeName字段 nominatedPods := getLowerPriorityNominatedPods(logger, fh, pod, c.Name()) if err := util.ClearNominatedNodeName(ctx, cs, nominatedPods...); err != nil { // 如果清除NominatedNodeName字段失败不会影响流程 logger.Error(err, \"Cannot clear 'NominatedNodeName' field\") } return nil } 上面有提到PreemptPod()方法是在创建评估器时注册的，具体实现如下。首先判断受害者Pod是否在等待准入插件返回结果，如果正处于Waiting状态则直接调用Reject()方法终止其调度。如果不是Waiting状态先构造一个新的状态信息，然后先进行本地更新，如果本地更新成功才会调用API接口更新API Server中受害者Pod的状态信息并删除实例对象。 PreemptPod = func(ctx context.Context, c Candidate, preemptor, victim *v1.Pod, pluginName string) error { logger := klog.FromContext(ctx) // 检查受害者Pod是否为WaitingPod if waitingPod := ev.Handler.GetWaitingPod(victim.UID); waitingPod != nil { // 如果是WaitingPod 直接调用Reject方法 waitingPod.Reject(pluginName, \"preempted\") logger.V(2).Info(\"Preemptor pod rejected a waiting pod\", \"preemptor\", klog.KObj(preemptor), \"waitingPod\", klog.KObj(victim), \"node\", c.Name()) } else { // 受害者Pod不是WaitingPod // 构造PodCondition信息 condition := &v1.PodCondition{ Type: v1.DisruptionTarget, Status: v1.ConditionTrue, Reason: v1.PodReasonPreemptionByScheduler, Message: fmt.Sprintf(\"%s: preempting to accommodate a higher priority pod\", preemptor.Spec.SchedulerName), } newStatus := victim.Status.DeepCopy() // 本地更新Pod状态 updated := apipod.UpdatePodCondition(newStatus, condition) if updated { // 本地状态成功更新后调用Api对Pod状态进行Patch if err := util.PatchPodStatus(ctx, ev.Handler.ClientSet(), victim, newStatus); err != nil { logger.Error(err, \"Could not add DisruptionTarget condition due to preemption\", \"pod\", klog.KObj(victim), \"preemptor\", klog.KObj(preemptor)) return err } } // 删除Pod if err := util.DeletePod(ctx, ev.Handler.ClientSet(), victim); err != nil { if !apierrors.IsNotFound(err) { logger.Error(err, \"Tried to preempted pod\", \"pod\", klog.KObj(victim), \"preemptor\", klog.KObj(preemptor)) return err } logger.V(2).Info(\"Victim Pod is already deleted\", \"preemptor\", klog.KObj(preemptor), \"victim\", klog.KObj(victim), \"node\", c.Name()) return nil } logger.V(2).Info(\"Preemptor Pod preempted victim Pod\", \"preemptor\", klog.KObj(preemptor), \"victim\", klog.KObj(victim), \"node\", c.Name()) } ev.Handler.EventRecorder().Eventf(victim, preemptor, v1.EventTypeNormal, \"Preempted\", \"Preempting\", \"Preempted by pod %v on node %v\", preemptor.UID, c.Name()) return nil } 异步抢占 异步抢占能够避免阻塞调度主流程，可以提高调度器的吞吐量和响应速度。异步抢占的核心逻辑和同步抢占基本相同，但是异步抢占会额外维护评估器中的preempting集合，在其中记录正处于抢占状态的Pod，并通过加锁/解锁避免并发问题。在驱逐受害者Pod时，同步抢占直接并行驱逐所有，而异步抢占的驱逐动作通过协程启动，首先并行驱逐N-1个受害者Pod，驱逐成功后删除preempting集合中的对象，并单独驱逐最后一个受害者Pod。 func (ev *Evaluator) prepareCandidateAsync(c Candidate, pod *v1.Pod, pluginName string) { metrics.PreemptionVictims.Observe(float64(len(c.Victims().Pods))) // 初始化对象 ctx, cancel := context.WithCancel(context.Background()) errCh := parallelize.NewErrorChannel() // 和同步抢占相同 preemptPod := func(index int) { victim := c.Victims().Pods[index] if err := ev.PreemptPod(ctx, c, pod, victim, pluginName); err != nil { errCh.SendErrorWithCancel(err, cancel) } } // 评估器的preempting集合保存正在抢占的Pod ev.mu.Lock() ev.preempting.Insert(pod.UID) ev.mu.Unlock() logger := klog.FromContext(ctx) // 启动协程执行 go func() { startTime := time.Now() result := metrics.GoroutineResultSuccess defer metrics.PreemptionGoroutinesDuration.WithLabelValues(result).Observe(metrics.SinceInSeconds(startTime)) defer metrics.PreemptionGoroutinesExecutionTotal.WithLabelValues(result).Inc() defer func() { if result == metrics.GoroutineResultError { // 如果最终结果失败 把Pod放回到activeQ ev.Handler.Activate(logger, map[string]*v1.Pod{pod.Name: pod}) } }() defer cancel() logger.V(2).Info(\"Start the preemption asynchronously\", \"preemptor\", klog.KObj(pod), \"node\", c.Name(), \"numVictims\", len(c.Victims().Pods)) // 是否有低于抢占者优先级的Pod提名到同一节点 清除NominatedNodeName字段 nominatedPods := getLowerPriorityNominatedPods(logger, ev.Handler, pod, c.Name()) if err := util.ClearNominatedNodeName(ctx, ev.Handler.ClientSet(), nominatedPods...); err != nil { // 如果清除NominatedNodeName字段失败不会影响流程 logger.Error(err, \"Cannot clear 'NominatedNodeName' field from lower priority pods on the same target node\", \"node\", c.Name()) result = metrics.GoroutineResultError } // 如果没有受害者Pod需要驱逐 从评估器的preempting集合中删除当前Pod并返回 if len(c.Victims().Pods) == 0 { ev.mu.Lock() delete(ev.preempting, pod.UID) ev.mu.Unlock() return } // 并行删除N-1个受害者Pod ev.Handler.Parallelizer().Until(ctx, len(c.Victims().Pods)-1, preemptPod, ev.PluginName) if err := errCh.ReceiveError(); err != nil { logger.Error(err, \"Error occurred during async preemption\") result = metrics.GoroutineResultError } // 从评估器的preempting集合中删除当前Pod ev.mu.Lock() delete(ev.preempting, pod.UID) ev.mu.Unlock() // 删除最后一个受害者Pod if err := ev.PreemptPod(ctx, c, pod, c.Victims().Pods[len(c.Victims().Pods)-1], pluginName); err != nil { logger.Error(err, \"Error occurred during async preemption\") result = metrics.GoroutineResultError } logger.V(2).Info(\"Async Preemption finished completely\", \"preemptor\", klog.KObj(pod), \"node\", c.Name(), \"result\", result) }() } 抢占的结束 在成功驱逐了所有受害者Pod后，返回PostFilterResult类型的抢占结果，在其中包含了提名节点的名称。组装错误信息并返回，这一个调度周期就结束了，在后续的错误处理环节，如果发现结果的nominatingInfo字段不为空，则修改Pod的Status.NominatedNodeName字段为nominatingInfo的值，Pod状态设置为Unschedulable状态，在后面的调度周期会重新入队并尝试调度到提名节点。 func (sched *Scheduler) schedulingCycle( ctx context.Context, state *framework.CycleState, fwk framework.Framework, podInfo *framework.QueuedPodInfo, start time.Time, podsToActivate *framework.PodsToActivate, ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) { logger := klog.FromContext(ctx) pod := podInfo.Pod scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod) if err != nil { ...... // 抢占流程 result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatus) msg := status.Message() fitError.Diagnosis.PostFilterMsg = msg if status.Code() == framework.Error { logger.Error(nil, \"Status after running PostFilter plugins for pod\", \"pod\", klog.KObj(pod), \"status\", msg) } else { logger.V(5).Info(\"Status after running PostFilter plugins for pod\", \"pod\", klog.KObj(pod), \"status\", msg) } var nominatingInfo *framework.NominatingInfo if result != nil { nominatingInfo = result.NominatingInfo } // 返回抢占结果和Unschedulable状态 return ScheduleResult{nominatingInfo: nominatingInfo}, podInfo, framework.NewStatus(framework.Unschedulable).WithError(err) } ...... } func (sched *Scheduler) ScheduleOne(ctx context.Context) { ...... scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() { // 进行错误处理 sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return } ...... } 调度的失败处理 在ScheduleOne()方法中可以看到调度器在两个位置进行了失败处理，不难想到这两处就是SchedulingCycle与BindingCycle的结尾，在两个生命周期结束时进行错误判断与处理。 func (sched *Scheduler) ScheduleOne(ctx context.Context) { ...... scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate) if !status.IsSuccess() { sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start) return } go func() { ...... status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate) if !status.IsSuccess() { sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status) return } }() } 失败处理接口FailureHandler是FailureHandlerFn类型的函数，在调度器创建时的applyDefaultHandlers()方法设置。 func (sched *Scheduler) applyDefaultHandlers() { sched.SchedulePod = sched.schedulePod sched.FailureHandler = sched.handleSchedulingFailure } 下面来详细分析handleSchedulingFailure()方法内的逻辑。首先来看函数签名，该方法接收六个参数，分别是上下文信息ctx，调度配置fwk，Pod信息podInfo，返回状态status，提名信息nominatingInfo和调度起始时间start。整体上包括调度事件记录、Pod提名节点信息处理、Pod对象重新入队和Pod状态更新。 func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) { calledDone := false defer func() { if !calledDone { // 一般情况下AddUnschedulableIfNotPresent内部会调用SchedulingQueue.Done(pod.UID) // 避免没有调用的特殊情况 正确释放Pod资源 sched.SchedulingQueue.Done(podInfo.Pod.UID) } }() logger := klog.FromContext(ctx) // 初始化错误原因 reason := v1.PodReasonSchedulerError if status.IsRejected() { // 如果状态是被拒绝表示不可调度 reason = v1.PodReasonUnschedulable } // 记录指标 switch reason { case v1.PodReasonUnschedulable: metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start)) case v1.PodReasonSchedulerError: metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start)) } // 获取失败信息 pod := podInfo.Pod err := status.AsError() errMsg := status.Message() if err == ErrNoNodesAvailable { // 集群中没有Node注册 logger.V(2).Info(\"Unable to schedule pod; no nodes are registered to the cluster; waiting\", \"pod\", klog.KObj(pod)) } else if fitError, ok := err.(*framework.FitError); ok { // 不符合条件被调度插件拒绝 // 记录UnschedulablePlugins和PendingPlugins podInfo.UnschedulablePlugins = fitError.Diagnosis.UnschedulablePlugins podInfo.PendingPlugins = fitError.Diagnosis.PendingPlugins logger.V(2).Info(\"Unable to schedule pod; no fit; waiting\", \"pod\", klog.KObj(pod), \"err\", errMsg) } else { // 其他内部错误 logger.Error(err, \"Error scheduling pod; retrying\", \"pod\", klog.KObj(pod)) } // 使用Lister获取最新信息并在其中查找当前Pod podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister() cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name) if e != nil { logger.Info(\"Pod doesn't exist in informer cache\", \"pod\", klog.KObj(pod), \"err\", e) } else { // 检查是否有NodeName信息 if len(cachedPod.Spec.NodeName) != 0 { logger.Info(\"Pod has been assigned to node. Abort adding it back to queue.\", \"pod\", klog.KObj(pod), \"node\", cachedPod.Spec.NodeName) } else { // 没有NodeName信息 把Pod深拷贝一份后重新入队 podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy()) if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(logger, podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil { logger.Error(err, \"Error occurred\") } calledDone = true } } // 尝试添加带有提名节点信息的Pod到Nominator if sched.SchedulingQueue != nil { sched.SchedulingQueue.AddNominatedPod(logger, podInfo.PodInfo, nominatingInfo) } if err == nil { return } // 记录事件 msg := truncateMessage(errMsg) fwk.EventRecorder().Eventf(pod, nil, v1.EventTypeWarning, \"FailedScheduling\", \"Scheduling\", msg) // 更新Pod状态 包括提名节点信息和Condition if err := updatePod(ctx, sched.client, pod, &v1.PodCondition{ Type: v1.PodScheduled, Status: v1.ConditionFalse, Reason: reason, Message: errMsg, }, nominatingInfo); err != nil { logger.Error(err, \"Error updating pod\", \"pod\", klog.KObj(pod)) } } © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-scheduler/08-调度器扩展与实践.html":{"url":"kube-scheduler/08-调度器扩展与实践.html","title":"调度器扩展与实践","keywords":"","body":"调度器扩展与实践 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 14:59:34 "},"kube-controller-manager/":{"url":"kube-controller-manager/","title":"Controller Manager","keywords":"","body":"kube-controller-manager © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 13:52:51 "},"kube-controller-manager/01-ControllerManager创建流程.html":{"url":"kube-controller-manager/01-ControllerManager创建流程.html","title":"ControllerManager创建流程","keywords":"","body":" 控制器创建入口函数 控制器核心创建逻辑 ServiceAccountToken控制器的创建 ControllerManager创建流程 控制器创建入口函数 根据在之前调度器学习过程中对Cobra框架构建组件的了解，首先就会想到kube-controller- manager的创建入口也在cmd/kube-controller-manager/controller-manager.go中，其中同样也只包含简单的三行代码。 func main() { command := app.NewControllerManagerCommand() code := cli.Run(command) os.Exit(code) } 这和调度器中是完全相同的，下面进入cmd/kube-controller-manager/app/controllermanager.go路径下去看具体逻辑。还是关注RunE()中return的Run()函数。 // NewControllerManagerCommand creates a *cobra.Command object with default parameters func NewControllerManagerCommand() *cobra.Command { // 初始化特性门控 _, _ = featuregate.DefaultComponentGlobalsRegistry.ComponentGlobalsOrRegister( featuregate.DefaultKubeComponent, utilversion.DefaultBuildEffectiveVersion(), utilfeature.DefaultMutableFeatureGate) // 初始化配置信息KubeControllerManagerOptions对象 s, err := options.NewKubeControllerManagerOptions() if err != nil { klog.Background().Error(err, \"Unable to initialize command options\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } cmd := &cobra.Command{ ...... // 核心逻辑 RunE: func(cmd *cobra.Command, args []string) error { verflag.PrintAndExitIfRequested() // Activate logging as soon as possible, after that // show flags with the final logging configuration. if err := logsapi.ValidateAndApply(s.Logs, utilfeature.DefaultFeatureGate); err != nil { return err } cliflag.PrintFlags(cmd.Flags()) // KubeControllerManagerOptions-->Config c, err := s.Config(KnownControllers(), ControllersDisabledByDefault(), ControllerAliases()) if err != nil { return err } // add feature enablement metrics fg := s.ComponentGlobalsRegistry.FeatureGateFor(featuregate.DefaultKubeComponent) fg.(featuregate.MutableFeatureGate).AddMetrics() // 传入CompletedConfig创建控制器实例 return Run(context.Background(), c.Complete()) }, Args: func(cmd *cobra.Command, args []string) error { for _, arg := range args { if len(arg) > 0 { return fmt.Errorf(\"%q does not take any arguments, got %q\", cmd.CommandPath(), args) } } return nil }, } fs := cmd.Flags() namedFlagSets := s.Flags(KnownControllers(), ControllersDisabledByDefault(), ControllerAliases()) verflag.AddFlags(namedFlagSets.FlagSet(\"global\")) globalflag.AddGlobalFlags(namedFlagSets.FlagSet(\"global\"), cmd.Name(), logs.SkipLoggingConfigurationFlags()) for _, f := range namedFlagSets.FlagSets { fs.AddFlagSet(f) } cols, _, _ := term.TerminalSize(cmd.OutOrStdout()) cliflag.SetUsageAndHelpFunc(cmd, namedFlagSets, cols) return cmd } 配置信息不是我们需要关注的，在控制器中配置的创建实际和调度器中是基本一致的的，都是Options -> Config -> CompletedConfig，把完整配置传入核心逻辑Run()函数， 控制器核心创建逻辑 Run()函数的实现也和调度器十分相似，首先初始化日志记录器，打印基本环境信息，然后初始化事件广播器，注册配置和健康检查设置，启动Server并创建两个不同权限的客户端。这里涉及了一个重要的闭包函数run()。 func Run(ctx context.Context, c *config.CompletedConfig) error { // 初始化日志记录器 logger := klog.FromContext(ctx) stopCh := ctx.Done() // 打印版本信息 logger.Info(\"Starting\", \"version\", utilversion.Get()) // 打印Golang环境变量 logger.Info(\"Golang settings\", \"GOGC\", os.Getenv(\"GOGC\"), \"GOMAXPROCS\", os.Getenv(\"GOMAXPROCS\"), \"GOTRACEBACK\", os.Getenv(\"GOTRACEBACK\")) // 初始化事件广播器 c.EventBroadcaster.StartStructuredLogging(0) c.EventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: c.Client.CoreV1().Events(\"\")}) defer c.EventBroadcaster.Shutdown() // 注册配置信息 if cfgz, err := configz.New(ConfigzName); err == nil { cfgz.Set(c.ComponentConfig) } else { logger.Error(err, \"Unable to register configz\") } // 健康检查设置 var checks []healthz.HealthChecker var electionChecker *leaderelection.HealthzAdaptor if c.ComponentConfig.Generic.LeaderElection.LeaderElect { electionChecker = leaderelection.NewLeaderHealthzAdaptor(time.Second * 20) checks = append(checks, electionChecker) } healthzHandler := controllerhealthz.NewMutableHealthzHandler(checks...) // 启动http服务器 var unsecuredMux *mux.PathRecorderMux if c.SecureServing != nil { unsecuredMux = genericcontrollermanager.NewBaseHandler(&c.ComponentConfig.Generic.Debugging, healthzHandler) slis.SLIMetricsWithReset{}.Install(unsecuredMux) handler := genericcontrollermanager.BuildHandlerChain(unsecuredMux, &c.Authorization, &c.Authentication) if _, _, err := c.SecureServing.Serve(handler, 0, stopCh); err != nil { return err } } // 创建root权限客户端和普通客户端 clientBuilder, rootClientBuilder := createClientBuilders(c) saTokenControllerDescriptor := newServiceAccountTokenControllerDescriptor(rootClientBuilder) // 闭包函数 run := func(ctx context.Context, controllerDescriptors map[string]*ControllerDescriptor) { controllerContext, err := CreateControllerContext(ctx, c, rootClientBuilder, clientBuilder) if err != nil { logger.Error(err, \"Error building controller context\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } if err := StartControllers(ctx, controllerContext, controllerDescriptors, unsecuredMux, healthzHandler); err != nil { logger.Error(err, \"Error starting controllers\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } controllerContext.InformerFactory.Start(stopCh) controllerContext.ObjectOrMetadataInformerFactory.Start(stopCh) close(controllerContext.InformersStarted) 在对run()闭包函数以及内部逻辑做解释之前，先了解一个数据结构ControllerDescriptor，也就是该函数的入参类型，它用于描述和管理控制器的信息。 type InitFunc func(ctx context.Context, controllerContext ControllerContext, controllerName string) (controller controller.Interface, enabled bool, err error) type ControllerDescriptor struct { // 控制器名称 name string // 初始化函数 initFunc InitFunc // 特性门控列表 requiredFeatureGates []featuregate.Feature // 别名 aliases []string // 是否默认禁用 isDisabledByDefault bool // 是否和云供应商有关 isCloudProviderController bool // 是否有特殊处理逻辑 requiresSpecialHandling bool } 以不开启选举的流程为例，不涉及选主逻辑会直接启动控制器，首先会初始化ControllerDescriptor集合，然后传递给run()。 if !c.ComponentConfig.Generic.LeaderElection.LeaderElect { controllerDescriptors := NewControllerDescriptors() controllerDescriptors[names.ServiceAccountTokenController] = saTokenControllerDescriptor run(ctx, controllerDescriptors) return nil } NewControllerDescriptors()函数返回了一个key是控制器名称，value是ControllerDescriptor的映射，其中有一个需要注意的地方，ServiceAccountTokenControllerDescriptor是唯一特殊的控制器，需要最先启动而且使用具有根权限的客户端初始化，在之前的代码中以及创建了对象saTokenControllerDescriptor，那么为什么在下面这段函数中还要注册呢？主要的原因是NewControllerDescriptors()函数没有入参而ServiceAccountTokenControllerDescriptor的初始化函数需要传入根权限的客户端，但是要保证和其他控制器元数据创建时的一致性，并且其中register()校验了控制器描述符的合法性，虽然后面会被单独创建的saTokenControllerDescriptor替换，但是不影响和其他控制器描述符一起初始化一次。 func NewControllerDescriptors() map[string]*ControllerDescriptor { // 初始化 controllers := map[string]*ControllerDescriptor{} // 使用Set避免重复元素 aliases := sets.NewString() // 合法性校验与集合元素添加 register := func(controllerDesc *ControllerDescriptor) { if controllerDesc == nil { panic(\"received nil controller for a registration\") } name := controllerDesc.Name() if len(name) == 0 { panic(\"received controller without a name for a registration\") } if _, found := controllers[name]; found { panic(fmt.Sprintf(\"controller name %q was registered twice\", name)) } if controllerDesc.GetInitFunc() == nil { panic(fmt.Sprintf(\"controller %q does not have an init function\", name)) } for _, alias := range controllerDesc.GetAliases() { if aliases.Has(alias) { panic(fmt.Sprintf(\"controller %q has a duplicate alias %q\", name, alias)) } aliases.Insert(alias) } controllers[name] = controllerDesc } // 注册所有的ControllerDescriptor register(newServiceAccountTokenControllerDescriptor(nil)) register(newEndpointsControllerDescriptor()) register(newEndpointSliceControllerDescriptor()) register(newEndpointSliceMirroringControllerDescriptor()) register(newReplicationControllerDescriptor()) register(newPodGarbageCollectorControllerDescriptor()) register(newResourceQuotaControllerDescriptor()) register(newNamespaceControllerDescriptor()) register(newServiceAccountControllerDescriptor()) register(newGarbageCollectorControllerDescriptor()) register(newDaemonSetControllerDescriptor()) register(newJobControllerDescriptor()) register(newDeploymentControllerDescriptor()) register(newReplicaSetControllerDescriptor()) register(newHorizontalPodAutoscalerControllerDescriptor()) register(newDisruptionControllerDescriptor()) register(newStatefulSetControllerDescriptor()) register(newCronJobControllerDescriptor()) register(newCertificateSigningRequestSigningControllerDescriptor()) register(newCertificateSigningRequestApprovingControllerDescriptor()) register(newCertificateSigningRequestCleanerControllerDescriptor()) register(newTTLControllerDescriptor()) register(newBootstrapSignerControllerDescriptor()) register(newTokenCleanerControllerDescriptor()) register(newNodeIpamControllerDescriptor()) register(newNodeLifecycleControllerDescriptor()) register(newServiceLBControllerDescriptor()) // cloud provider controller register(newNodeRouteControllerDescriptor()) // cloud provider controller register(newCloudNodeLifecycleControllerDescriptor()) // cloud provider controller register(newPersistentVolumeBinderControllerDescriptor()) register(newPersistentVolumeAttachDetachControllerDescriptor()) register(newPersistentVolumeExpanderControllerDescriptor()) register(newClusterRoleAggregrationControllerDescriptor()) register(newPersistentVolumeClaimProtectionControllerDescriptor()) register(newPersistentVolumeProtectionControllerDescriptor()) register(newVolumeAttributesClassProtectionControllerDescriptor()) register(newTTLAfterFinishedControllerDescriptor()) register(newRootCACertificatePublisherControllerDescriptor()) register(newKubeAPIServerSignerClusterTrustBundledPublisherDescriptor()) register(newEphemeralVolumeControllerDescriptor()) // feature gated register(newStorageVersionGarbageCollectorControllerDescriptor()) register(newResourceClaimControllerDescriptor()) register(newLegacyServiceAccountTokenCleanerControllerDescriptor()) register(newValidatingAdmissionPolicyStatusControllerDescriptor()) register(newTaintEvictionControllerDescriptor()) register(newServiceCIDRsControllerDescriptor()) register(newStorageVersionMigratorControllerDescriptor()) register(newSELinuxWarningControllerDescriptor()) for _, alias := range aliases.UnsortedList() { if _, ok := controllers[alias]; ok { panic(fmt.Sprintf(\"alias %q conflicts with a controller name\", alias)) } } // 返回ControllerDescriptor集合 return controllers } 所有的ControllerDescriptor元数据都初始化后，替换ServiceAccountTokenControllerDescriptor为此前创建的内容，然后调用核心入口逻辑闭包函数run()，下面来看它的实现逻辑。 run := func(ctx context.Context, controllerDescriptors map[string]*ControllerDescriptor) { // 创建控制器上下文 controllerContext, err := CreateControllerContext(ctx, c, rootClientBuilder, clientBuilder) if err != nil { logger.Error(err, \"Error building controller context\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } // 启动控制器 if err := StartControllers(ctx, controllerContext, controllerDescriptors, unsecuredMux, healthzHandler); err != nil { logger.Error(err, \"Error starting controllers\") klog.FlushAndExit(klog.ExitFlushTimeout, 1) } // 启动SharedInformer工厂 监控Kubernetes标准资源 controllerContext.InformerFactory.Start(stopCh) // 启动MetadataInformerFactory工厂 监控类型化资源如CRD controllerContext.ObjectOrMetadataInformerFactory.Start(stopCh) close(controllerContext.InformersStarted) 关于CreateControllerContext()函数，它的作用是 func CreateControllerContext(ctx context.Context, s *config.CompletedConfig, rootClientBuilder, clientBuilder clientbuilder.ControllerClientBuilder) (ControllerContext, error) { // 闭包函数 用于裁剪obj对象的ManagedFields字段来提高内存效率 trim := func(obj interface{}) (interface{}, error) { // 获取obj对象元数据 if accessor, err := meta.Accessor(obj); err == nil { if accessor.GetManagedFields() != nil { // 裁剪ManagedFields字段 accessor.SetManagedFields(nil) } } return obj, nil } // 创建SharedInformer工厂 versionedClient := rootClientBuilder.ClientOrDie(\"shared-informers\") sharedInformers := informers.NewSharedInformerFactoryWithOptions(versionedClient, ResyncPeriod(s)(), informers.WithTransform(trim)) // 创建MetadataInformers工厂 metadataClient := metadata.NewForConfigOrDie(rootClientBuilder.ConfigOrDie(\"metadata-informers\")) metadataInformers := metadatainformer.NewSharedInformerFactoryWithOptions(metadataClient, ResyncPeriod(s)(), metadatainformer.WithTransform(trim)) // 等待ApiServer启动 超时时间设置为10s if err := genericcontrollermanager.WaitForAPIServer(versionedClient, 10*time.Second); err != nil { return ControllerContext{}, fmt.Errorf(\"failed to wait for apiserver being healthy: %v\", err) } // 创建Discovery客户端 discoveryClient := rootClientBuilder.DiscoveryClientOrDie(\"controller-discovery\") // 把Discovery客户端包装成一个缓存客户端 cachedClient := cacheddiscovery.NewMemCacheClient(discoveryClient) // 再把缓存客户端包装成一个REST映射器 restMapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedClient) // 启动一个协程定时每30s刷新REST映射器缓存 确保获取最新的API信息 go wait.Until(func() { restMapper.Reset() }, 30*time.Second, ctx.Done()) // 组装ControllerContext对象 controllerContext := ControllerContext{ ClientBuilder: clientBuilder, InformerFactory: sharedInformers, ObjectOrMetadataInformerFactory: informerfactory.NewInformerFactory(sharedInformers, metadataInformers), ComponentConfig: s.ComponentConfig, RESTMapper: restMapper, InformersStarted: make(chan struct{}), ResyncPeriod: ResyncPeriod(s), ControllerManagerMetrics: controllersmetrics.NewControllerManagerMetrics(\"kube-controller-manager\"), } // 如果开启了GarbageCollectorController垃圾回收控制器 if controllerContext.ComponentConfig.GarbageCollectorController.EnableGarbageCollector && controllerContext.IsControllerEnabled(NewControllerDescriptors()[names.GarbageCollectorController]) { ignoredResources := make(map[schema.GroupResource]struct{}) for _, r := range controllerContext.ComponentConfig.GarbageCollectorController.GCIgnoredResources { // 获取忽略资源列表 ignoredResources[schema.GroupResource{Group: r.Group, Resource: r.Resource}] = struct{}{} } // 创建GraphBuilder用来构建资源关系依赖 controllerContext.GraphBuilder = garbagecollector.NewDependencyGraphBuilder( ctx, metadataClient, controllerContext.RESTMapper, ignoredResources, controllerContext.ObjectOrMetadataInformerFactory, controllerContext.InformersStarted, ) } // 注册指标计数器 controllersmetrics.Register() return controllerContext, nil } 由于ControllerManager的运行所谓等待ApiServer成功启动，就是等待它的/healthz端点返回OK。 func WaitForAPIServer(client clientset.Interface, timeout time.Duration) error { var lastErr error // 轮询器执行目标函数 err := wait.PollImmediate(time.Second, timeout, func() (bool, error) { healthStatus := 0 result := client.Discovery().RESTClient().Get().AbsPath(\"/healthz\").Do(context.TODO()).StatusCode(&healthStatus) if result.Error() != nil { lastErr = fmt.Errorf(\"failed to get apiserver /healthz status: %v\", result.Error()) return false, nil } if healthStatus != http.StatusOK { content, _ := result.Raw() lastErr = fmt.Errorf(\"APIServer isn't healthy: %v\", string(content)) klog.Warningf(\"APIServer isn't healthy yet: %v. Waiting a little while.\", string(content)) return false, nil } // 返回200OK结束轮询 return true, nil }) if err != nil { return fmt.Errorf(\"%v: %v\", err, lastErr) } return nil } 在CreateControllerContext()函数返回了控制器的公共配置后，就进入到下一个重要的步骤，也就是逐个启动控制器。 首先会启动ServiceAccountToken控制器，因为与ApiServer交互时会用到令牌验证身份，如果该控制器没有启动会影响到其他控制器的正常运行。然后再遍历ControllerDescriptor集合启动其他控制器。 func StartControllers(ctx context.Context, controllerCtx ControllerContext, controllerDescriptors map[string]*ControllerDescriptor, unsecuredMux *mux.PathRecorderMux, healthzHandler *controllerhealthz.MutableHealthzHandler) error { var controllerChecks []healthz.HealthChecker // ServiceAccountTokenController需要第一个被启动 if serviceAccountTokenControllerDescriptor, ok := controllerDescriptors[names.ServiceAccountTokenController]; ok { check, err := StartController(ctx, controllerCtx, serviceAccountTokenControllerDescriptor, unsecuredMux) if err != nil { return err } if check != nil { // HealthChecker should be present when controller has started controllerChecks = append(controllerChecks, check) } } // 遍历启动其他控制器 for _, controllerDesc := range controllerDescriptors { if controllerDesc.RequiresSpecialHandling() { continue } check, err := StartController(ctx, controllerCtx, controllerDesc, unsecuredMux) if err != nil { return err } if check != nil { // HealthChecker should be present when controller has started controllerChecks = append(controllerChecks, check) } } healthzHandler.AddHealthChecker(controllerChecks...) return nil } ServiceAccountToken控制器的创建 看一下ServiceAccountTokenController是如何启动的，StartController()是该控制器启动的直接步骤。经过一系列的检查后，调用此前在ControllerDescriptor对象中注册的InitFunc初始化函数创建控制器实例，并注册调试接口和创建健康检查器。 func StartController(ctx context.Context, controllerCtx ControllerContext, controllerDescriptor *ControllerDescriptor, unsecuredMux *mux.PathRecorderMux) (healthz.HealthChecker, error) { // 初始化日志记录器 logger := klog.FromContext(ctx) controllerName := controllerDescriptor.Name() // 校验需要的特性门控是否全部开启 for _, featureGate := range controllerDescriptor.GetRequiredFeatureGates() { if !utilfeature.DefaultFeatureGate.Enabled(featureGate) { logger.Info(\"Controller is disabled by a feature gate\", \"controller\", controllerName, \"requiredFeatureGates\", controllerDescriptor.GetRequiredFeatureGates()) return nil, nil } } // 如果是云厂商控制器则跳过 if controllerDescriptor.IsCloudProviderController() { logger.Info(\"Skipping a cloud provider controller\", \"controller\", controllerName) return nil, nil } // 校验当前控制器是否被启用 if !controllerCtx.IsControllerEnabled(controllerDescriptor) { logger.Info(\"Warning: controller is disabled\", \"controller\", controllerName) return nil, nil } // 随机延迟启动控制器 避免资源竞争 time.Sleep(wait.Jitter(controllerCtx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter)) logger.V(1).Info(\"Starting controller\", \"controller\", controllerName) // 执行ControllerDescriptor中的InitFunc初始化控制器实例 initFunc := controllerDescriptor.GetInitFunc() ctrl, started, err := initFunc(klog.NewContext(ctx, klog.LoggerWithName(logger, controllerName)), controllerCtx, controllerName) if err != nil { logger.Error(err, \"Error starting controller\", \"controller\", controllerName) return nil, err } if !started { logger.Info(\"Warning: skipping controller\", \"controller\", controllerName) return nil, nil } check := controllerhealthz.NamedPingChecker(controllerName) if ctrl != nil { // 注册调试接口 if debuggable, ok := ctrl.(controller.Debuggable); ok && unsecuredMux != nil { if debugHandler := debuggable.DebuggingHandler(); debugHandler != nil { basePath := \"/debug/controllers/\" + controllerName unsecuredMux.UnlistedHandle(basePath, http.StripPrefix(basePath, debugHandler)) unsecuredMux.UnlistedHandlePrefix(basePath+\"/\", http.StripPrefix(basePath, debugHandler)) } } // 创建健康检查器 if healthCheckable, ok := ctrl.(controller.HealthCheckable); ok { if realCheck := healthCheckable.HealthChecker(); realCheck != nil { check = controllerhealthz.NamedHealthChecker(controllerName, realCheck) } } } logger.Info(\"Started controller\", \"controller\", controllerName) return check, nil } 在这里先不关注各种控制器初始化函数的具体实现，后续会在分析每种具体控制器时一并说明，其他控制器也都通过StartController创建出来后，继续回到闭包函数run()中，剩下的最后一个步骤就是通过工厂启动ControllerContext中的两类Informer，所有Informer实例都启动后关闭控制器上下文中的ControllerContext.InformersStarted通道，最后通过常见的方式挂起主线程，直至收到停止信号后优雅退出。 © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 13:52:51 "},"kube-controller-manager/02-DeploymentController原理详解.html":{"url":"kube-controller-manager/02-DeploymentController原理详解.html","title":"DeploymentController原理详解","keywords":"","body":" 实例创建 启动逻辑 调谐基本流程 下属资源对象的获取 API资源的描述 标签选择的转换 Deployment下属资源的获取 ReplicaSet的获取 Pod的获取 调谐的具体动作 Deployment对象正在删除中 获取旧的ReplicaSet对象 获取新的ReplicaSet对象 处理新的ReplicaSet对象 给新的ReplicaSet对象设置注解 同步Deployment对象的状态 Deployment对象需要扩缩容或被手动暂停 扩缩容逻辑 场景一 场景二 场景三(核心场景) 扩缩容调整ReplicaSet对象的比例 滚动更新数量计算规则 Deployment对象需要回滚 Deployment对象滚动更新 Recreate策略 RollingUpdate策略 尝试扩容新ReplicaSet 扩容新副本数的计算 尝试缩容旧ReplicaSet 缩容不健康副本 正常缩容 DeploymentController原理详解 在上一章节中，我们简单了解了ControllerManager的创建，本篇文章中深入学习Kubernetes中最重要的控制器之一DeploymentController。 当一个Deployment创建的时候，其实总共创建了三种资源对象，分别是Deployment、ReplicaSet、Pod，这是非常重要的，因为Deployment资源并不会直接管理Pod，而是通过管理ReplicaSet对象来间接地管理Pod，所以一个无状态负载的Pod的直接归属是ReplicaSet。这种设计和滚动更新相关，当一个Deployment中定义的Pod模板发生变化时，会创建出一个新的ReplicaSet，再根据一定的规则去替换旧的ReplicaSet对象。 实例创建 下面先从DeploymentController的创建开始学习，它的初始化函数如下，其中包含创建控制器实例和启动控制器两个逻辑。 func startDeploymentController(ctx context.Context, controllerContext ControllerContext, controllerName string) (controller.Interface, bool, error) { dc, err := deployment.NewDeploymentController( ctx, controllerContext.InformerFactory.Apps().V1().Deployments(), controllerContext.InformerFactory.Apps().V1().ReplicaSets(), controllerContext.InformerFactory.Core().V1().Pods(), controllerContext.ClientBuilder.ClientOrDie(\"deployment-controller\"), ) if err != nil { return nil, true, fmt.Errorf(\"error creating Deployment controller: %v\", err) } go dc.Run(ctx, int(controllerContext.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs)) return nil, true, nil } DeploymentController的实现逻辑都在pkg/controller/deployment路径下，NewDeploymentController()方法创建了一个控制器实例，根据函数签名来看，它接收上下文参数ctx，三种Informer对象用来监测Deployment/ReplicaSet/Pod资源的变化，以及客户端client。照惯例先创建事件广播器和日志记录器，然后初始化DeploymentController对象，其中包括客户端、事件广播器、事件记录器、限速工作队列、和用于对ReplicaSet对象进行Patch的操作器rsControl。再通过AddEventHandler()注册事件的处理函数，并初始化各种资源的Lister func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) { // 初始化事件处理器和日志记录器 eventBroadcaster := record.NewBroadcaster(record.WithContext(ctx)) logger := klog.FromContext(ctx) // 初始化DeploymentController实例 dc := &DeploymentController{ client: client, eventBroadcaster: eventBroadcaster, eventRecorder: eventBroadcaster.NewRecorder(scheme.Scheme, v1.EventSource{Component: \"deployment-controller\"}), queue: workqueue.NewTypedRateLimitingQueueWithConfig( workqueue.DefaultTypedControllerRateLimiter[string](), workqueue.TypedRateLimitingQueueConfig[string]{ Name: \"deployment\", }, ), } dc.rsControl = controller.RealRSControl{ KubeClient: client, Recorder: dc.eventRecorder, } // 注册Deployment资源变化处理函数 dInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addDeployment(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateDeployment(logger, oldObj, newObj) }, // This will enter the sync loop and no-op, because the deployment has been deleted from the store. DeleteFunc: func(obj interface{}) { dc.deleteDeployment(logger, obj) }, }) // 注册ReplicaSet资源变化处理函数 rsInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { dc.addReplicaSet(logger, obj) }, UpdateFunc: func(oldObj, newObj interface{}) { dc.updateReplicaSet(logger, oldObj, newObj) }, DeleteFunc: func(obj interface{}) { dc.deleteReplicaSet(logger, obj) }, }) // 注册Pod资源变化处理函数 podInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ DeleteFunc: func(obj interface{}) { dc.deletePod(logger, obj) }, }) // 注册调谐函数 dc.syncHandler = dc.syncDeployment // 注册事件入队函数 dc.enqueueDeployment = dc.enqueue // 初始化资源对象Lister dc.dLister = dInformer.Lister() dc.rsLister = rsInformer.Lister() dc.podLister = podInformer.Lister() // 初始化缓存状态检查函数 dc.dListerSynced = dInformer.Informer().HasSynced dc.rsListerSynced = rsInformer.Informer().HasSynced dc.podListerSynced = podInformer.Informer().HasSynced return dc, nil } 最后就返回了一个完整的DeploymentController对象，其结构如下。 type DeploymentController struct { // 用于操作ReplicaSet对象 rsControl controller.RSControlInterface // Kubernetes客户端 client clientset.Interface // 事件广播器 eventBroadcaster record.EventBroadcaster // 事件记录器 eventRecorder record.EventRecorder // 同步函数 syncHandler func(ctx context.Context, dKey string) error // 单测使用 入队函数 enqueueDeployment func(deployment *apps.Deployment) // Deployments资源的Lister dLister appslisters.DeploymentLister // ReplicaSet资源的Lister rsLister appslisters.ReplicaSetLister // Pod资源的Lister podLister corelisters.PodLister // 缓存状态检查函数 dListerSynced cache.InformerSynced rsListerSynced cache.InformerSynced podListerSynced cache.InformerSynced // 限速队列 queue workqueue.TypedRateLimitingInterface[string] } 启动逻辑 运行控制器实例的代码如下，另起一个协程，传入上下文控制生命周期，还有一个参数表示允许并发同步的Deployment对象数量。 go dc.Run(ctx, int(controllerContext.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs)) 来看Run()方法的具体实现逻辑，首先还是标准的初始化流程和日志打印，然后会通过WaitForNamedCacheSync()方法确认Informer监听的资源是否同步成功，内部会调用PollImmediateUntil()函数阻塞等待InformerSynced返回的结果。然后根据传入的worker数值启动对应数量的协程去处理事件，最后通过接收Done信号的方式阻塞主线程。 func (dc *DeploymentController) Run(ctx context.Context, workers int) { // 异常处理 用于捕获panic defer utilruntime.HandleCrash() // 启动事件广播器 dc.eventBroadcaster.StartStructuredLogging(3) dc.eventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: dc.client.CoreV1().Events(\"\")}) // 退出时停止事件广播器和控制器的工作队列 defer dc.eventBroadcaster.Shutdown() defer dc.queue.ShutDown() // 日志记录 logger := klog.FromContext(ctx) logger.Info(\"Starting controller\", \"controller\", \"deployment\") defer logger.Info(\"Shutting down controller\", \"controller\", \"deployment\") // 确认缓存同步成功 if !cache.WaitForNamedCacheSync(\"deployment\", ctx.Done(), dc.dListerSynced, dc.rsListerSynced, dc.podListerSynced) { return } // 启动worker线程 for i := 0; i 调谐基本流程 循环执行的逻辑是worker()方法，根据其中方法的命名，很明显它要做的就是不停地处理下一个元素。 func (dc *DeploymentController) worker(ctx context.Context) { for dc.processNextWorkItem(ctx) { } } 来看元素是如何被处理的，首先从工作队列中取出一个元素，返回的是一个字符串类型的对象名称，然后交给调谐方法也就是syncHandler()去处理。如果获取元素时发现队列已经关闭了就返回一个false，worker协程也随之关闭。 func (dc *DeploymentController) processNextWorkItem(ctx context.Context) bool { // 取出一个元素 key, quit := dc.queue.Get() // 如果队列为空且已经调用过shutdown关闭 quit会返回true if quit { return false } // 结束时通知队列处理完成(成功/失败重新入队) defer dc.queue.Done(key) // 通过调谐方法处理 err := dc.syncHandler(ctx, key) // 错误处理 dc.handleErr(ctx, err, key) return true } 下面就是控制器中最核心的逻辑了，一般来说会叫做reconciler()，此处仅命名不同。在队列中取出key的格式为namespcae/deploymentname，调谐时会先切分出namespace和name，然后通过Lister从缓存中获取到具体的Deployment对象并拷贝，在调度器的学习过程中对于Pod的处理也是要拷贝的，因为缓存中是反映系统实际状态的信息，避免在处理过程中影响原始内容，所以后续操作都要用深拷贝的对象。在开始调谐逻辑之前会先检查Deployment对象的Selector字段是否为空，如果是则记录错误并跳过当前对象的调谐。 func (dc *DeploymentController) syncDeployment(ctx context.Context, key string) error { logger := klog.FromContext(ctx) // 获取命名空间和对象名称 namespace, name, err := cache.SplitMetaNamespaceKey(key) if err != nil { logger.Error(err, \"Failed to split meta namespace cache key\", \"cacheKey\", key) return err } // 记录开始时间 startTime := time.Now() logger.V(4).Info(\"Started syncing deployment\", \"deployment\", klog.KRef(namespace, name), \"startTime\", startTime) // 延迟打印结束日志 defer func() { logger.V(4).Info(\"Finished syncing deployment\", \"deployment\", klog.KRef(namespace, name), \"duration\", time.Since(startTime)) }() // 通过Lister获取Deployment deployment, err := dc.dLister.Deployments(namespace).Get(name) if errors.IsNotFound(err) { logger.V(2).Info(\"Deployment has been deleted\", \"deployment\", klog.KRef(namespace, name)) return nil } if err != nil { return err } // 缓存中的对象只读 深拷贝对象以避免影响缓存内容 d := deployment.DeepCopy() everything := metav1.LabelSelector{} // 保护逻辑 Selector为空时意味着选择所有Pod 这是一个错误事件 if reflect.DeepEqual(d.Spec.Selector, &everything) { dc.eventRecorder.Eventf(d, v1.EventTypeWarning, \"SelectingAll\", \"This deployment is selecting all pods. A non-empty selector is required.\") // 直接更新Generation并返回 不执行后续调谐动作 if d.Status.ObservedGeneration 下属资源对象的获取 我们知道资源对象归属关系的匹配是基于标签选择的，在一个yaml文件的声明中，上层资源如Deployment、 StatefulSet等对下层资源如Pod的标签选择常有以下的表示形式： apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: selector: matchLabels: component: redis matchExpressions: - { key: tier, operator: In, values: [cache] } - { key: environment, operator: NotIn, values: [dev] } template: metadata: labels: component: redis tier: cache environment: test spec: containers: ...... 标签的选择规则定义在字段spec.selector下，在和下层资源匹配时必须全部满足，所以在内部匹配时会进行一个非常重要的阶段，也就是把规则或一组规则的集合转换为统一的标识方法，然后在所有下层资源中过滤符合所有条件的，即认为两者具有从属关系。 API资源的描述 根据Deployment类型为开始层层分析，首先Deployment结构体中包含DeploymentSpec类型的描述信息，后面如果学习Operator开发会了解到，一般定义一个API对象，通常会包含metav1.TypeMeta、metav1.ObjectMeta、Spec以及Status四个字段。 type Deployment struct { metav1.TypeMeta // +optional metav1.ObjectMeta // Specification of the desired behavior of the Deployment. // +optional Spec DeploymentSpec // Most recently observed status of the Deployment. // +optional Status DeploymentStatus } 在一个yaml文件中，apiVersion和kind这两个字段属于TypeMeta，说明了API的版本和类型信息(GVK)，metadata字段属于ObjectMeta，描述了对象元数据，包括名称、命名空间、标签和注解，spec字段属于类型的Spec，表示该资源对象的期望状态，包括副本数量和容器配置等。 # TypeMeta apiVersion: apps/v1 kind: Deployment ---------------------------------------------------------- # ObjectMeta metadata: name: my-deployment namespace: default labels: app: my-app tier: frontend annotations: description: This is my deployment ---------------------------------------------------------- # Spec spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-container image: my-image 回到DeploymentSpec类型中，其Selector字段为metav1.LabelSelector类型的指针。 type DeploymentSpec struct { Replicas int32 Selector *metav1.LabelSelector Template api.PodTemplateSpec Strategy DeploymentStrategy MinReadySeconds int32 RevisionHistoryLimit *int32 Paused bool RollbackTo *RollbackConfig ProgressDeadlineSeconds *int32 } 继续看LabelSelector类型的定义，它正符合在一个yaml文件中对于标签选择的定义规范，即：1.选择标签与某个值是匹配的;2.标签和某些值存在In/NotIn/Exists/DoesNotExist的关系。 type LabelSelector struct { // matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels // map is equivalent to an element of matchExpressions, whose key field is \"key\", the // operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. // +optional MatchLabels map[string]string `json:\"matchLabels,omitempty\" protobuf:\"bytes,1,rep,name=matchLabels\"` // matchExpressions is a list of label selector requirements. The requirements are ANDed. // +optional // +listType=atomic MatchExpressions []LabelSelectorRequirement `json:\"matchExpressions,omitempty\" protobuf:\"bytes,2,rep,name=matchExpressions\"` } 标签选择的转换 上面说到过，在控制器内部进行从属资源选择时，会对上层资源进行标签的转换以匹配所属资，metav1.LabelSelectorAsSelector()方法实现了这一逻辑，把metav1.LabelSelector类型转换为labels.Selector对象，下面来看它的实现。 首先对传入的LabelSelector对象进行检查，如果是空则表示不匹配标签，不为空但长度是0表示匹配所有标签。首先处理MatchLabels字段，这一部分都是期望标签与目标值一致的，所以操作符使用Equals。然后遍历处理MatchExpressions字段，根据其中Operator的值进行转换，然后初始化一个labels.Selector接口，然后调用Add()方法添加之前处理好的标签，最终Api对象中的标签会以labelkey--operator--labelvalue切片的内部标签形式统一存在。 func LabelSelectorAsSelector(ps *LabelSelector) (labels.Selector, error) { // 对象检查 if ps == nil { return labels.Nothing(), nil } if len(ps.MatchLabels)+len(ps.MatchExpressions) == 0 { return labels.Everything(), nil } requirements := make([]labels.Requirement, 0, len(ps.MatchLabels)+len(ps.MatchExpressions)) // 处理MatchLabels字段 for k, v := range ps.MatchLabels { r, err := labels.NewRequirement(k, selection.Equals, []string{v}) if err != nil { return nil, err } requirements = append(requirements, *r) } // 处理MatchExpressions字段 for _, expr := range ps.MatchExpressions { var op selection.Operator switch expr.Operator { case LabelSelectorOpIn: op = selection.In case LabelSelectorOpNotIn: op = selection.NotIn case LabelSelectorOpExists: op = selection.Exists case LabelSelectorOpDoesNotExist: op = selection.DoesNotExist default: return nil, fmt.Errorf(\"%q is not a valid label selector operator\", expr.Operator) } r, err := labels.NewRequirement(expr.Key, op, append([]string(nil), expr.Values...)) if err != nil { return nil, err } requirements = append(requirements, *r) } // 初始化一个internalSelector类型 selector := labels.NewSelector() // 添加requirements selector = selector.Add(requirements...) return selector, nil } Deployment下属资源的获取 ReplicaSet的获取 getReplicaSetsForDeployment()方法用于获取Deployment下属的ReplicaSet实例，首先获取命名空间下的所有ReplocaSet对象，然后把Deployment对象的标签解析为内部形式。基于rsControl、Selector等封装出一个ReplicaSetControllerRefManager结构对象用于处理该Deployment与ReplicaSet之间的从属关系，最后调用其ClaimReplicaSets()方法认领属于当前Deployment的ReplicaSet对象。 func (dc *DeploymentController) getReplicaSetsForDeployment(ctx context.Context, d *apps.Deployment) ([]*apps.ReplicaSet, error) { // 获取命名空间下所有ReplicaSet rsList, err := dc.rsLister.ReplicaSets(d.Namespace).List(labels.Everything()) if err != nil { return nil, err } // Deployment标签转换 deploymentSelector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, fmt.Errorf(\"deployment %s/%s has invalid label selector: %v\", d.Namespace, d.Name, err) } // 认领ReplicaSet前会再次检查 避免List和Adopt之间的Deployment对象的变更 canAdoptFunc := controller.RecheckDeletionTimestamp(func(ctx context.Context) (metav1.Object, error) { //直接从ApiServer获取最新对象 并通过UID进行一致性确认 fresh, err := dc.client.AppsV1().Deployments(d.Namespace).Get(ctx, d.Name, metav1.GetOptions{}) if err != nil { return nil, err } if fresh.UID != d.UID { return nil, fmt.Errorf(\"original Deployment %v/%v is gone: got uid %v, wanted %v\", d.Namespace, d.Name, fresh.UID, d.UID) } return fresh, nil }) // 创建Replicaset对象的引用管理器 cm := controller.NewReplicaSetControllerRefManager(dc.rsControl, d, deploymentSelector, controllerKind, canAdoptFunc) // 认领ReplicaSet return cm.ClaimReplicaSets(ctx, rsList) } ReplicaSet的认领逻辑在ClaimReplicaSets()方法中实现，其中定义了三个函数，分别对应标签选择、认领和释放。遍历ReplicaSet列表，然后把认领的对象加入claimed变量并返回给上层。 func (m *ReplicaSetControllerRefManager) ClaimReplicaSets(ctx context.Context, sets []*apps.ReplicaSet) ([]*apps.ReplicaSet, error) { var claimed []*apps.ReplicaSet var errlist []error // 三个辅助函数 match := func(obj metav1.Object) bool { return m.Selector.Matches(labels.Set(obj.GetLabels())) } adopt := func(ctx context.Context, obj metav1.Object) error { return m.AdoptReplicaSet(ctx, obj.(*apps.ReplicaSet)) } release := func(ctx context.Context, obj metav1.Object) error { return m.ReleaseReplicaSet(ctx, obj.(*apps.ReplicaSet)) } // 遍历处理 for _, rs := range sets { ok, err := m.ClaimObject(ctx, rs, match, adopt, release) if err != nil { errlist = append(errlist, err) continue } if ok { claimed = append(claimed, rs) } } return claimed, utilerrors.NewAggregate(errlist) } 具体的处理逻辑体现在ClaimObkect()方法中，其中包含很多的if-else，下面进行分析。 拿到RepicaSet对象的第一步是获取它的OwnerReferences对象，判断逻辑如下： 第一种情况：所属控制器存在 其所属控制器存在，但不是当前的控制器，跳过处理; 其所属控制器存在，是当前的控制器，还需要检查一次标签选择，避免由于Selector动态修改导致的不匹配; 其所属控制器存在，是当前的控制器，但标签不匹配，如果当前控制器正在删除中，也跳过处理; 其所属控制器存在，是当前的控制器，但标签不匹配，控制器正常，尝试释放对象; 第二种情况：所属控制器不存在，孤儿对象 控制器被删除或标签不匹配，跳过处理; 控制器被删除或标签匹配，ReplicaSet对象正在被删除，跳过处理; 控制器被删除或标签匹配，ReplicaSet对象正常，命名空间不匹配，跳过处理; 控制器被删除或标签匹配，ReplicaSet对象正常，命名空间匹配，尝试认领; func (m *BaseControllerRefManager) ClaimObject(ctx context.Context, obj metav1.Object, match func(metav1.Object) bool, adopt, release func(context.Context, metav1.Object) error) (bool, error) { controllerRef := metav1.GetControllerOfNoCopy(obj) // 有所属控制器 if controllerRef != nil { // 不属于当前控制器 if controllerRef.UID != m.Controller.GetUID() { // 忽略 return false, nil } // 属于当前控制器 if match(obj) { // 标签匹配 返回 return true, nil } // 属于当前控制器 标签不匹配 if m.Controller.GetDeletionTimestamp() != nil { // 控制器在被删除 忽略 return false, nil } // 控制器没被删除 释放 if err := release(ctx, obj); err != nil { // 对象已经不存在了 忽略 if errors.IsNotFound(err) { return false, nil } // 可能被其他人释放 忽略 return false, err } // 成功释放 return false, nil } // 另一种情况 没有所属控制器：孤儿对象 if m.Controller.GetDeletionTimestamp() != nil || !match(obj) { // 控制器正在被删除或标签不匹配 忽略 return false, nil } // 控制器没被删除 标签也匹配 if obj.GetDeletionTimestamp() != nil { // 目标对象正在被删除 忽略 return false, nil } if len(m.Controller.GetNamespace()) > 0 && m.Controller.GetNamespace() != obj.GetNamespace() { // 命名空间不匹配 忽略 return false, nil } // 控制器正常 标签匹配 命名空间匹配 尝试认领 if err := adopt(ctx, obj); err != nil { // 对象已经被删除 忽略 if errors.IsNotFound(err) { return false, nil } // 已被其他人认领 忽略 return false, err } // 认领成功 return true, nil } Pod的获取 确认了Deployment下属的ReplicaSet列表后，使用getPodMapForDeployment()方法获取Pod的列表。根据函数签名，入参是Deployment和ReplicaSet列表，返回的是一个以ReplicaSet的UID为key，Pod对象为value的列表。 首先进行控制器的标签转换，再获取到同一命名空间下标签匹配的Pod列表。在滚动更新过程中，可能存在多个ReplicaSet实例，并且每个实例下都还包含Pod，所以会先以ReplicaSet实例的UID为key初始化一个Map，然后遍历所有Pod， func (dc *DeploymentController) getPodMapForDeployment(d *apps.Deployment, rsList []*apps.ReplicaSet) (map[types.UID][]*v1.Pod, error) { // 标签转换 selector, err := metav1.LabelSelectorAsSelector(d.Spec.Selector) if err != nil { return nil, err } // 列出命名空间下所有标签匹配Pod pods, err := dc.podLister.Pods(d.Namespace).List(selector) if err != nil { return nil, err } // 以UID为key初始化空集合 podMap := make(map[types.UID][]*v1.Pod, len(rsList)) for _, rs := range rsList { podMap[rs.UID] = []*v1.Pod{} } // 遍历Pod 根据其OwnerReference的UID加入对应集合 for _, pod := range pods { controllerRef := metav1.GetControllerOf(pod) if controllerRef == nil { continue } // Only append if we care about this UID. if _, ok := podMap[controllerRef.UID]; ok { podMap[controllerRef.UID] = append(podMap[controllerRef.UID], pod) } } return podMap, nil } 调谐的具体动作 根据不同的场景会有不同的调谐动作，场景大概可以分为几类： Deployment对象正在删除中 Deployment对象手动暂停 Deployment对象需要回滚 Deployment对象副本扩缩容 Deployment对象滚动更新 下面根据几种场景，结合代码分别进行详细的说明。 Deployment对象正在删除中 在这种情况下，仅会同步状态，但不做任何可能影响资源状态的操作。 func (dc *DeploymentController) syncStatusOnly(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { // 获取新就版本的ReplicaSet newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } // 合并ReplicaSet allRSs := append(oldRSs, newRS) // 同步Deployment的status return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } 其中getAllReplicaSetsAndSyncRevision()方法用于获取所有新旧版本的ReplicaSet对象，是Deployment Controller调谐过程中的一个通用方法，在rolling\\rollback\\recreate过程中也被使用。 func (dc *DeploymentController) getAllReplicaSetsAndSyncRevision(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, []*apps.ReplicaSet, error) { // 找到所有旧的ReplicaSet _, allOldRSs := deploymentutil.FindOldReplicaSets(d, rsList) // 获取新的ReplicaSet并更新版本号 newRS, err := dc.getNewReplicaSet(ctx, d, rsList, allOldRSs, createIfNotExisted) if err != nil { return nil, nil, err } return newRS, allOldRSs, nil } 获取旧的ReplicaSet对象 这部分的逻辑也比较简单，首先获取新的ReplicaSet对象，然后遍历所有ReplicaSet并根据UID判断是否是旧的对象，并且如果旧的ReplicaSet还关联Pod，单独存放一份到requiredRSs中，返回的两个列表分别是：有Pod存在的旧ReplicaSet和旧ReplicaSet全集。 func FindOldReplicaSets(deployment *apps.Deployment, rsList []*apps.ReplicaSet) ([]*apps.ReplicaSet, []*apps.ReplicaSet) { var requiredRSs []*apps.ReplicaSet var allRSs []*apps.ReplicaSet newRS := FindNewReplicaSet(deployment, rsList) for _, rs := range rsList { // Filter out new replica set if newRS != nil && rs.UID == newRS.UID { continue } allRSs = append(allRSs, rs) if *(rs.Spec.Replicas) != 0 { requiredRSs = append(requiredRSs, rs) } } return requiredRSs, allRSs } 获取新的ReplicaSet对象 来看新的对象是如何获取的，ReplicaSetsByCreationTimestamp类型是[]*apps.ReplicaSet类型的别名，专门为了实现ReplicaSet对象基于创建时间戳的排序而存在。第一步是先对所有的ReplicaSet进行排序，按照创建时间戳升序排列。第二步会遍历所有的对象，返回和最新ReplicaSet对象的Template描述完全一致的最早版本，这是Kubernetes中确定性原则的体现：避免了随机选择，并且避免了集群信息中存在多个相同Template的ReplicaSet情况下的处理异常。 func FindNewReplicaSet(deployment *apps.Deployment, rsList []*apps.ReplicaSet) *apps.ReplicaSet { // 按创建时间升序排列ReplicaSet sort.Sort(controller.ReplicaSetsByCreationTimestamp(rsList)) for i := range rsList { if EqualIgnoreHash(&rsList[i].Spec.Template, &deployment.Spec.Template) { // In rare cases, such as after cluster upgrades, Deployment may end up with // having more than one new ReplicaSets that have the same template as its template, // see https://github.com/kubernetes/kubernetes/issues/40415 // We deterministically choose the oldest new ReplicaSet. return rsList[i] } } // new ReplicaSet does not exist. return nil } Template字段的比较函数如下，先对两个对象做深拷贝，然后删除ReplicaSet对象的pod-template-hash标签，该标签是在ReplicaSet创建时自动添加的根据Pod模板哈希而来的一个Label，用于帮助ReplicaSet选择并隔离不同版本的Pod，此处的一致性判断逻辑关注于用户的配置，删除该标签避免了用户配置相同但哈希结果不同的特殊情况。 func EqualIgnoreHash(template1, template2 *v1.PodTemplateSpec) bool { t1Copy := template1.DeepCopy() t2Copy := template2.DeepCopy() // Remove hash labels from template.Labels before comparing delete(t1Copy.Labels, apps.DefaultDeploymentUniqueLabelKey) delete(t2Copy.Labels, apps.DefaultDeploymentUniqueLabelKey) return apiequality.Semantic.DeepEqual(t1Copy, t2Copy) } 处理新的ReplicaSet对象 在getAllReplicaSetsAndSyncRevision()方法中，新ReplicaSet对象是由getNewReplicaSet()方法返回的，用于生成和管理滚动更新过程中的ReplicaSet新对象。 首先尝试获取最新的ReplicaSet对象和最新对象的预期版本号Revision，如果该ReplicaSet对象存在检查其是否需要更新，如果要更新就向ApiServer发送一个更新请求并返回，然后检查Deployment对象是否需要更新，如果需要更新同样向ApiServer请求。如果ReplicaSet更新使函数返回，不用担心Deployment对象无法被更新，因为ReplicaSet的更新可以触发控制器的调谐动作，如果Deployment对象需要更新也会在下个调谐周期被处理。 如果预期的ReplicaSet对象不存在，就需要去创建它，然后更新Deployment对象，最后返回新的ReplicaSet对象。 func (dc *DeploymentController) getNewReplicaSet(ctx context.Context, d *apps.Deployment, rsList, oldRSs []*apps.ReplicaSet, createIfNotExisted bool) (*apps.ReplicaSet, error) { logger := klog.FromContext(ctx) // 获取最新ReplicaSet existingNewRS := deploymentutil.FindNewReplicaSet(d, rsList) // 获取旧ReplicaSet的最大版本号 maxOldRevision := deploymentutil.MaxRevision(logger, oldRSs) // 新ReplicaSet的版本号设置为maxOldRevision+1 newRevision := strconv.FormatInt(maxOldRevision+1, 10) // 最新的ReplicaSet已经存在时 if existingNewRS != nil { rsCopy := existingNewRS.DeepCopy() // ReplicaSet对象的注解是否更新 annotationsUpdated := deploymentutil.SetNewReplicaSetAnnotations(ctx, d, rsCopy, newRevision, true, maxRevHistoryLengthInChars) // MinReadySeconds字段是否更新 minReadySecondsNeedsUpdate := rsCopy.Spec.MinReadySeconds != d.Spec.MinReadySeconds // 如果需要更新 向ApiServer发送更新请求 if annotationsUpdated || minReadySecondsNeedsUpdate { rsCopy.Spec.MinReadySeconds = d.Spec.MinReadySeconds return dc.client.AppsV1().ReplicaSets(rsCopy.ObjectMeta.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) } // Deployment对象的版本号是否要更新 needsUpdate := deploymentutil.SetDeploymentRevision(d, rsCopy.Annotations[deploymentutil.RevisionAnnotation]) // Deployment对象是否有进度状态条件信息 cond := deploymentutil.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) // 如果设置了进度截止时间但没有状态条件信息 if deploymentutil.HasProgressDeadline(d) && cond == nil { msg := fmt.Sprintf(\"Found new replica set %q\", rsCopy.Name) // 更新Deployment状态条件信息字段和标识位needsUpdate condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.FoundNewRSReason, msg) deploymentutil.SetDeploymentCondition(&d.Status, *condition) needsUpdate = true } // 如果Deployment需要更新 同样向ApiServer发送更新请求 if needsUpdate { var err error if _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}); err != nil { return nil, err } } // 返回最终的新ReplicaSet对象 return rsCopy, nil } // 如果最新ReplicaSet不存在 但是不允许创建 if !createIfNotExisted { return nil, nil } // 如果最新ReplicaSet不存在 需要创建 newRSTemplate := *d.Spec.Template.DeepCopy() podTemplateSpecHash := controller.ComputeHash(&newRSTemplate, d.Status.CollisionCount) newRSTemplate.Labels = labelsutil.CloneAndAddLabel(d.Spec.Template.Labels, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // Selector中也需要pod-template-hash newRSSelector := labelsutil.CloneSelectorAndAddLabel(d.Spec.Selector, apps.DefaultDeploymentUniqueLabelKey, podTemplateSpecHash) // 组装ReplicaSet对象 newRS := apps.ReplicaSet{ ObjectMeta: metav1.ObjectMeta{ Name: d.Name + \"-\" + podTemplateSpecHash, Namespace: d.Namespace, OwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(d, controllerKind)}, Labels: newRSTemplate.Labels, }, Spec: apps.ReplicaSetSpec{ Replicas: new(int32), MinReadySeconds: d.Spec.MinReadySeconds, Selector: newRSSelector, Template: newRSTemplate, }, } allRSs := append(oldRSs, &newRS) // 获取目标副本数 newReplicasCount, err := deploymentutil.NewRSNewReplicas(d, allRSs, &newRS) if err != nil { return nil, err } // 更新副本数 *(newRS.Spec.Replicas) = newReplicasCount // 设置ReplicaSet对象的注解 deploymentutil.SetNewReplicaSetAnnotations(ctx, d, &newRS, newRevision, false, maxRevHistoryLengthInChars) // 创建ReplicaSet对象并处理异常 alreadyExists := false createdRS, err := dc.client.AppsV1().ReplicaSets(d.Namespace).Create(ctx, &newRS, metav1.CreateOptions{}) switch { // ReplicaSet对象已经存在(哈希冲突) case errors.IsAlreadyExists(err): alreadyExists = true rs, rsErr := dc.rsLister.ReplicaSets(newRS.Namespace).Get(newRS.Name) if rsErr != nil { return nil, rsErr } controllerRef := metav1.GetControllerOf(rs) if controllerRef != nil && controllerRef.UID == d.UID && deploymentutil.EqualIgnoreHash(&d.Spec.Template, &rs.Spec.Template) { createdRS = rs err = nil break } if d.Status.CollisionCount == nil { d.Status.CollisionCount = new(int32) } preCollisionCount := *d.Status.CollisionCount *d.Status.CollisionCount++ // Update the collisionCount for the Deployment and let it requeue by returning the original // error. _, dErr := dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) if dErr == nil { logger.V(2).Info(\"Found a hash collision for deployment - bumping collisionCount to resolve it\", \"deployment\", klog.KObj(d), \"oldCollisionCount\", preCollisionCount, \"newCollisionCount\", *d.Status.CollisionCount) } return nil, err // 命名空间正在删除导致的异常 case errors.HasStatusCause(err, v1.NamespaceTerminatingCause): // if the namespace is terminating, all subsequent creates will fail and we can safely do nothing return nil, err case err != nil: msg := fmt.Sprintf(\"Failed to create new replica set %q: %v\", newRS.Name, err) if deploymentutil.HasProgressDeadline(d) { cond := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, deploymentutil.FailedRSCreateReason, msg) deploymentutil.SetDeploymentCondition(&d.Status, *cond) _, _ = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } dc.eventRecorder.Eventf(d, v1.EventTypeWarning, deploymentutil.FailedRSCreateReason, msg) return nil, err } // 创建成功记录事件 if !alreadyExists && newReplicasCount > 0 { dc.eventRecorder.Eventf(d, v1.EventTypeNormal, \"ScalingReplicaSet\", \"Scaled up replica set %s from 0 to %d\", createdRS.Name, newReplicasCount) } // 检查Deployment是否需要更新 needsUpdate := deploymentutil.SetDeploymentRevision(d, newRevision) if !alreadyExists && deploymentutil.HasProgressDeadline(d) { msg := fmt.Sprintf(\"Created new replica set %q\", createdRS.Name) condition := deploymentutil.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, deploymentutil.NewReplicaSetReason, msg) deploymentutil.SetDeploymentCondition(&d.Status, *condition) needsUpdate = true } // 更新Deployment对象 if needsUpdate { _, err = dc.client.AppsV1().Deployments(d.Namespace).UpdateStatus(ctx, d, metav1.UpdateOptions{}) } return createdRS, err } 给新的ReplicaSet对象设置注解 SetNewReplicaSetAnnotations()方法返回一个bool值来表示注解是否被修改。 func SetNewReplicaSetAnnotations(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, newRevision string, exists bool, revHistoryLimitInChars int) bool { logger := klog.FromContext(ctx) // 基于Deployment对象更新注解 annotationChanged := copyDeploymentAnnotationsToReplicaSet(deployment, newRS) // 更新注解部分的版本号Revision if newRS.Annotations == nil { newRS.Annotations = make(map[string]string) } // 获取ReplicaSet对象当前的版本号 oldRevision, ok := newRS.Annotations[RevisionAnnotation] oldRevisionInt, err := strconv.ParseInt(oldRevision, 10, 64) if err != nil { if oldRevision != \"\" { logger.Info(\"Updating replica set revision OldRevision not int\", \"err\", err) return false } //If the RS annotation is empty then initialise it to 0 oldRevisionInt = 0 } newRevisionInt, err := strconv.ParseInt(newRevision, 10, 64) if err != nil { logger.Info(\"Updating replica set revision NewRevision not int\", \"err\", err) return false } // 比较ReplicaSet对象当前版本号和目标版本号是否相等 if oldRevisionInt revHistoryLimitInChars && start 同步Deployment对象的状态 该流程的最后一步是同步Deployment对象的状态，逻辑很简单，首先计算一个预期的Status，然后和原始数据做比较，如果不同就向ApiServer发送一个更新请求。 func (dc *DeploymentController) syncDeploymentStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error { newStatus := calculateStatus(allRSs, newRS, d) if reflect.DeepEqual(d.Status, newStatus) { return nil } newDeployment := d newDeployment.Status = newStatus _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{}) return err } Deployment对象需要扩缩容或被手动暂停 在d.Spec.Paused的值为true时，表示Deployment对象被手动暂停，isScalingEvent()方法根据Deployment对象的Spec.Replicas与注释信息\"desired-replicas\"的值是否一致来判断是否要进行扩缩容操作。两种情况的结果都是直接调用sync()方法。 扩缩容的入口sync()方法对比syncStatusOnly()方法多了两个步骤，一个是执行扩缩容操作scale()，另一个差别是判断如果是暂停状态且没有回滚目标，就需要清理旧的ReplicaSet对象。 func (dc *DeploymentController) sync(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } if err := dc.scale(ctx, d, newRS, oldRSs); err != nil { // If we get an error while trying to scale, the deployment will be requeued // so we can abort this resync return err } // Clean up the deployment when it's paused and no rollback is in flight. if d.Spec.Paused && getRollbackTo(d) == nil { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } allRSs := append(oldRSs, newRS) return dc.syncDeploymentStatus(ctx, allRSs, newRS, d) } 扩缩容逻辑 scale()是管理ReplicaSet副本数的核心方法，其中包含三个实际场景以及处理逻辑。 场景一 func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { // 场景一：单ReplicaSet活跃 if activeOrLatest := deploymentutil.FindActiveOrLatest(newRS, oldRSs); activeOrLatest != nil { if *(activeOrLatest.Spec.Replicas) == *(deployment.Spec.Replicas) { return nil } // 实际副本数和期望副本数不一致 对其进行调节 _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, activeOrLatest, *(deployment.Spec.Replicas), deployment) return err } ...... } 只有一个ReplicaSet实例存在Pod，具体情况包括：1.新的ReplicaSet被创建;2.回滚后只剩下旧ReplicaSet;3.滚动更新时旧ReplicaSet副本已经归零。 首先获取活跃的ReplicaSet对象，所谓活跃就是副本数大于0。其内部逻辑是在所有的ReplicaSet中查找副本数大于0的对象，如果结果是1个直接返回;如果是0个，先看最新的ReplicaSet对象是否存在，如果存在就返回它，不存在就返回列表中第一个旧的ReplicaSet对象;如果超过1个表示正在滚动更新过程中，返回nil。如果activeOrLatest不为空，对比活跃ReplicaSet对象的副本数和Deployment对象中是否是一致的，如果一致则不做处理。数量不一致则调用scaleReplicaSetAndRecordEvent()方法调整副本数，保证在后续逻辑开始前ReplicaSet副本实际状态和预期状态的一致性。 场景二 func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { ...... // 场景二：新ReplicaSet已经饱和 需要缩容旧ReplicaSet if deploymentutil.IsSaturated(deployment, newRS) { // 找到旧ReplicaSet中实例不为0的 for _, old := range controller.FilterActiveReplicaSets(oldRSs) { // 以0为期望值进行更新 if _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, old, 0, deployment); err != nil { return err } } return nil } ...... } 清理阶段，新的ReplicaSet已经饱和(副本数量达到Deployment的期望)，需要把剩余旧的ReplicaSet管理的副本数量缩至0。判断依据是ReplicaSet中三个字段的值要和Deployment的Spec.Replicas中设置相同：1.Spec.Replicas;2.Annotations中desired-replicas的值;3.Status.AvailableReplicas。此处可能会有疑问，为什么要同时确认Spec和Annotations中的值，其实他们表示的语义是不同的，Spec只能表示一个当前的期望状态，可能会动态变化，而Annotations是在创建ReplicaSet对象注入的Deployment最终期望。 执行的动作就是找出旧的ReplicaSet中副本数不为0的对象，然后调用scaleReplicaSetAndRecordEvent()方法把它们的期望值更新为0，和场景一的处理方式基本相同。 场景三(核心场景) 此为多ReplicaSet共存的滚动更新中间场景，首先确定策略是否为滚动更新，然后获取所有当前副本数大于0的ReplicaSet对象，根据Replicas和MaxSurge计算本次进行调整的副本总数。需要注意的是，在该滚动更新操作中，扩/缩容的动作是单向的，不会有一个对象扩容的同时另一个对象缩容的情况。通过反复扩容-缩容的动作，再经过场景二的收尾，最终实际的副本数与期望值相同，并且由新ReplicaSet替换了旧的对象。 func (dc *DeploymentController) scale(ctx context.Context, deployment *apps.Deployment, newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet) error { ...... // 场景三：滚动更新进行中 // 判断策略是否为RollingUpdate if deploymentutil.IsRollingUpdate(deployment) { // 获取所有存在副本的ReplicaSet以及对应的副本数量 allRSs := controller.FilterActiveReplicaSets(append(oldRSs, newRS)) allRSsReplicas := deploymentutil.GetReplicaCountForReplicaSets(allRSs) allowedSize := int32(0) // 计算允许最大存在副本数 if *(deployment.Spec.Replicas) > 0 { allowedSize = *(deployment.Spec.Replicas) + deploymentutil.MaxSurge(*deployment) } // 根据当前总副本数判断下一步是扩容新ReplicaSet还是缩容旧ReplicaSet deploymentReplicasToAdd := allowedSize - allRSsReplicas switch { case deploymentReplicasToAdd > 0: // 扩容优先处理新的ReplicaSet对象 sort.Sort(controller.ReplicaSetsBySizeNewer(allRSs)) case deploymentReplicasToAdd 扩缩容调整ReplicaSet对象的比例 先根据Deployment对象检查副本数和注释信息是否有需要调整的，如果需要调整就深拷贝一份最新ReplicaSet对象，然后先向ApiServer发注释信息的更新请求，然后判断是否有扩/缩容的需要，记录并将标识位返回给上层。 func (dc *DeploymentController) scaleReplicaSet(ctx context.Context, rs *apps.ReplicaSet, newScale int32, deployment *apps.Deployment) (bool, *apps.ReplicaSet, error) { // 检查副本数是否需要调整 sizeNeedsUpdate := *(rs.Spec.Replicas) != newScale // 检查注释信息是否需要调整 annotationsNeedUpdate := deploymentutil.ReplicasAnnotationsNeedUpdate(rs, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) scaled := false var err error // 需要更新时 if sizeNeedsUpdate || annotationsNeedUpdate { oldScale := *(rs.Spec.Replicas) // 深拷贝保证对象信息最新 rsCopy := rs.DeepCopy() *(rsCopy.Spec.Replicas) = newScale // 设置ReplicaSet对象注释信息 deploymentutil.SetReplicasAnnotations(rsCopy, *(deployment.Spec.Replicas), *(deployment.Spec.Replicas)+deploymentutil.MaxSurge(*deployment)) // 发起更新请求 rs, err = dc.client.AppsV1().ReplicaSets(rsCopy.Namespace).Update(ctx, rsCopy, metav1.UpdateOptions{}) if err == nil && sizeNeedsUpdate { var scalingOperation string // 判断后续动作是扩容还是缩容 if oldScale 滚动更新数量计算规则 由GetReplicaSetProportion()函数返回给外层一个整数，这个数值的绝对值不超过允许值。 func GetReplicaSetProportion(logger klog.Logger, rs *apps.ReplicaSet, d apps.Deployment, deploymentReplicasToAdd, deploymentReplicasAdded int32) int32 { if rs == nil || *(rs.Spec.Replicas) == 0 || deploymentReplicasToAdd == 0 || deploymentReplicasToAdd == deploymentReplicasAdded { return int32(0) } // 计算调整比例 rsFraction := getReplicaSetFraction(logger, *rs, d) allowed := deploymentReplicasToAdd - deploymentReplicasAdded // 限制调整的绝对值不超过allowed限制 if deploymentReplicasToAdd > 0 { return min(rsFraction, allowed) } return max(rsFraction, allowed) } 期望更新副本的差额计算逻辑在getReplicaSetFraction()函数中实现，如果Deployment要把副本数缩容到0，就直接返回当前ReplicaSet副本数作为差额。然后检查ReplicaSet注释中的最大容量，然后根据公式期望容量=当前副本数*当前最大容量/上一轮最大容量，返回本轮要扩/缩容的数量。 func getReplicaSetFraction(logger klog.Logger, rs apps.ReplicaSet, d apps.Deployment) int32 { // 如果想要缩容至0 直接返回当前的副本数 if *(d.Spec.Replicas) == int32(0) { return -*(rs.Spec.Replicas) } // 获取Deployment的当前最大容量和上一轮为ReplicaSet对象注入的最大容量 deploymentMaxReplicas := *(d.Spec.Replicas) + MaxSurge(d) deploymentMaxReplicasBeforeScale, ok := getMaxReplicasAnnotation(logger, &rs) // 如果ReplicaSet对象的最大容量注解缺失或值为0 if !ok || deploymentMaxReplicasBeforeScale == 0 { // 用当前Deployment的副本数重新写入 deploymentMaxReplicasBeforeScale = d.Status.Replicas // 异常情况 返回0给上层 避免后续计算出无效比例0 if deploymentMaxReplicasBeforeScale == 0 { return 0 } } // 获取ReplicaSet当前副本数 scaleBase := *(rs.Spec.Replicas) // 计算规则为 当前副本数*当前最大容量/上一轮最大容量 newRSsize := (float64(scaleBase * deploymentMaxReplicas)) / float64(deploymentMaxReplicasBeforeScale) // 返回期望调整的副本数量 return integer.RoundToInt32(newRSsize) - *(rs.Spec.Replicas) } Deployment对象需要回滚 根据Deployment对象Annotation中\"deprecated.deployment.rollback.to\"的值来显式指定回滚的版本，会在未来被逐渐弃用并使用kubectl rollback命令控制回滚，修改资源对象是不被推荐的行为，该回滚逻辑的代码在rollback()方法中实现。 首先获取ReplicaSet的信息，然后从注解信息中找出期望回滚的Revision版本号，如果是0尝试回滚到最近的一个版本。正常情况下遍历所有的ReplicaSet对象，并尝试根据Revision进行匹配，然后用ReplicaSet的Pod描述也就是Template字段更新当前Deployment中的内容，同时也更新注释信息，最后向ApiServer发送对Deployment对象的更新请求并请求回滚注解。 func (dc *DeploymentController) rollback(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { logger := klog.FromContext(ctx) // 获取ReplicaSet对象 newRS, allOldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } allRSs := append(allOldRSs, newRS) // 获取目标版本号 rollbackTo := getRollbackTo(d) // 特殊情况处理 if rollbackTo.Revision == 0 { // Revision为0时尝试回滚到上一个版本 if rollbackTo.Revision = deploymentutil.LastRevision(logger, allRSs); rollbackTo.Revision == 0 { dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, \"Unable to find last revision.\") // 清除rollbackto注解 return dc.updateDeploymentAndClearRollbackTo(ctx, d) } } for _, rs := range allRSs { v, err := deploymentutil.Revision(rs) if err != nil { logger.V(4).Info(\"Unable to extract revision from deployment's replica set\", \"replicaSet\", klog.KObj(rs), \"err\", err) continue } // 匹配Revision if v == rollbackTo.Revision { logger.V(4).Info(\"Found replica set with desired revision\", \"replicaSet\", klog.KObj(rs), \"revision\", v) // 更新Template和注解并向ApiServer发送更新请求 performedRollback, err := dc.rollbackToTemplate(ctx, d, rs) if performedRollback && err == nil { dc.emitRollbackNormalEvent(d, fmt.Sprintf(\"Rolled back deployment %q to revision %d\", d.Name, rollbackTo.Revision)) } return err } } dc.emitRollbackWarningEvent(d, deploymentutil.RollbackRevisionNotFound, \"Unable to find the revision to rollback to.\") // 清理rollbackto注解 return dc.updateDeploymentAndClearRollbackTo(ctx, d) } Deployment对象滚动更新 Recreate策略 如果经过判断，滚动更新的策略为Recreate，其更新的处理方式为先终止旧的Pod，再启动新的Pod，在代码中由rolloutRecreate()方法为入口进入后续逻辑，一些核心的逻辑在之前的扩缩容部分已经有所涉及，下面分析该部分代码。 在一开始先获取新旧ReplicaSet对象，值得注意的是getAllReplicaSetsAndSyncRevision()方法传入一个false，因为Recreate逻辑严格要求先把旧的实例删掉才能创建新的，如果缩容操作前创建新ReplicaSet会导致新旧版本实例共存。然后获取旧版本中有Pod实例存在的ReplicaSet对象，并修改它们的Spec.Replicas为0，然后在直到没有旧版本的Pod运行前都对Deployment的状态进行同步，缩容操作完成后如果新的ReplicaSet不存在，再次调用getAllReplicaSetsAndSyncRevision()方法传入true，创建该对象。扩容新ReplicaSet，扩容完成后清理旧版本对象并同步Deployment状态。 func (dc *DeploymentController) rolloutRecreate(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) error { // 第四个入参表示如果ReplicaSet不存在是否创建 // 在缩容阶段避免新旧Pod共存 此处不直接创建 newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, false) if err != nil { return err } allRSs := append(oldRSs, newRS) // 获取活跃的旧ReplicaSet对象 activeOldRSs := controller.FilterActiveReplicaSets(oldRSs) // 把所有活跃的旧ReplicaSet副本数设置为0 scaledDown, err := dc.scaleDownOldReplicaSetsForRecreate(ctx, activeOldRSs, d) if err != nil { return err } // 缩容旧的ReplicaSet if scaledDown { // 同步Deployment状态 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // 等待缩容结束 if oldPodsRunning(newRS, oldRSs, podMap) { // 同步Deployment状态 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // 如果新的ReplicaSet对象不存在 自动创建它 if newRS == nil { newRS, oldRSs, err = dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } // 记录对象 allRSs = append(oldRSs, newRS) } // 扩容新的ReplicaSet if _, err := dc.scaleUpNewReplicaSetForRecreate(ctx, newRS, d); err != nil { return err } // 清理旧的ReplicaSet if util.DeploymentComplete(d, &d.Status) { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } // 同步Deployment状态 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } oldPodsRunning()函数用来检查是否有Pod实例在运行中，首先获取所有的Pod集合，然后检查其Status.Phase字段，该字段表示Pod的状态，包括Pending/Running/Running/Failed/Unknown，对应各生命周期状态。对于属于新版本ReplicaSet管理的跳过处理，该逻辑只确认旧版本的Pod是否有仍处于或可能处于运行状态的。 func oldPodsRunning(newRS *apps.ReplicaSet, oldRSs []*apps.ReplicaSet, podMap map[types.UID][]*v1.Pod) bool { if oldPods := util.GetActualReplicaCountForReplicaSets(oldRSs); oldPods > 0 { return true } // 遍历Pod for rsUID, podList := range podMap { // 跳过属于新ReplicaSet管理的Pod if newRS != nil && newRS.UID == rsUID { continue } for _, pod := range podList { switch pod.Status.Phase { case v1.PodFailed, v1.PodSucceeded: // 退出状态 continue case v1.PodUnknown: // 异常状态 return true default: // 其他运行状态 return true } } } return false } syncRolloutStatus()方法用来同步缩容的状态，首先会根据当前观测到的Generation、Replicas、UpdatedReplicas、ReadyReplicas、AvailableReplicas等副本数量信息，判断当前Deployment对象的状态是否达成了最低的可用条件，然后更新CondType为DeploymentAvailable的状态并组装一个最新的DeploymentStatus类型的状态信息。然后尝试获取DeploymentProgressing的状态信息，并对Deployment状态进行判断，如果副本数和新版本副本数相等且状态信息为新ReplicaSet可用(NewReplicaSetAvailable)表示部署完成。如果结果表示未完成部署，则对结果进行确认并向ApiServer发送更新Deployment的请求。 func (dc *DeploymentController) syncRolloutStatus(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, d *apps.Deployment) error { // 计算Deployment最新状态 newStatus := calculateStatus(allRSs, newRS, d) // 如果没有配置截止时间 就删除CondType为DeploymentProgressing为状态信息 if !util.HasProgressDeadline(d) { util.RemoveDeploymentCondition(&newStatus, apps.DeploymentProgressing) } // 获取CondType为DeploymentProgressing为状态信息 currentCond := util.GetDeploymentCondition(d.Status, apps.DeploymentProgressing) // Deployment状态判断 isCompleteDeployment := newStatus.Replicas == newStatus.UpdatedReplicas && currentCond != nil && currentCond.Reason == util.NewRSAvailableReason // 未部署完成 进行状态判断并更新 if util.HasProgressDeadline(d) && !isCompleteDeployment { switch { // 已完成 case util.DeploymentComplete(d, &newStatus): msg := fmt.Sprintf(\"Deployment %q has successfully progressed.\", d.Name) if newRS != nil { msg = fmt.Sprintf(\"ReplicaSet %q has successfully progressed.\", newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.NewRSAvailableReason, msg) util.SetDeploymentCondition(&newStatus, *condition) // 处理中 case util.DeploymentProgressing(d, &newStatus): msg := fmt.Sprintf(\"Deployment %q is progressing.\", d.Name) if newRS != nil { msg = fmt.Sprintf(\"ReplicaSet %q is progressing.\", newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionTrue, util.ReplicaSetUpdatedReason, msg) if currentCond != nil { if currentCond.Status == v1.ConditionTrue { condition.LastTransitionTime = currentCond.LastTransitionTime } util.RemoveDeploymentCondition(&newStatus, apps.DeploymentProgressing) } util.SetDeploymentCondition(&newStatus, *condition) // 已超时 case util.DeploymentTimedOut(ctx, d, &newStatus): msg := fmt.Sprintf(\"Deployment %q has timed out progressing.\", d.Name) if newRS != nil { msg = fmt.Sprintf(\"ReplicaSet %q has timed out progressing.\", newRS.Name) } condition := util.NewDeploymentCondition(apps.DeploymentProgressing, v1.ConditionFalse, util.TimedOutReason, msg) util.SetDeploymentCondition(&newStatus, *condition) } } // 处理失败状态Condition if replicaFailureCond := dc.getReplicaFailures(allRSs, newRS); len(replicaFailureCond) > 0 { // 只会返回一条信息 util.SetDeploymentCondition(&newStatus, replicaFailureCond[0]) } else { // 没有失败信息就从Map中删除该key util.RemoveDeploymentCondition(&newStatus, apps.DeploymentReplicaFailure) } // 新旧状态是否一致 if reflect.DeepEqual(d.Status, newStatus) { // 如果状态一致 把Deployment对象重新入队并返回 dc.requeueStuckDeployment(ctx, d, newStatus) return nil } newDeployment := d newDeployment.Status = newStatus // 更新对象 _, err := dc.client.AppsV1().Deployments(newDeployment.Namespace).UpdateStatus(ctx, newDeployment, metav1.UpdateOptions{}) return err } RollingUpdate策略 如果经过判断，更新策略为RollingUpdate，则采用滚动更新方式，逻辑入口为rolloutRolling()方法。从外层的逻辑来看很清晰，首先获取对象的信息，然后有限尝试扩容新ReplicaSet对象，如果扩容则本次调谐返回并更新状态，如果无法进行扩容动作，则对旧ReplicaSet进行缩容操作，如果缩容也返回并更新Deployment状态，如果两者都没有就根据Spec和Status的一致性检查Deployment对象是否为部署成功的状态，如果是就清理旧ReplicaSet对象，最后更新状态。 func (dc *DeploymentController) rolloutRolling(ctx context.Context, d *apps.Deployment, rsList []*apps.ReplicaSet) error { newRS, oldRSs, err := dc.getAllReplicaSetsAndSyncRevision(ctx, d, rsList, true) if err != nil { return err } allRSs := append(oldRSs, newRS) // 尝试扩容新版本 scaledUp, err := dc.reconcileNewReplicaSet(ctx, allRSs, newRS, d) if err != nil { return err } if scaledUp { // 结束本次调谐 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // 尝试缩容旧版本 scaledDown, err := dc.reconcileOldReplicaSets(ctx, allRSs, controller.FilterActiveReplicaSets(oldRSs), newRS, d) if err != nil { return err } if scaledDown { // 结束本次调谐 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } // 检查是否完成部署 if deploymentutil.DeploymentComplete(d, &d.Status) { if err := dc.cleanupDeployment(ctx, oldRSs, d); err != nil { return err } } // 同步状态 return dc.syncRolloutStatus(ctx, allRSs, newRS, d) } 尝试扩容新ReplicaSet 对这段逻辑进行简单的解释，首先对ReplicaSet和Deployment其中的Spec.Replicas字段做比较。如果新ReplicaSet已经和Deployment的期望副本数一致了则不做处理;如果是非预期的新Replicas期望副本数大于Deployment，则调整ReplicaSet的期望副本数为Deployment的期望副本数;其他情况就只剩下新Replicas期望副本数小于Deployment了，计算一下本次调整后的新ReplicaSet副本数并执行更新操作。 func (dc *DeploymentController) reconcileNewReplicaSet(ctx context.Context, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { if *(newRS.Spec.Replicas) == *(deployment.Spec.Replicas) { // Scaling not required. return false, nil } if *(newRS.Spec.Replicas) > *(deployment.Spec.Replicas) { // Scale down. scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, *(deployment.Spec.Replicas), deployment) return scaled, err } newReplicasCount, err := deploymentutil.NewRSNewReplicas(deployment, allRSs, newRS) if err != nil { return false, err } scaled, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, newRS, newReplicasCount, deployment) return scaled, err } 扩容新副本数的计算 通过NewRSNewReplicas()函数计算出新ReplicaSet调整后的副本数量，规则也很简单。如果是Recreate策略直接返回Deployment的期望值;如果是RollingUpdate策略，根据MaxSurge计算Deployment的副本数量上限，然后根据Deployment副本数上限与当前总副本数的差值和Deployment期望副本数与新ReplicaSet期望副本数的差值，选择其中较小的加上当前新ReplicaSet的期望副本数，返回给上层作为调整后的期望副本数值。 func NewRSNewReplicas(deployment *apps.Deployment, allRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet) (int32, error) { switch deployment.Spec.Strategy.Type { case apps.RollingUpdateDeploymentStrategyType: // Check if we can scale up. maxSurge, err := intstrutil.GetScaledValueFromIntOrPercent(deployment.Spec.Strategy.RollingUpdate.MaxSurge, int(*(deployment.Spec.Replicas)), true) if err != nil { return 0, err } // Find the total number of pods currentPodCount := GetReplicaCountForReplicaSets(allRSs) maxTotalPods := *(deployment.Spec.Replicas) + int32(maxSurge) if currentPodCount >= maxTotalPods { // Cannot scale up. return *(newRS.Spec.Replicas), nil } // Scale up. scaleUpCount := maxTotalPods - currentPodCount // Do not exceed the number of desired replicas. scaleUpCount = min(scaleUpCount, *(deployment.Spec.Replicas)-*(newRS.Spec.Replicas)) return *(newRS.Spec.Replicas) + scaleUpCount, nil case apps.RecreateDeploymentStrategyType: return *(deployment.Spec.Replicas), nil default: return 0, fmt.Errorf(\"deployment type %v isn't supported\", deployment.Spec.Strategy.Type) } } 尝试缩容旧ReplicaSet 缩容旧ReplicaSet的过程中首先计算最大可缩容数量，其计算公式为当前副本数-最小可用副本数-新ReplicaSet不可用副本数，然后根据最大缩容数量去缩容处理旧版本的ReplicaSet，总共会经历两轮缩容，第一次先清理旧ReplicaSet中的不健康副本，返回一个数量cleanupCount，然后再正常进行缩容，返回一个数量scaledDownCount，如果两者的和大于0表示进行了缩容操作。 func (dc *DeploymentController) reconcileOldReplicaSets(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, newRS *apps.ReplicaSet, deployment *apps.Deployment) (bool, error) { logger := klog.FromContext(ctx) oldPodsCount := deploymentutil.GetReplicaCountForReplicaSets(oldRSs) // 没有副本可以缩容 if oldPodsCount == 0 { return false, nil } allPodsCount := deploymentutil.GetReplicaCountForReplicaSets(allRSs) logger.V(4).Info(\"New replica set\", \"replicaSet\", klog.KObj(newRS), \"availableReplicas\", newRS.Status.AvailableReplicas) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) minAvailable := *(deployment.Spec.Replicas) - maxUnavailable newRSUnavailablePodCount := *(newRS.Spec.Replicas) - newRS.Status.AvailableReplicas // 计算最大缩容数量 maxScaledDown := allPodsCount - minAvailable - newRSUnavailablePodCount if maxScaledDown 0, nil } 缩容不健康副本 首先对所有旧版本的副本按创建时间进行排序，优先处理创建更早的副本。遍历所有旧的ReplicaSet对象，总共缩容数量不能超过方法中传入的maxCleanupCount，每个ReplicaSet的缩容选择缩容余额和不健康副本数两者中较小的，更新ReplicaSet的副本数为Spec.Replicas-scaledDownCount，并更新本地缓存中的ReplicaSet对象，每次缩容的值进行累加最终返回给上层。 func (dc *DeploymentController) cleanupUnhealthyReplicas(ctx context.Context, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment, maxCleanupCount int32) ([]*apps.ReplicaSet, int32, error) { logger := klog.FromContext(ctx) // 根据创建时间从早到晚排序 sort.Sort(controller.ReplicaSetsByCreationTimestamp(oldRSs)) // 初始化计数 totalScaledDown := int32(0) // 遍历ReplicaSet for i, targetRS := range oldRSs { // 受maxCleanupCount限制清理的最大数量 if totalScaledDown >= maxCleanupCount { break } if *(targetRS.Spec.Replicas) == 0 { // 跳过没有副本的ReplicaSet continue } logger.V(4).Info(\"Found available pods in old RS\", \"replicaSet\", klog.KObj(targetRS), \"availableReplicas\", targetRS.Status.AvailableReplicas) if *(targetRS.Spec.Replicas) == targetRS.Status.AvailableReplicas { // no unhealthy replicas found, no scaling required. continue } // 计算缩容数量 取缩容余额和不可用副本数中较小的 scaledDownCount := min(maxCleanupCount-totalScaledDown, *(targetRS.Spec.Replicas)-targetRS.Status.AvailableReplicas) newReplicasCount := *(targetRS.Spec.Replicas) - scaledDownCount if newReplicasCount > *(targetRS.Spec.Replicas) { return nil, 0, fmt.Errorf(\"when cleaning up unhealthy replicas, got invalid request to scale down %s/%s %d -> %d\", targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount) } // 更新ReplicaSet副本数 _, updatedOldRS, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment) if err != nil { return nil, totalScaledDown, err } // 累加计数 totalScaledDown += scaledDownCount // 更新旧ReplicaSet缓存 oldRSs[i] = updatedOldRS } return oldRSs, totalScaledDown, nil } 正常缩容 正常缩容的逻辑和处理不健康副本类似，先进行排序，然后相当于是重新计算了最大可缩容副本数，遍历ReplicaSet并选择缩容余额和期望副本数中较小的作为缩容数量，更新ReplicaSet对象并计数缩容的副本。 func (dc *DeploymentController) scaleDownOldReplicaSetsForRollingUpdate(ctx context.Context, allRSs []*apps.ReplicaSet, oldRSs []*apps.ReplicaSet, deployment *apps.Deployment) (int32, error) { logger := klog.FromContext(ctx) maxUnavailable := deploymentutil.MaxUnavailable(*deployment) minAvailable := *(deployment.Spec.Replicas) - maxUnavailable availablePodCount := deploymentutil.GetAvailableReplicaCountForReplicaSets(allRSs) if availablePodCount = totalScaleDownCount { break } if *(targetRS.Spec.Replicas) == 0 { // 跳过没有副本的ReplicaSet continue } // 计算缩容数量 scaleDownCount := min(*(targetRS.Spec.Replicas), totalScaleDownCount-totalScaledDown) newReplicasCount := *(targetRS.Spec.Replicas) - scaleDownCount if newReplicasCount > *(targetRS.Spec.Replicas) { return 0, fmt.Errorf(\"when scaling down old RS, got invalid request to scale down %s/%s %d -> %d\", targetRS.Namespace, targetRS.Name, *(targetRS.Spec.Replicas), newReplicasCount) } // 更新ReplicaSet副本数 _, _, err := dc.scaleReplicaSetAndRecordEvent(ctx, targetRS, newReplicasCount, deployment) if err != nil { return totalScaledDown, err } // 累加计数 totalScaledDown += scaleDownCount } return totalScaledDown, nil } © 2025 lts0609. All rights reserved. all right reserved，powered by Gitbook最后更新时间： 2025-07-16 17:07:25 "}}